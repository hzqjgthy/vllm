# vLLM åœ¨çº¿æœåŠ¡æ–‡æ¡£

æœ¬ç›®å½•åŒ…å« vLLM åœ¨çº¿æœåŠ¡éƒ¨ç½²çš„å®Œæ•´æ–‡æ¡£å’Œä»£ç ã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `vLLMåœ¨çº¿æœåŠ¡éƒ¨ç½²å®Œæ•´æŒ‡å—.md` | å®Œæ•´çš„éƒ¨ç½²æŒ‡å—ï¼ŒåŒ…å«åŸç†ã€é…ç½®ã€é—®é¢˜æ’æŸ¥ç­‰ |
| `vllm_client.py` | å¯ç›´æ¥ä½¿ç”¨çš„å®¢æˆ·ç«¯å°è£…ä»£ç  |
| `README.md` | æœ¬æ–‡ä»¶ï¼Œå¿«é€Ÿå…¥é—¨æŒ‡å— |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æœåŠ¡å™¨ç«¯å¯åŠ¨ vLLM

```bash
vllm serve /root/autodl-tmp/vllm/Qwen/Qwen3-4B \
    --served-model-name Qwen3-4B \
    --api_key muyu \
    --host 0.0.0.0 \
    --port 9000 \
    --trust_remote_code \
    --tensor_parallel_size 1
```

### 2. æœ¬åœ°å»ºç«‹ SSH éš§é“

```powershell
# Windows å‘½ä»¤è¡Œ
ssh -p 47055 -L 9000:localhost:9000 root@connect.nmb2.seetacloud.com
```

### 3. ä½¿ç”¨å®¢æˆ·ç«¯

#### æ–¹å¼ Aï¼šå¤åˆ¶ vllm_client.py åˆ°ä½ çš„é¡¹ç›®

```python
from vllm_client import VLLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = VLLMClient(backend='requests')

# ç®€å•å¯¹è¯
response = client.chat("ä½ å¥½")
print(response)
```

#### æ–¹å¼ Bï¼šç›´æ¥ä½¿ç”¨ requests

```python
import requests

url = "http://localhost:9000/v1/chat/completions"
headers = {
    "Content-Type": "application/json",
    "Authorization": "Bearer muyu"
}
data = {
    "model": "Qwen3-4B",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 512
}

response = requests.post(url, headers=headers, json=data)
print(response.json()['choices'][0]['message']['content'])
```

#### æ–¹å¼ Cï¼šä½¿ç”¨ OpenAI SDK

```python
from openai import OpenAI
import httpx

# å¿…é¡»é…ç½®å®Œæ•´çš„ httpx å‚æ•°ï¼
http_client = httpx.Client(
    timeout=120.0,
    limits=httpx.Limits(
        max_keepalive_connections=0,
        max_connections=100,
        keepalive_expiry=0
    ),
    transport=httpx.HTTPTransport(retries=0)
)

client = OpenAI(
    base_url="http://localhost:9000/v1",
    api_key="muyu",
    http_client=http_client
)

completion = client.chat.completions.create(
    model="Qwen3-4B",
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    max_tokens=512
)

print(completion.choices[0].message.content)
```

## ğŸ“– å®Œæ•´æ–‡æ¡£

è¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹ï¼š[vLLMåœ¨çº¿æœåŠ¡éƒ¨ç½²å®Œæ•´æŒ‡å—.md](./vLLMåœ¨çº¿æœåŠ¡éƒ¨ç½²å®Œæ•´æŒ‡å—.md)

åŒ…å«å†…å®¹ï¼š
- âœ… è¯¦ç»†çš„é…ç½®è¯´æ˜
- âœ… å¸¸è§é—®é¢˜æ’æŸ¥
- âœ… é”™è¯¯å¤„ç†æ–¹æ¡ˆ
- âœ… ç”Ÿäº§çº§ä»£ç ç¤ºä¾‹
- âœ… FAQ å¸¸è§é—®é¢˜

## âš ï¸ é‡è¦æç¤º

1. **æœåŠ¡å™¨ç«¯**ï¼šå¿…é¡»ä½¿ç”¨ `--host 0.0.0.0`
2. **SSH éš§é“**ï¼šä¿æŒç»ˆç«¯çª—å£æ‰“å¼€
3. **max_tokens**ï¼šå¿…é¡»æ˜¾å¼è®¾ç½®ï¼ˆé»˜è®¤å€¼åªæœ‰ 16ï¼‰
4. **OpenAI SDK**ï¼šå¿…é¡»å®Œæ•´é…ç½® httpx çš„å››ä¸ªå‚æ•°ï¼Œç¼ºä¸€ä¸å¯ï¼

## ğŸ¯ æ¨èæ–¹æ¡ˆ

| åœºæ™¯ | æ¨è | åŸå›  |
|------|------|------|
| ç¨³å®šæ€§ä¼˜å…ˆ | **requests** | ç®€å•å¯é ï¼Œæ— å…¼å®¹æ€§é—®é¢˜ |
| éœ€è¦æµå¼è¾“å‡º | **OpenAI SDK** | åŸç”Ÿæ”¯æŒ stream |
| å¿«é€Ÿå¼€å‘ | **vllm_client.py** | å°è£…å®Œå–„ï¼Œå¼€ç®±å³ç”¨ |

## ğŸ“ æ›´æ–°æ—¥å¿—

- 2025-10-05ï¼šåˆå§‹ç‰ˆæœ¬ï¼ŒåŒ…å«å®Œæ•´çš„éƒ¨ç½²æŒ‡å—å’Œå®¢æˆ·ç«¯ä»£ç 

## ğŸ¤ è´¡çŒ®

å¦‚æœ‰é—®é¢˜æˆ–æ”¹è¿›å»ºè®®ï¼Œè¯·æ›´æ–°ç›¸å…³æ–‡æ¡£ã€‚
