# Ollama æœåŠ¡ç®¡ç†å’Œ GPU ä½¿ç”¨æŒ‡å—

my_inset:
    ä½¿ç”¨ollamaæ‹‰å–æ¨¡å‹å¦‚æœå¾ˆæ…¢ï¼Œå¯æ‹‰å–å‰æ·»åŠ é•œåƒæºï¼š
        # è®¾ç½®é˜¿é‡Œäº‘é•œåƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        export OLLAMA_MIRRORS="https://ollama.mirror.aliyuncs.com"
        ä¸Šé¢çš„ï¼Œä¹Ÿå¯ä»¥ä¸æ·»åŠ é•œåƒï¼Œåœæ­¢é‡æ–°ä¸‹è½½å°±ä¼šæ¥ç€ä¸‹è½½ï¼Œé€Ÿåº¦å˜å¿«


## ğŸ“‹ ç›®å½•

1. [å®‰è£…çŠ¶æ€åˆ†æ](#å®‰è£…çŠ¶æ€åˆ†æ)
2. [åœæ­¢ Ollama æœåŠ¡](#åœæ­¢-ollama-æœåŠ¡)
3. [å¯ç”¨ GPU åŠ é€Ÿ](#å¯ç”¨-gpu-åŠ é€Ÿ)
4. [æœåŠ¡ç®¡ç†](#æœåŠ¡ç®¡ç†)
5. [GPU ç›‘æ§å’ŒéªŒè¯](#gpu-ç›‘æ§å’ŒéªŒè¯)
6. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ“Š å®‰è£…çŠ¶æ€åˆ†æ

### âœ… å®‰è£…æˆåŠŸçš„ç»„ä»¶

æ ¹æ®å®‰è£…æ—¥å¿—åˆ†æï¼š

```bash
>>> Installing ollama to /usr/local
>>> Downloading Linux amd64 bundle
>>> Creating ollama user...
>>> Adding ollama user to video group...
>>> The Ollama API is now available at 127.0.0.1:11434.
>>> Install complete.
```

**æˆåŠŸé¡¹**ï¼š
- âœ… Ollama ä¸»ç¨‹åºå·²å®‰è£…åˆ° `/usr/local`
- âœ… Ollama ç”¨æˆ·å’Œæƒé™ç»„å·²åˆ›å»º
- âœ… API æœåŠ¡é…ç½®å®Œæˆï¼Œç›‘å¬ `127.0.0.1:11434`

### âš ï¸ å®‰è£…è­¦å‘Š

#### è­¦å‘Š 1: systemd æœªè¿è¡Œ
```
WARNING: systemd is not running
```

**åŸå› **: Docker/AutoDL å®¹å™¨ç¯å¢ƒä¸è¿è¡Œå®Œæ•´çš„ systemd æœåŠ¡

**å½±å“**: Ollama ä¸ä¼šä½œä¸ºç³»ç»ŸæœåŠ¡è‡ªåŠ¨å¯åŠ¨ï¼Œéœ€è¦æ‰‹åŠ¨å¯åŠ¨

#### è­¦å‘Š 2: GPU æ£€æµ‹å¤±è´¥ï¼ˆå·²è§£å†³ï¼‰
```
WARNING: Unable to detect NVIDIA/AMD GPU
```

**åŸå› **: å®‰è£…æ—¶ç¼ºå°‘ç¡¬ä»¶æ£€æµ‹å·¥å…· `lspci` å’Œ `lshw`

**è§£å†³æ–¹æ¡ˆ**: å·²é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…
```bash
apt-get update && apt-get install -y pciutils lshw
```

---

## ğŸ›‘ åœæ­¢ Ollama æœåŠ¡

### æ–¹æ³• 1: ä½¿ç”¨ PID åœæ­¢ï¼ˆæ¨èï¼‰

å¦‚æœæ‚¨çŸ¥é“è¿›ç¨‹ PIDï¼ˆå¯åŠ¨æ—¶ä¼šæ˜¾ç¤ºï¼‰ï¼š

```bash
# æŸ¥çœ‹å¯åŠ¨æ—¥å¿—ä¸­çš„ PIDï¼Œä¾‹å¦‚ï¼š
# [1] 11661

# æ­£å¸¸åœæ­¢
kill 11661

# å¼ºåˆ¶åœæ­¢ï¼ˆå¦‚æœè¿›ç¨‹æ— å“åº”ï¼‰
kill -9 11661
```

### æ–¹æ³• 2: ä½¿ç”¨ pkill åœæ­¢

```bash
# åœæ­¢æ‰€æœ‰ ollama è¿›ç¨‹
pkill ollama

# å¼ºåˆ¶åœæ­¢
pkill -9 ollama
```

### æ–¹æ³• 3: ä½¿ç”¨ killall åœæ­¢

```bash
# åœæ­¢æ‰€æœ‰åä¸º ollama çš„è¿›ç¨‹
killall ollama

# å¼ºåˆ¶åœæ­¢
killall -9 ollama
```

### éªŒè¯æœåŠ¡å·²åœæ­¢

```bash
# æ£€æŸ¥ ollama è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
ps aux | grep ollama

# æ£€æŸ¥ç«¯å£æ˜¯å¦è¿˜è¢«å ç”¨
lsof -i :11434

# æˆ–ä½¿ç”¨ netstat
netstat -tulpn | grep 11434
```

---

## ğŸš€ å¯ç”¨ GPU åŠ é€Ÿ

### ç³»ç»Ÿ GPU é…ç½®

æ‚¨çš„ç³»ç»Ÿé…ç½®ï¼ˆéå¸¸å¼ºå¤§ï¼‰ï¼š

```
ç¡¬ä»¶é…ç½®ï¼š
- 8 Ã— NVIDIA GeForce RTX 3090 (24GB æ˜¾å­˜)
- CUDA Version: 12.8
- Driver Version: 570.124.04
```

æ£€æµ‹åˆ°çš„ GPU åˆ—è¡¨ï¼š
```bash
$ lspci | grep -i vga
03:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family
4f:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090]
52:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090]
56:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090]
57:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090]
ce:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090]
d1:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090]
d5:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090]
d6:00.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090]
```

### å¯åŠ¨ Ollama æœåŠ¡ï¼ˆè‡ªåŠ¨æ£€æµ‹ GPUï¼‰

Ollama ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å¯ç”¨çš„ GPUï¼Œæ— éœ€ç‰¹æ®Šé…ç½®ï¼š

```bash
# åå°å¯åŠ¨æœåŠ¡
nohup ollama serve > /tmp/ollama.log 2>&1 &

# æŸ¥çœ‹å¯åŠ¨æ—¥å¿—
tail -f /tmp/ollama.log
```

### æŒ‡å®šä½¿ç”¨ç‰¹å®š GPUï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦é™åˆ¶ä½¿ç”¨ç‰¹å®š GPUï¼š

```bash
# åªä½¿ç”¨ GPU 0
CUDA_VISIBLE_DEVICES=0 ollama serve

# ä½¿ç”¨ GPU 0 å’Œ 1
CUDA_VISIBLE_DEVICES=0,1 ollama serve

# ä½¿ç”¨ GPU 0,1,2,3
CUDA_VISIBLE_DEVICES=0,1,2,3 ollama serve

# åå°è¿è¡Œå¹¶æŒ‡å®š GPU
CUDA_VISIBLE_DEVICES=0,1 nohup ollama serve > /tmp/ollama.log 2>&1 &
```

### éªŒè¯ GPU æ”¯æŒ

```bash
# æŸ¥çœ‹ Ollama ç‰ˆæœ¬ä¿¡æ¯
ollama --version

# å¯ç”¨è°ƒè¯•æ¨¡å¼æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
OLLAMA_DEBUG=1 ollama serve
```

---

## ğŸ”§ æœåŠ¡ç®¡ç†

### å¯åŠ¨æœåŠ¡

```bash
# æ–¹å¼ 1: åå°å¯åŠ¨ï¼ˆæ¨èï¼‰
nohup ollama serve > /tmp/ollama.log 2>&1 &

# æ–¹å¼ 2: ä½¿ç”¨ screen ä¿æŒä¼šè¯
screen -dmS ollama ollama serve

# æ–¹å¼ 3: ä½¿ç”¨ tmux ä¿æŒä¼šè¯
tmux new-session -d -s ollama 'ollama serve'

# æ–¹å¼ 4: å‰å°è¿è¡Œï¼ˆè°ƒè¯•ç”¨ï¼‰
ollama serve
```

### æ£€æŸ¥æœåŠ¡çŠ¶æ€

```bash
# æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿è¡Œ
ps aux | grep ollama

# æ£€æŸ¥ API æ˜¯å¦å¯ç”¨
curl http://127.0.0.1:11434

# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹
ollama list

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
tail -50 /tmp/ollama.log
```

### æ¨¡å‹ç®¡ç†

```bash
# æ‹‰å–æ¨¡å‹
ollama pull llama2          # 7B æ¨¡å‹
ollama pull llama2:13b      # 13B æ¨¡å‹
ollama pull llama2:70b      # 70B æ¨¡å‹
ollama pull mistral         # Mistral æ¨¡å‹
ollama pull codellama       # ä»£ç ä¸“ç”¨æ¨¡å‹

# åˆ—å‡ºå·²å®‰è£…æ¨¡å‹
ollama list

# åˆ é™¤æ¨¡å‹
ollama rm llama2

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
ollama show llama2
```

### è¿è¡Œæ¨¡å‹

```bash
# äº¤äº’å¼è¿è¡Œ
ollama run llama2

# å•æ¬¡æŸ¥è¯¢
ollama run llama2 "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ "

# ä½¿ç”¨ä¸­æ–‡æç¤º
ollama run llama2 "è¯·ç”¨ä¸­æ–‡è§£é‡Šä»€ä¹ˆæ˜¯ GraphRAG"

# é€€å‡ºäº¤äº’æ¨¡å¼
/bye
```

---

## ğŸ“Š GPU ç›‘æ§å’ŒéªŒè¯

### å®æ—¶ç›‘æ§ GPU ä½¿ç”¨

æ‰“å¼€ä¸¤ä¸ªç»ˆç«¯çª—å£è¿›è¡Œç›‘æ§ï¼š

**ç»ˆç«¯ 1 - GPU ç›‘æ§**
```bash
# æ¯ç§’åˆ·æ–°ä¸€æ¬¡ GPU çŠ¶æ€
watch -n 1 nvidia-smi

# æˆ–æŒç»­ç›‘æ§
nvidia-smi -l 1

# ç®€åŒ–è¾“å‡º
nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv -l 1
```

**ç»ˆç«¯ 2 - è¿è¡Œæ¨¡å‹**
```bash
# å¯åŠ¨æ¨¡å‹æ¨ç†
ollama run llama2 "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²"
```

### GPU ä½¿ç”¨æŒ‡æ ‡è¯´æ˜

è¿è¡Œæ¨¡å‹æ—¶ï¼Œåœ¨ `nvidia-smi` è¾“å‡ºä¸­è§‚å¯Ÿï¼š

```
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3090        On  |   00000000:D1:00.0 Off |                  N/A |
| 75%   65C    P2            280W /  350W |   15234MiB / 24576MiB |     95%      Default |
+-----------------------------------------+------------------------+----------------------+
```

**å…³é”®æŒ‡æ ‡**ï¼š
- **GPU-Util**: GPU ä½¿ç”¨ç‡ï¼ˆç©ºé—²æ—¶ 0%ï¼Œæ¨ç†æ—¶é€šå¸¸ 80-100%ï¼‰
- **Memory-Usage**: æ˜¾å­˜ä½¿ç”¨é‡ï¼ˆæ¨¡å‹åŠ è½½åä¼šå¢åŠ ï¼‰
- **Temp**: æ¸©åº¦ï¼ˆé€šå¸¸åœ¨ 60-80Â°Cï¼‰
- **Pwr:Usage**: åŠŸè€—ï¼ˆç©ºé—² ~30Wï¼Œæ»¡è½½å¯è¾¾ 350Wï¼‰

### éªŒè¯ GPU åŠ é€Ÿæ˜¯å¦ç”Ÿæ•ˆ

```bash
# 1. æŸ¥çœ‹ ollama è¿›ç¨‹æ˜¯å¦å‡ºç°åœ¨ GPU è¿›ç¨‹åˆ—è¡¨
nvidia-smi | grep ollama

# 2. æ¯”è¾ƒæ¨ç†é€Ÿåº¦
# CPU æ¨¡å¼ä¸‹ï¼šé€šå¸¸å‡ å tokens/ç§’
# GPU æ¨¡å¼ä¸‹ï¼šå¯è¾¾æ•°ç™¾ tokens/ç§’

# 3. è§‚å¯Ÿæ˜¾å­˜ä½¿ç”¨
# 7B æ¨¡å‹ï¼šçº¦ 4-8GB æ˜¾å­˜
# 13B æ¨¡å‹ï¼šçº¦ 8-16GB æ˜¾å­˜
# 70B æ¨¡å‹ï¼šçº¦ 40-70GB æ˜¾å­˜ï¼ˆå¯èƒ½éœ€è¦å¤šå¡ï¼‰
```

---

## ğŸ” å®Œæ•´æ“ä½œæµç¨‹

### åˆæ¬¡ä½¿ç”¨æµç¨‹

```bash
# 1. åœæ­¢æ—§æœåŠ¡ï¼ˆå¦‚æœæœ‰ï¼‰
pkill ollama

# 2. å¯åŠ¨æœåŠ¡
nohup ollama serve > /tmp/ollama.log 2>&1 &

# 3. ç­‰å¾…å‡ ç§’ï¼ŒæŸ¥çœ‹å¯åŠ¨æ—¥å¿—
sleep 3
tail -20 /tmp/ollama.log

# 4. æµ‹è¯• API è¿æ¥
curl http://127.0.0.1:11434/api/tags

# 5. æ‹‰å–æ¨¡å‹
ollama pull llama2

# 6. æ‰“å¼€ GPU ç›‘æ§ï¼ˆæ–°ç»ˆç«¯ï¼‰
watch -n 1 nvidia-smi

# 7. è¿è¡Œæ¨¡å‹æµ‹è¯•
ollama run llama2 "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"

# 8. è§‚å¯Ÿ GPU ä½¿ç”¨æƒ…å†µ
```

### åœ¨ GraphRAG ä¸­ä½¿ç”¨ Ollama

é…ç½® GraphRAG ä½¿ç”¨æœ¬åœ° Ollamaï¼š

```yaml
# settings.yaml é…ç½®ç¤ºä¾‹
llm:
  type: openai_chat
  api_base: http://127.0.0.1:11434/v1  # Ollama å…¼å®¹ OpenAI API
  model: llama2
  api_key: ollama  # éšæ„å¡«å†™ï¼ŒOllama ä¸éªŒè¯

embeddings:
  type: openai_embedding
  api_base: http://127.0.0.1:11434/v1
  model: llama2
  api_key: ollama
```

æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š

```bash
export OLLAMA_HOST=http://127.0.0.1:11434
export OPENAI_API_BASE=http://127.0.0.1:11434/v1
export OPENAI_API_KEY=ollama
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: æœåŠ¡å¯åŠ¨å curl æ— å“åº”

**é—®é¢˜**ï¼š
```bash
$ curl http://127.0.0.1:11434
# æ²¡æœ‰è¾“å‡º
```

**åŸå› **: æœåŠ¡å¯èƒ½è¿˜åœ¨å¯åŠ¨ä¸­

**è§£å†³**ï¼š
```bash
# 1. æŸ¥çœ‹æ—¥å¿—
tail -f /tmp/ollama.log

# 2. ç­‰å¾…å‡ ç§’åé‡è¯•
sleep 5
curl http://127.0.0.1:11434

# 3. æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
ps aux | grep ollama
```

### Q2: æ¨¡å‹æ¨ç†é€Ÿåº¦å¾ˆæ…¢

**åŸå› **: å¯èƒ½æ²¡æœ‰ä½¿ç”¨ GPU æˆ– GPU é…ç½®ä¸å½“

**æ’æŸ¥**ï¼š
```bash
# 1. è¿è¡Œæ¨¡å‹æ—¶æ£€æŸ¥ GPU ä½¿ç”¨
nvidia-smi

# 2. æŸ¥çœ‹æ˜¯å¦æœ‰é”™è¯¯æ—¥å¿—
tail -100 /tmp/ollama.log | grep -i error

# 3. ç¡®è®¤ CUDA å¯ç”¨
python3 -c "import torch; print(torch.cuda.is_available())"

# 4. é‡å¯æœåŠ¡
pkill ollama
nohup ollama serve > /tmp/ollama.log 2>&1 &
```

### Q3: æ˜¾å­˜ä¸è¶³ (Out of Memory)

**åŸå› **: æ¨¡å‹å¤ªå¤§æˆ–å¤šä¸ªæ¨¡å‹åŒæ—¶åŠ è½½

**è§£å†³**ï¼š
```bash
# 1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull llama2:7b  # è€Œä¸æ˜¯ 70b

# 2. æŒ‡å®šä½¿ç”¨å¤šå¼  GPU
CUDA_VISIBLE_DEVICES=0,1,2,3 ollama serve

# 3. æŸ¥çœ‹å½“å‰æ˜¾å­˜ä½¿ç”¨
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# 4. åœæ­¢å…¶ä»– GPU è¿›ç¨‹
# æ‰¾åˆ°å ç”¨ GPU çš„è¿›ç¨‹å¹¶åœæ­¢
```

### Q4: æœåŠ¡è«ååœæ­¢

**åŸå› **: å¯èƒ½è¢«ç³»ç»Ÿæ€æ­»æˆ–å‡ºç°é”™è¯¯

**æ’æŸ¥**ï¼š
```bash
# 1. æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—
dmesg | tail -50

# 2. æŸ¥çœ‹ Ollama æ—¥å¿—
tail -100 /tmp/ollama.log

# 3. ä½¿ç”¨ screen æˆ– tmux ä¿æŒæœåŠ¡
screen -dmS ollama ollama serve

# 4. æ·»åŠ è‡ªåŠ¨é‡å¯è„šæœ¬
cat > /root/start_ollama.sh << 'EOF'
#!/bin/bash
while true; do
    if ! pgrep -x "ollama" > /dev/null; then
        echo "Ollama stopped, restarting..."
        nohup ollama serve > /tmp/ollama.log 2>&1 &
    fi
    sleep 60
done
EOF
chmod +x /root/start_ollama.sh
nohup /root/start_ollama.sh &
```

### Q5: å¦‚ä½•ä½¿ç”¨ç‰¹å®šçš„ GPU

**éœ€æ±‚**: åœ¨å¤š GPU ç³»ç»Ÿä¸­æŒ‡å®šä½¿ç”¨ç‰¹å®šæ˜¾å¡

**æ–¹æ³•**ï¼š
```bash
# åªä½¿ç”¨ç¬¬ä¸€å¼  RTX 3090 (GPU 0)
CUDA_VISIBLE_DEVICES=0 ollama serve

# ä½¿ç”¨ GPU 0,2,4,6ï¼ˆå¶æ•°å¡ï¼‰
CUDA_VISIBLE_DEVICES=0,2,4,6 ollama serve

# ä½¿ç”¨æ‰€æœ‰ GPU é™¤äº† GPU 7
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6 ollama serve
```

### Q6: å¦‚ä½•åœ¨ Python ä¸­è°ƒç”¨ Ollama

```python
import requests
import json

def query_ollama(prompt, model="llama2"):
    url = "http://127.0.0.1:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    response = requests.post(url, json=payload)
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
result = query_ollama("è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ ")
print(result['response'])
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- Ollama å®˜ç½‘: https://ollama.ai
- GitHub: https://github.com/ollama/ollama
- æ¨¡å‹åº“: https://ollama.ai/library

### æ¨èæ¨¡å‹

| æ¨¡å‹åç§° | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|---------|--------|---------|---------|
| llama2 | 7B | ~4GB | é€šç”¨å¯¹è¯ã€é—®ç­” |
| llama2:13b | 13B | ~8GB | æ›´é«˜è´¨é‡å¯¹è¯ |
| llama2:70b | 70B | ~40GB | ä¸“ä¸šä»»åŠ¡ã€å¤æ‚æ¨ç† |
| mistral | 7B | ~4GB | é«˜æ•ˆæ¨ç†ã€ä»£ç ç”Ÿæˆ |
| codellama | 7B | ~4GB | ä»£ç ç”Ÿæˆã€ç¼–ç¨‹åŠ©æ‰‹ |
| yi:34b | 34B | ~20GB | ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ |
| qwen:14b | 14B | ~9GB | ä¸­æ–‡ä¸“ç”¨æ¨¡å‹ |

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

**å¯¹äºæ‚¨çš„ 8Ã—RTX 3090 é…ç½®**ï¼š
1. å¯ä»¥åŒæ—¶è¿è¡Œå¤šä¸ªå°æ¨¡å‹ï¼ˆ7B/13Bï¼‰
2. æˆ–è¿è¡Œä¸€ä¸ªå¤§æ¨¡å‹ï¼ˆ70Bï¼‰ä½¿ç”¨å¤šå¡
3. å»ºè®®ä¸ºä¸åŒä»»åŠ¡åˆ†é…ä¸åŒ GPU
4. ä½¿ç”¨ `CUDA_VISIBLE_DEVICES` éš”ç¦» GPU èµ„æº

---

## ğŸ“ ç»´æŠ¤å»ºè®®

### å®šæœŸç»´æŠ¤ä»»åŠ¡

```bash
# 1. æ›´æ–° Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. æ¸…ç†æœªä½¿ç”¨çš„æ¨¡å‹
ollama list
ollama rm <unused-model>

# 3. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
ls -lh /tmp/ollama.log

# 4. è½®è½¬æ—¥å¿—
mv /tmp/ollama.log /tmp/ollama.log.old
pkill ollama
nohup ollama serve > /tmp/ollama.log 2>&1 &

# 5. ç›‘æ§ GPU å¥åº·çŠ¶æ€
nvidia-smi -q
```

### æ€§èƒ½ç›‘æ§è„šæœ¬

```bash
# åˆ›å»ºç›‘æ§è„šæœ¬
cat > /root/monitor_ollama.sh << 'EOF'
#!/bin/bash
echo "=== Ollama æœåŠ¡çŠ¶æ€ ==="
if pgrep -x "ollama" > /dev/null; then
    echo "âœ… Ollama æœåŠ¡è¿è¡Œä¸­"
    echo "PID: $(pgrep -x ollama)"
else
    echo "âŒ Ollama æœåŠ¡æœªè¿è¡Œ"
fi

echo -e "\n=== GPU ä½¿ç”¨æƒ…å†µ ==="
nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv

echo -e "\n=== Ollama æ¨¡å‹åˆ—è¡¨ ==="
ollama list

echo -e "\n=== ç«¯å£ç›‘å¬çŠ¶æ€ ==="
netstat -tulpn | grep 11434
EOF

chmod +x /root/monitor_ollama.sh

# è¿è¡Œç›‘æ§
/root/monitor_ollama.sh
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-02  
**é€‚ç”¨ç¯å¢ƒ**: Ubuntu 22.04, CUDA 12.8, 8Ã—RTX 3090  
**ä½œè€…**: AI Assistant 