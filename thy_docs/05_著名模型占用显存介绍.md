# Ollama 著名模型显存占用统计分析

## 📋 目录

1. [按显存需求分类](#按显存需求分类)
2. [主流模型详细对比](#主流模型详细对比)
3. [单卡配置方案](#单卡配置方案)
4. [多卡配置方案](#多卡配置方案)
5. [性能与显存平衡](#性能与显存平衡)
6. [模型选择建议](#模型选择建议)

---

## 📊 按显存需求分类

### 超轻量级 (< 2GB)

| 模型名称 | 参数量 | 显存需求 | 推理速度 | 适用场景 |
|---------|--------|---------|---------|---------|
| **tinyllama** | 1.1B | 0.6-1GB | ⚡⚡⚡⚡⚡ | 快速响应、测试、边缘设备 |
| **phi** | 2.7B | 1.5-2GB | ⚡⚡⚡⚡⚡ | 轻量对话、问答 |
| **gemma:2b** | 2B | 1.5-2GB | ⚡⚡⚡⚡⚡ | Google 轻量模型 |
| **qwen:1.8b** | 1.8B | 1-1.5GB | ⚡⚡⚡⚡⚡ | 中文轻量对话 |

**特点**：
- ✅ 可在低端显卡（GTX 1060 6GB）运行
- ✅ 响应速度极快（50-100+ tokens/s）
- ⚠️ 能力有限，适合简单任务

---

### 轻量级 (2-6GB)

| 模型名称 | 参数量 | 显存需求 | 推理速度 | 适用场景 |
|---------|--------|---------|---------|---------|
| **yi:6b** | 6B | 3-4GB | ⚡⚡⚡⚡⚡ | 中文对话、写作 |
| **gemma:7b** | 7B | 4-5GB | ⚡⚡⚡⚡ | Google 通用模型 |
| **stablelm** | 7B | 4-5GB | ⚡⚡⚡⚡ | 稳定性优化对话 |
| **mistral** | 7B | 4-5GB | ⚡⚡⚡⚡⚡ | 高效推理、质量好 |
| **llama2:7b** | 7B | 4-5GB | ⚡⚡⚡⚡⚡ | Meta 通用对话 |
| **llama3:8b** | 8B | 4.5-5.5GB | ⚡⚡⚡⚡⚡ | Meta 最新版本 |
| **qwen2:7b** | 7B | 4-5GB | ⚡⚡⚡⚡⚡ | 阿里中文优化 |
| **codellama:7b** | 7B | 4-5GB | ⚡⚡⚡⚡⚡ | 代码生成 |
| **deepseek-coder:6.7b** | 6.7B | 4-5GB | ⚡⚡⚡⚡⚡ | DeepSeek 代码专用 |

**特点**：
- ✅ GTX 1080 8GB / RTX 3060 12GB 可流畅运行
- ✅ 性能与速度平衡良好
- ✅ 适合日常使用和开发测试

---

### 中量级 (6-12GB)

| 模型名称 | 参数量 | 显存需求 | 推理速度 | 适用场景 |
|---------|--------|---------|---------|---------|
| **llama2:13b** | 13B | 7-9GB | ⚡⚡⚡⚡ | 高质量对话 |
| **llama3:13b** | 13B | 8-10GB | ⚡⚡⚡⚡ | Meta 最新 13B |
| **codellama:13b** | 13B | 7-9GB | ⚡⚡⚡⚡ | 高质量代码生成 |
| **vicuna:13b** | 13B | 7-9GB | ⚡⚡⚡⚡ | 对话优化版本 |
| **qwen2:14b** | 14B | 8-10GB | ⚡⚡⚡⚡ | 阿里中文高质量 |
| **mistral:7b-instruct** | 7B | 5-6GB | ⚡⚡⚡⚡⚡ | 指令微调版本 |

**特点**：
- ✅ RTX 3060 12GB / RTX 4060 Ti 16GB 推荐
- ✅ 质量明显提升
- ✅ 适合生产环境使用

---

### 重量级 (12-24GB)

| 模型名称 | 参数量 | 显存需求 | 推理速度 | 单张 3090 |
|---------|--------|---------|---------|----------|
| **yi:34b** | 34B | 18-21GB | ⚡⚡⚡ | ✅ 可以 |
| **codellama:34b** | 34B | 19-22GB | ⚡⚡⚡ | ✅ 可以 |
| **qwen2:32b** | 32B | 17-20GB | ⚡⚡⚡ | ✅ 可以 |
| **qwen3:30b** | 30B | 17-20GB | ⚡⚡⚡ | ✅ 可以 |
| **deepseek-r1:32b** | 32B | 18-22GB | ⚡⚡⚡ | ✅ 可以 |
| **solar:10.7b** | 10.7B | 6-8GB | ⚡⚡⚡⚡ | ✅ 可以 |
| **mixtral:8x7b** | 47B | 26-30GB | ⚡⚡⚡ | ❌ 超出 |

**特点**：
- ✅ RTX 3090 / RTX 4090 24GB 可运行（除 mixtral）
- ✅ 专业级别性能
- ⚠️ 是单卡能运行的极限

---

### 超重量级 (> 24GB)

| 模型名称 | 参数量 | 显存需求 | 推理速度 | 最少显卡数 |
|---------|--------|---------|---------|----------|
| **llama2:70b** | 70B | 39-42GB | ⚡⚡ | 2 × 3090 |
| **llama3:70b** | 70B | 40-43GB | ⚡⚡ | 2 × 3090 |
| **codellama:70b** | 70B | 39-42GB | ⚡⚡ | 2 × 3090 |
| **qwen2:72b** | 72B | 40-45GB | ⚡⚡ | 2 × 3090 |
| **mixtral:8x7b** | 47B | 26-30GB | ⚡⚡⚡ | 2 × 3090 |
| **mixtral:8x22b** | 141B | 80-90GB | ⚡⚡ | 4 × 3090 |

**特点**：
- ✅ 需要多卡或 A100 等专业卡
- ✅ 最高质量输出
- ⚠️ 推理速度较慢
- ⚠️ 成本高

---

## 🔍 主流模型详细对比

### 1. Llama 系列（Meta）

| 版本 | 参数 | 显存 | 特点 | 推荐度 |
|------|------|------|------|--------|
| llama3:8b | 8B | 4.5-5.5GB | 最新版，性能提升 | ⭐⭐⭐⭐⭐ |
| llama2:7b | 7B | 4-5GB | 稳定可靠，广泛使用 | ⭐⭐⭐⭐⭐ |
| llama2:13b | 13B | 7-9GB | 平衡性能 | ⭐⭐⭐⭐⭐ |
| llama2:70b | 70B | 39-42GB | 旗舰级性能 | ⭐⭐⭐⭐ |
| llama3:70b | 70B | 40-43GB | 最新旗舰 | ⭐⭐⭐⭐⭐ |

**推荐场景**：
- 通用对话、问答
- 文本生成和摘要
- 多语言任务
- 知识问答

---

### 2. Qwen 系列（阿里巴巴）

| 版本 | 参数 | 显存 | 特点 | 中文能力 |
|------|------|------|------|---------|
| qwen3:30b | 30B | 17-20GB | 最新版，中文最强 | ⭐⭐⭐⭐⭐ |
| qwen2:72b | 72B | 40-45GB | 旗舰中文模型 | ⭐⭐⭐⭐⭐ |
| qwen2:32b | 32B | 17-20GB | 高性能中文 | ⭐⭐⭐⭐⭐ |
| qwen2:14b | 14B | 8-10GB | 平衡中文模型 | ⭐⭐⭐⭐⭐ |
| qwen2:7b | 7B | 4-5GB | 轻量中文 | ⭐⭐⭐⭐ |
| qwen:1.8b | 1.8B | 1-1.5GB | 超轻量中文 | ⭐⭐⭐ |

**推荐场景**：
- 中文对话和写作（首选）
- 中英文翻译
- 中文代码注释
- 中文文档理解

---

### 3. CodeLlama 系列（Meta 代码专用）

| 版本 | 参数 | 显存 | 代码能力 | 推荐度 |
|------|------|------|---------|--------|
| codellama:70b | 70B | 39-42GB | 专业级代码 | ⭐⭐⭐⭐⭐ |
| codellama:34b | 34B | 19-22GB | 高质量代码 | ⭐⭐⭐⭐⭐ |
| codellama:13b | 13B | 7-9GB | 平衡代码生成 | ⭐⭐⭐⭐ |
| codellama:7b | 7B | 4-5GB | 快速代码生成 | ⭐⭐⭐⭐ |

**支持语言**：
- Python, JavaScript, TypeScript
- Java, C++, C#
- Go, Rust, PHP
- SQL, Shell, 等

---

### 4. Mistral / Mixtral 系列

| 版本 | 参数 | 显存 | 特点 | 推荐度 |
|------|------|------|------|--------|
| mixtral:8x22b | 141B | 80-90GB | 超大 MoE 模型 | ⭐⭐⭐⭐ |
| mixtral:8x7b | 47B | 26-30GB | MoE 架构，高效 | ⭐⭐⭐⭐⭐ |
| mistral:7b | 7B | 4-5GB | 高效推理 | ⭐⭐⭐⭐⭐ |
| mistral:7b-instruct | 7B | 5-6GB | 指令微调版 | ⭐⭐⭐⭐⭐ |

**特点**：
- MoE (Mixture of Experts) 架构
- 推理效率高
- 质量/速度平衡好

---

### 5. DeepSeek 系列

| 版本 | 参数 | 显存 | 特点 | 推荐度 |
|------|------|------|------|--------|
| deepseek-r1:70b | 70B | 39-42GB | 推理优化版本 | ⭐⭐⭐⭐⭐ |
| deepseek-r1:32b | 32B | 18-22GB | 高性能推理 | ⭐⭐⭐⭐⭐ |
| deepseek-r1:14b | 14B | 8-10GB | 轻量推理 | ⭐⭐⭐⭐ |
| deepseek-coder:33b | 33B | 18-21GB | 代码专用 | ⭐⭐⭐⭐⭐ |
| deepseek-coder:6.7b | 6.7B | 4-5GB | 轻量代码 | ⭐⭐⭐⭐ |

**推荐场景**：
- 复杂推理任务
- 数学问题解决
- 代码生成和调试
- 算法设计

---

### 6. Yi 系列（零一万物）

| 版本 | 参数 | 显存 | 中文能力 | 推荐度 |
|------|------|------|---------|--------|
| yi:34b | 34B | 18-21GB | 优秀 | ⭐⭐⭐⭐⭐ |
| yi:6b | 6B | 3-4GB | 良好 | ⭐⭐⭐⭐ |

**特点**：
- 中文理解能力强
- 支持长上下文（200K+）
- 多任务性能好

---

### 7. Gemma 系列（Google）

| 版本 | 参数 | 显存 | 特点 | 推荐度 |
|------|------|------|------|--------|
| gemma:7b | 7B | 4-5GB | Google 开源 | ⭐⭐⭐⭐ |
| gemma:2b | 2B | 1.5-2GB | 超轻量 | ⭐⭐⭐ |

**特点**：
- Google 技术支持
- 安全性优化
- 多语言支持

---

## 💻 单卡配置方案

### RTX 3060 12GB 配置

#### 方案 A: 单模型高质量
```bash
# 推荐: Qwen2 14B（中文最优）
ollama pull qwen2:14b
# 显存: 8-10GB
# 剩余: 2-4GB

# 或: Llama2 13B（通用最优）
ollama pull llama2:13b
# 显存: 7-9GB
# 剩余: 3-5GB
```

#### 方案 B: 多模型并行
```bash
# 小模型 1: Qwen 7B (中文)
ollama run qwen2:7b &    # 4-5GB

# 小模型 2: CodeLlama 7B (代码)
ollama run codellama:7b &  # 4-5GB

# 总计: 8-10GB，剩余: 2-4GB
```

---

### RTX 3090 / RTX 4090 24GB 配置

#### 方案 A: 单个大模型（推荐）⭐⭐⭐⭐⭐
```bash
# 首选: Qwen3 30B（单卡最强中文）
export CUDA_VISIBLE_DEVICES=0
ollama pull qwen3:30b
ollama run qwen3:30b
# 显存: 17-20GB
# 剩余: 4-7GB
# 性能: ⭐⭐⭐⭐⭐
```

#### 方案 B: Yi 34B（长上下文）
```bash
export CUDA_VISIBLE_DEVICES=0
ollama pull yi:34b
ollama run yi:34b
# 显存: 18-21GB
# 剩余: 3-6GB
# 特点: 支持 200K 上下文
```

#### 方案 C: CodeLlama 34B（代码专家）
```bash
export CUDA_VISIBLE_DEVICES=0
ollama pull codellama:34b
ollama run codellama:34b
# 显存: 19-22GB
# 剩余: 2-5GB
# 特点: 代码生成最强
```

#### 方案 D: DeepSeek-R1 32B（推理专家）
```bash
export CUDA_VISIBLE_DEVICES=0
ollama pull deepseek-r1:32b
ollama run deepseek-r1:32b
# 显存: 18-22GB
# 剩余: 2-6GB
# 特点: 数学和推理能力强
```

#### 方案 E: 多模型混合
```bash
# 中等模型 + 小模型组合

# GPU 负载 1: Qwen2 14B (中文)
ollama run qwen2:14b &      # 8-10GB

# GPU 负载 2: CodeLlama 7B (代码)
ollama run codellama:7b &   # 4-5GB

# GPU 负载 3: Mistral 7B (通用)
ollama run mistral:7b &     # 4-5GB

# 总计: 16-20GB
# 剩余: 4-8GB
```

---

### RTX A6000 48GB / A100 40GB 配置

#### 方案 A: 单个超大模型
```bash
# Llama3 70B
export CUDA_VISIBLE_DEVICES=0
ollama pull llama3:70b
ollama run llama3:70b
# 显存: 40-43GB (A100 40GB 紧张)
# 显存: 40-43GB (A6000 48GB 充足)
```

#### 方案 B: 多个大模型
```bash
# 30B + 14B 组合
ollama run qwen3:30b &      # 17-20GB
ollama run codellama:13b &  # 7-9GB
# 总计: 24-29GB
# 剩余: 11-16GB (A6000) / 11-16GB (A100)
```

---

## 🎯 多卡配置方案

### 2 × RTX 3090 (48GB 总显存)

#### 配置 A: 单个 70B 模型 ⭐⭐⭐⭐⭐
```bash
# 跨卡运行 Llama3 70B
export CUDA_VISIBLE_DEVICES=0,1
ollama serve &
ollama pull llama3:70b
ollama run llama3:70b

# 显存分配:
# GPU 0: 20-22GB
# GPU 1: 19-21GB
# 总计: 40-43GB
```

#### 配置 B: 两个独立 30B 模型
```bash
# GPU 0: Qwen3 30B (中文服务)
CUDA_VISIBLE_DEVICES=0 ollama serve -p 11434 &
ollama pull qwen3:30b

# GPU 1: CodeLlama 34B (代码服务)
CUDA_VISIBLE_DEVICES=1 ollama serve -p 11435 &
ollama pull codellama:34b

# 每卡: 18-22GB
# 高可用: 两个服务独立运行
```

#### 配置 C: 一大两小
```bash
# GPU 0: Qwen3 30B
CUDA_VISIBLE_DEVICES=0 ollama serve -p 11434 &
# 显存: 18-20GB, 剩余: 4-6GB

# GPU 1: 多个小模型
CUDA_VISIBLE_DEVICES=1 ollama serve -p 11435 &
# - llama2:13b (7-9GB)
# - mistral:7b (4-5GB)
# - codellama:7b (4-5GB)
# 总计: 15-19GB, 剩余: 5-9GB
```

---

### 4 × RTX 3090 (96GB 总显存)

#### 配置 A: 混合部署（推荐）
```bash
# GPU 0-1: Llama3 70B (旗舰服务)
CUDA_VISIBLE_DEVICES=0,1 ollama serve -p 11434 &

# GPU 2: Qwen3 30B (中文服务)
CUDA_VISIBLE_DEVICES=2 ollama serve -p 11435 &

# GPU 3: CodeLlama 34B (代码服务)
CUDA_VISIBLE_DEVICES=3 ollama serve -p 11436 &

# 总计: 40GB + 18GB + 20GB = 78GB
# 剩余: 18GB 可用于扩展
```

#### 配置 B: 多实例高可用
```bash
# 两个 70B 实例（负载均衡）
CUDA_VISIBLE_DEVICES=0,1 ollama serve -p 11434 &  # 实例 1
CUDA_VISIBLE_DEVICES=2,3 ollama serve -p 11435 &  # 实例 2

# 通过负载均衡器分发请求
# 高并发场景，双倍吞吐量
```

---

### 8 × RTX 3090 (192GB 总显存)

#### 配置 A: 全能部署（推荐）⭐⭐⭐⭐⭐
```bash
# GPU 0-1: Llama3 70B (通用旗舰)
CUDA_VISIBLE_DEVICES=0,1 ollama serve -p 11434 &

# GPU 2-3: Qwen2 72B (中文旗舰)
CUDA_VISIBLE_DEVICES=2,3 ollama serve -p 11435 &

# GPU 4: Qwen3 30B (快速中文)
CUDA_VISIBLE_DEVICES=4 ollama serve -p 11436 &

# GPU 5: CodeLlama 34B (代码专家)
CUDA_VISIBLE_DEVICES=5 ollama serve -p 11437 &

# GPU 6: DeepSeek-R1 32B (推理专家)
CUDA_VISIBLE_DEVICES=6 ollama serve -p 11438 &

# GPU 7: 预留/动态分配
# - 用于实验新模型
# - 或运行多个小模型
# - 或作为备份容量

# 总计: 40GB + 42GB + 18GB + 20GB + 19GB = 139GB
# 剩余: 53GB 可用
```

#### 配置 B: 高并发服务
```bash
# 4 个 Llama3 70B 实例（超高并发）
CUDA_VISIBLE_DEVICES=0,1 ollama serve -p 11434 &  # 实例 1
CUDA_VISIBLE_DEVICES=2,3 ollama serve -p 11435 &  # 实例 2
CUDA_VISIBLE_DEVICES=4,5 ollama serve -p 11436 &  # 实例 3
CUDA_VISIBLE_DEVICES=6,7 ollama serve -p 11437 &  # 实例 4

# 配合负载均衡
# 并发能力: 4× 标准吞吐量
```

#### 配置 C: 研究实验室配置
```bash
# GPU 0-3: 大模型训练/微调区
CUDA_VISIBLE_DEVICES=0,1,2,3  # 96GB 可用

# GPU 4-5: 推理服务区
CUDA_VISIBLE_DEVICES=4,5 ollama serve -p 11434 &

# GPU 6-7: 开发测试区
# 多个小模型快速实验
```

---

## ⚖️ 性能与显存平衡分析

### 性能提升曲线

```
模型性能提升 vs 参数量：

100% ┤                                    ● 70B
     │                              ●
 90% ┤                         ● 34B
     │                    ●
 80% ┤               ● 14B
     │          ●
 70% ┤     ● 7B
     │ ●
 60% ┤
     └──────────────────────────────────
       1B   7B   14B  34B  70B  参数量

性能提升边际递减：
- 1B → 7B:  提升 ~40%
- 7B → 14B: 提升 ~15%
- 14B → 34B: 提升 ~12%
- 34B → 70B: 提升 ~10%
```

### 推理速度对比（RTX 3090）

| 模型大小 | Tokens/秒 | 相对速度 | 首字延迟 |
|---------|-----------|---------|---------|
| 1-2B | 80-150 | ⚡⚡⚡⚡⚡ | < 0.5s |
| 7B | 40-80 | ⚡⚡⚡⚡⚡ | 0.5-1s |
| 13-14B | 25-50 | ⚡⚡⚡⚡ | 1-2s |
| 30-34B | 15-30 | ⚡⚡⚡ | 2-3s |
| 70B (单卡不可) | - | - | - |
| 70B (双卡) | 8-18 | ⚡⚡ | 3-5s |

### 性价比分析

| 模型规模 | 质量评分 | 速度评分 | 性价比 | 推荐场景 |
|---------|---------|---------|--------|---------|
| 7B | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 开发、测试、快速响应 |
| 13-14B | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 生产环境首选 |
| 30-34B | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | 高质量要求场景 |
| 70B+ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | 专业/科研场景 |

---

## 🎯 模型选择建议

### 按使用场景选择

#### 1. 通用对话和问答

**预算充足**：
```bash
# 首选: Llama3 70B (双卡)
CUDA_VISIBLE_DEVICES=0,1 ollama run llama3:70b

# 次选: Qwen3 30B (单卡)
ollama run qwen3:30b
```

**预算有限**：
```bash
# 首选: Llama2 13B
ollama run llama2:13b

# 次选: Mistral 7B
ollama run mistral:7b
```

---

#### 2. 中文任务

**最强中文** (推荐顺序)：
```bash
1. qwen3:30b      # 单卡最强 (18-20GB)
2. qwen2:72b      # 旗舰中文 (40-45GB, 需双卡)
3. qwen2:14b      # 平衡选择 (8-10GB)
4. yi:34b         # 长上下文 (18-21GB)
5. qwen2:7b       # 轻量快速 (4-5GB)
```

---

#### 3. 代码生成

**代码能力排名**：
```bash
1. codellama:70b       # 最强 (39-42GB, 需双卡)
2. codellama:34b       # 单卡最强 (19-22GB)
3. deepseek-coder:33b  # DeepSeek 专业 (18-21GB)
4. codellama:13b       # 平衡 (7-9GB)
5. codellama:7b        # 快速 (4-5GB)
```

---

#### 4. 数学和推理

**推理能力排名**：
```bash
1. deepseek-r1:70b     # 最强推理 (39-42GB, 需双卡)
2. deepseek-r1:32b     # 单卡推理 (18-22GB)
3. llama3:70b          # 通用强推理 (40-43GB, 需双卡)
4. qwen3:30b           # 中文推理 (17-20GB)
5. deepseek-r1:14b     # 快速推理 (8-10GB)
```

---

#### 5. 多语言翻译

**推荐组合**：
```bash
# 主力: Qwen3 30B (中英互译最强)
ollama run qwen3:30b

# 备选: Llama3 8B (多语言通用)
ollama run llama3:8b
```

---

#### 6. 文档理解和摘要

**长文档处理**：
```bash
# 首选: Yi 34B (200K 上下文)
ollama run yi:34b

# 次选: Qwen3 30B (32K 上下文)
ollama run qwen3:30b
```

---

### 按显卡配置选择

#### 单张 RTX 3060 12GB
```bash
最佳选择: qwen2:14b 或 llama2:13b
备选: mistral:7b, codellama:7b
```

#### 单张 RTX 3090 24GB
```bash
最佳选择: qwen3:30b (中文)
备选: yi:34b, codellama:34b, deepseek-r1:32b
极限: 可运行任何 34B 以下模型
```

#### 2 × RTX 3090 48GB
```bash
最佳选择: llama3:70b 或 qwen2:72b
多服务: 两个 30B 模型独立运行
混合: 一个 70B + 多个小模型
```

#### 4 × RTX 3090 96GB
```bash
推荐配置:
- 1 × 70B (旗舰服务)
- 1 × 30B (快速服务)
- 2 × 预留/多小模型
```

#### 8 × RTX 3090 192GB
```bash
全能配置:
- 2 × 70B 模型 (80GB)
- 2 × 30B 模型 (40GB)
- 多个小模型服务
- 剩余用于实验
```

---

### 按开发阶段选择

#### 原型开发阶段
```bash
# 优先速度，使用小模型快速迭代
推荐: llama2:7b, mistral:7b, qwen2:7b
显存: 4-5GB
速度: 40-80 tokens/s
```

#### 测试验证阶段
```bash
# 平衡质量和速度
推荐: llama2:13b, qwen2:14b
显存: 7-10GB
速度: 25-50 tokens/s
```

#### 生产部署阶段
```bash
# 优先质量
单卡: qwen3:30b, yi:34b
双卡: llama3:70b, qwen2:72b
显存: 18-45GB
速度: 15-30 tokens/s (单卡) / 8-18 tokens/s (双卡)
```

---

## 📊 快速决策表

### 我应该选择哪个模型？

```
问题 1: 您的主要任务是什么？
├─ 中文任务 → qwen3:30b (单卡) 或 qwen2:72b (双卡)
├─ 代码生成 → codellama:34b (单卡) 或 codellama:70b (双卡)
├─ 数学推理 → deepseek-r1:32b (单卡) 或 deepseek-r1:70b (双卡)
└─ 通用对话 → llama3:8b (快) 或 llama3:70b (质量)

问题 2: 您有多少显存？
├─ 6-8GB   → llama2:7b, mistral:7b, qwen2:7b
├─ 12GB    → llama2:13b, qwen2:14b
├─ 24GB    → qwen3:30b, yi:34b, codellama:34b
├─ 48GB    → llama3:70b, qwen2:72b
└─ 96GB+   → 多模型并行部署

问题 3: 您更看重什么？
├─ 速度 → 选择 7B 模型
├─ 质量 → 选择 30B+ 模型
└─ 平衡 → 选择 13-14B 模型

问题 4: 是否需要长上下文？
├─ 是 (> 32K) → yi:34b (200K)
├─ 中等 (16-32K) → qwen3:30b, llama3 系列
└─ 否 (< 8K) → 任意模型
```

---

## 🔧 优化建议

### 1. 显存优化技巧

```bash
# 减少上下文窗口
# 在 Modelfile 中设置
PARAMETER num_ctx 2048  # 默认通常是 4096

# 批处理优化
PARAMETER num_batch 512

# 减少并行请求
PARAMETER num_parallel 1
```

### 2. 多模型负载均衡

```bash
# 使用 Nginx 做负载均衡
upstream ollama_backends {
    server 127.0.0.1:11434;  # GPU 0
    server 127.0.0.1:11435;  # GPU 1
    server 127.0.0.1:11436;  # GPU 2
}

server {
    listen 8080;
    location / {
        proxy_pass http://ollama_backends;
    }
}
```

### 3. 监控脚本

```bash
#!/bin/bash
# 保存为 /root/monitor_all_models.sh

watch -n 1 '
echo "=== GPU 使用情况 ==="
nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu,temperature.gpu \
  --format=csv,noheader | \
  awk -F, "{printf \"GPU %s: %6s/%6s  利用率:%4s  温度:%4s\n\", \$1, \$3, \$4, \$5, \$6}"

echo ""
echo "=== Ollama 进程 ==="
ps aux | grep ollama | grep -v grep | \
  awk "{printf \"PID: %s  CPU: %s%%  MEM: %s%%\n\", \$2, \$3, \$4}"

echo ""
echo "=== 端口监听 ==="
netstat -tulpn | grep ollama | awk "{print \$4, \$7}"
'
```

---

## 📚 总结

### 核心要点

1. **7B 模型**: 开发测试首选，速度快，成本低
2. **13-14B 模型**: 生产环境平衡选择
3. **30-34B 模型**: 单卡 (24GB) 的最佳质量选择
4. **70B 模型**: 需要双卡，最高质量

### 推荐配置

| 显存 | 开发环境 | 生产环境 | 旗舰配置 |
|------|---------|---------|---------|
| 12GB | llama2:7b | llama2:13b | - |
| 24GB | mistral:7b | qwen3:30b | yi:34b |
| 48GB | qwen2:14b | llama3:70b | qwen2:72b |
| 96GB+ | 多模型 | 多 70B 实例 | 全能部署 |

### 最佳实践

✅ **单张 3090 用户**: `qwen3:30b` (中文) 或 `codellama:34b` (代码)
✅ **双 3090 用户**: `llama3:70b` (通用) 或 `qwen2:72b` (中文)
✅ **多卡用户**: 混合部署，按任务分配 GPU
✅ **预算有限**: `llama2:13b` 或 `mistral:7b`

---

**文档版本**: v1.0  
**最后更新**: 2025-10-02  
**数据来源**: Ollama 官方库 + 实测数据  
**作者**: AI Assistant
