# vLLM KV Cache æ˜¾å­˜ä¸è¶³é”™è¯¯è§£å†³æ–¹æ¡ˆ

## ğŸ“‹ é”™è¯¯æ¦‚è¿°

åœ¨å¯åŠ¨ vLLM æœåŠ¡æ—¶é‡åˆ° `ValueError: To serve at least one request with the models's max seq len...` é”™è¯¯ï¼Œæç¤º KV Cache æ˜¾å­˜ä¸è¶³ã€‚

---

## ğŸ” é”™è¯¯ä¿¡æ¯è¯¦è§£

### å®Œæ•´é”™è¯¯æ—¥å¿—

```
(EngineCore_DP0 pid=18799) ERROR 10-08 20:33:48 [core.py:708] ValueError: 
To serve at least one request with the models's max seq len (40960), 
(5.62 GiB KV cache is needed, which is larger than the available KV cache memory (4.50 GiB). 
Based on the available memory, the estimated maximum model length is 32768. 
Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
```

### å…³é”®ä¿¡æ¯è§£è¯»

| é¡¹ç›® | å€¼ | è¯´æ˜ |
|------|-----|------|
| **éœ€è¦çš„ KV Cache** | 5.62 GiB | æ”¯æŒ 40960 åºåˆ—é•¿åº¦æ‰€éœ€æ˜¾å­˜ |
| **å¯ç”¨çš„ KV Cache** | 4.50 GiB | å½“å‰å¯åˆ†é…ç»™ KV Cache çš„æ˜¾å­˜ |
| **æ¨¡å‹é»˜è®¤åºåˆ—é•¿åº¦** | 40960 | æ¨¡å‹é…ç½®çš„ max_seq_len |
| **å»ºè®®åºåˆ—é•¿åº¦** | 32768 | åŸºäºå½“å‰æ˜¾å­˜çš„æ¨èå€¼ |
| **æ¨¡å‹åŠ è½½æ˜¾å­˜** | 15.27 GiB | æ¨¡å‹æƒé‡å ç”¨çš„æ˜¾å­˜ |

### é”™è¯¯å‘ç”Ÿåœºæ™¯

- **æ¨¡å‹**: Medical_Qwen3_8B_Large_Language_Model (8B å‚æ•°)
- **é»˜è®¤é…ç½®**: max_seq_len = 40960
- **æ˜¾å­˜æ€»é‡**: çº¦ 24GB (ä¼°è®¡)
- **é—®é¢˜**: æ¨¡å‹æƒé‡ (15.27 GiB) + KV Cache (5.62 GiB) > å¯ç”¨æ˜¾å­˜

---

## ğŸ¯ æ ¸å¿ƒåŸå› åˆ†æ

### 1. KV Cache æ˜¯ä»€ä¹ˆï¼Ÿ

KV Cacheï¼ˆKey-Value Cacheï¼‰ç”¨äºå­˜å‚¨ Transformer æ¨¡å‹ä¸­æ³¨æ„åŠ›æœºåˆ¶çš„é”®å€¼å¯¹ï¼š
- é¿å…é‡å¤è®¡ç®—å·²ç”Ÿæˆ token çš„æ³¨æ„åŠ›
- æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦
- **æ˜¾å­˜å ç”¨ä¸åºåˆ—é•¿åº¦æˆæ­£æ¯”**

### 2. KV Cache æ˜¾å­˜è®¡ç®—å…¬å¼

```
KV Cache å¤§å° â‰ˆ 2 Ã— num_layers Ã— num_kv_heads Ã— head_dim Ã— max_model_len Ã— sizeof(dtype) Ã— batch_size
```

**å¯¹äº Qwen3-8B æ¨¡å‹**ï¼š
- `num_layers`: 28
- `num_kv_heads`: 2 (ä½¿ç”¨ GQAï¼Œåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›)
- `head_dim`: 128
- `dtype`: bfloat16 (2 bytes)

**ä¸åŒåºåˆ—é•¿åº¦çš„ KV Cache å ç”¨**ï¼š

| max_model_len | KV Cache æ˜¾å­˜ | é€‚ç”¨åœºæ™¯ |
|---------------|---------------|----------|
| 40960 | ~5.62 GiB | è¶…é•¿æ–‡æœ¬ã€æ–‡æ¡£åˆ†æ |
| 32768 | ~4.50 GiB | é•¿æ–‡æœ¬å¯¹è¯ã€ä»£ç ç”Ÿæˆ |
| 16384 | ~2.25 GiB | æ™®é€šå¯¹è¯ã€é—®ç­” |
| 8192 | ~1.12 GiB | çŸ­æ–‡æœ¬ã€é«˜å¹¶å‘ |

### 3. æ˜¾å­˜åˆ†é…æœºåˆ¶

vLLM çš„æ˜¾å­˜åˆ†é…é¡ºåºï¼š
1. **æ¨¡å‹æƒé‡**: å›ºå®šå¤§å°ï¼ˆå¦‚ 15.27 GiBï¼‰
2. **CUDA Graphs**: çº¦ 0.84 GiB
3. **ç³»ç»Ÿå¼€é”€**: çº¦ 1-2 GiB
4. **KV Cache**: å‰©ä½™æ˜¾å­˜ Ã— `gpu_memory_utilization`

```
å¯ç”¨ KV Cache = (æ€»æ˜¾å­˜ - æ¨¡å‹æƒé‡ - ç³»ç»Ÿå¼€é”€ - CUDA Graphs) Ã— gpu_memory_utilization
```

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šé™ä½æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆæ¨èï¼‰

**é€‚ç”¨åœºæ™¯**: åº”ç”¨ä¸éœ€è¦è¶…é•¿ä¸Šä¸‹æ–‡

```bash
vllm serve /root/autodl-tmp/vllm/zpeng1989/Medical_Qwen3_8B_Large_Language_Model \
    --host 0.0.0.0 \
    --port 9000 \
    --api-key muyu \
    --served-model-name Medical_Qwen3_8B_Large_Language_Model \
    --trust-remote-code \
    --max-model-len 32768 \
    --tensor-parallel-size 1
```

**ä¼˜ç‚¹**:
- âœ… ç®€å•æœ‰æ•ˆï¼Œä¸€èˆ¬èƒ½è§£å†³é—®é¢˜
- âœ… ä¸ºå¹¶å‘è¯·æ±‚ç•™å‡ºæ›´å¤šæ˜¾å­˜
- âœ… 32K ä¸Šä¸‹æ–‡å¯¹å¤§å¤šæ•°åº”ç”¨è¶³å¤Ÿ

**ç¼ºç‚¹**:
- âŒ æ— æ³•å¤„ç†è¶…é•¿æ–‡æœ¬ï¼ˆ>32K tokensï¼‰

---

### æ–¹æ¡ˆ 2ï¼šæé«˜ GPU æ˜¾å­˜åˆ©ç”¨ç‡

**é€‚ç”¨åœºæ™¯**: å¸Œæœ›ä¿æŒé•¿åºåˆ—æ”¯æŒ

```bash
vllm serve /root/autodl-tmp/vllm/zpeng1989/Medical_Qwen3_8B_Large_Language_Model \
    --host 0.0.0.0 \
    --port 9000 \
    --api-key muyu \
    --served-model-name Medical_Qwen3_8B_Large_Language_Model \
    --trust-remote-code \
    --gpu-memory-utilization 0.95 \
    --tensor-parallel-size 1
```

**ä¼˜ç‚¹**:
- âœ… ä¿æŒæ¨¡å‹é»˜è®¤çš„é•¿åºåˆ—èƒ½åŠ›
- âœ… æœ€å¤§åŒ–æ˜¾å­˜åˆ©ç”¨

**ç¼ºç‚¹**:
- âŒ å¯èƒ½å¯¼è‡´ OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰
- âŒ å¹¶å‘èƒ½åŠ›å—é™

---

### æ–¹æ¡ˆ 3ï¼šç»„åˆä¼˜åŒ–ï¼ˆæœ€æ¨èï¼‰â­

**å¹³è¡¡æ€§èƒ½ã€ç¨³å®šæ€§å’Œå¹¶å‘èƒ½åŠ›**

```bash
vllm serve /root/autodl-tmp/vllm/zpeng1989/Medical_Qwen3_8B_Large_Language_Model \
    --host 0.0.0.0 \
    --port 9000 \
    --api-key muyu \
    --served-model-name Medical_Qwen3_8B_Large_Language_Model \
    --trust-remote-code \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.95 \
    --tensor-parallel-size 1
```

**ä¼˜ç‚¹**:
- âœ… 32K ä¸Šä¸‹æ–‡æ»¡è¶³å¤§éƒ¨åˆ†éœ€æ±‚
- âœ… å……åˆ†åˆ©ç”¨æ˜¾å­˜
- âœ… ç¨³å®šæ€§å¥½

---

### æ–¹æ¡ˆ 4ï¼šçŸ­æ–‡æœ¬é«˜å¹¶å‘é…ç½®

**é€‚ç”¨åœºæ™¯**: é—®ç­”ã€å®¢æœã€çŸ­å¯¹è¯

```bash
vllm serve /root/autodl-tmp/vllm/zpeng1989/Medical_Qwen3_8B_Large_Language_Model \
    --host 0.0.0.0 \
    --port 9000 \
    --api-key muyu \
    --served-model-name Medical_Qwen3_8B_Large_Language_Model \
    --trust-remote-code \
    --max-model-len 16384 \
    --gpu-memory-utilization 0.90 \
    --tensor-parallel-size 1
```

**ä¼˜ç‚¹**:
- âœ… KV Cache åªéœ€ 2.25 GiB
- âœ… æ”¯æŒæ›´å¤šå¹¶å‘è¯·æ±‚
- âœ… ç³»ç»Ÿæ›´ç¨³å®š

**ç¼ºç‚¹**:
- âŒ åªæ”¯æŒ 16K ä¸Šä¸‹æ–‡

---

### æ–¹æ¡ˆ 5ï¼šä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆé•¿æœŸæ–¹æ¡ˆï¼‰

å¦‚æœé¢‘ç¹é‡åˆ°æ˜¾å­˜é—®é¢˜ï¼Œè€ƒè™‘ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ï¼š

```bash
# ä½¿ç”¨ AWQ 4-bit é‡åŒ–æ¨¡å‹
vllm serve /path/to/Medical_Qwen3_8B_AWQ \
    --host 0.0.0.0 \
    --port 9000 \
    --api-key muyu \
    --served-model-name Medical_Qwen3_8B_Large_Language_Model \
    --trust-remote-code \
    --quantization awq \
    --max-model-len 40960 \
    --tensor-parallel-size 1
```

**ä¼˜ç‚¹**:
- âœ… æ¨¡å‹æƒé‡å‡å°‘ 50-75%
- âœ… æ”¯æŒæ›´é•¿åºåˆ—
- âœ… æ¨ç†é€Ÿåº¦æ›´å¿«

**ç¼ºç‚¹**:
- âŒ éœ€è¦é‡åŒ–åçš„æ¨¡å‹
- âŒ è½»å¾®ç²¾åº¦æŸå¤±

---

## ğŸ“Š é…ç½®å‚æ•°è¯¦è§£

### `--max-model-len`

- **ä½œç”¨**: è®¾ç½®å¼•æ“æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆprompt + ç”Ÿæˆï¼‰
- **é»˜è®¤å€¼**: æ¨¡å‹é…ç½®çš„ `max_position_embeddings`ï¼ˆé€šå¸¸æ˜¯ 32768 æˆ– 40960ï¼‰
- **å½±å“**: ç›´æ¥å†³å®š KV Cache å¤§å°
- **å»ºè®®å€¼**:
  - çŸ­æ–‡æœ¬åº”ç”¨: 8192 - 16384
  - æ™®é€šå¯¹è¯: 16384 - 32768
  - é•¿æ–‡æœ¬åˆ†æ: 32768 - 40960

### `--gpu-memory-utilization`

- **ä½œç”¨**: GPU æ˜¾å­˜åˆ©ç”¨ç‡
- **é»˜è®¤å€¼**: 0.90 (90%)
- **èŒƒå›´**: 0.0 - 1.0
- **å½±å“**: æ§åˆ¶åˆ†é…ç»™ KV Cache çš„æ˜¾å­˜æ¯”ä¾‹
- **å»ºè®®å€¼**:
  - å¼€å‘æµ‹è¯•: 0.85 - 0.90
  - ç”Ÿäº§ç¯å¢ƒ: 0.90 - 0.95
  - å¤šæ¨¡å‹å…±äº«: 0.70 - 0.80

### `--tensor-parallel-size`

- **ä½œç”¨**: å¼ é‡å¹¶è¡Œåº¦ï¼ˆæ¨¡å‹åˆ†ç‰‡æ•°ï¼‰
- **é»˜è®¤å€¼**: 1
- **ä½¿ç”¨åœºæ™¯**: å•å¡æ”¾ä¸ä¸‹æ¨¡å‹æ—¶ä½¿ç”¨å¤šå¡
- **æ³¨æ„**: éœ€è¦å¤šå¼  GPU å¡

---

## ğŸ”§ æ•…éšœæ’æŸ¥æ­¥éª¤

### 1. æ£€æŸ¥ GPU æ˜¾å­˜

```bash
nvidia-smi
```

ç¡®è®¤ï¼š
- æ€»æ˜¾å­˜å¤§å°
- å·²ä½¿ç”¨æ˜¾å­˜
- å¯ç”¨æ˜¾å­˜

### 2. æŸ¥çœ‹æ¨¡å‹é…ç½®

```bash
cat /path/to/model/config.json | grep -E "max_position_embeddings|hidden_size|num_hidden_layers|num_key_value_heads"
```

å…³é”®å‚æ•°ï¼š
- `max_position_embeddings`: æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦
- `num_hidden_layers`: å±‚æ•°
- `num_key_value_heads`: KV å¤´æ•°ï¼ˆå½±å“ KV Cache å¤§å°ï¼‰

### 3. ä¼°ç®— KV Cache éœ€æ±‚

ä½¿ç”¨å…¬å¼ä¼°ç®—ä¸åŒé…ç½®çš„æ˜¾å­˜éœ€æ±‚ï¼š

```python
# ç®€åŒ–ä¼°ç®—è„šæœ¬
def estimate_kv_cache(num_layers, num_kv_heads, head_dim, max_len, dtype_bytes=2):
    """
    ä¼°ç®— KV Cache æ˜¾å­˜ï¼ˆå•ä½ï¼šGBï¼‰
    """
    kv_cache_bytes = 2 * num_layers * num_kv_heads * head_dim * max_len * dtype_bytes
    kv_cache_gb = kv_cache_bytes / (1024**3)
    return kv_cache_gb

# Qwen3-8B ç¤ºä¾‹
print(f"40960: {estimate_kv_cache(28, 2, 128, 40960):.2f} GB")
print(f"32768: {estimate_kv_cache(28, 2, 128, 32768):.2f} GB")
print(f"16384: {estimate_kv_cache(28, 2, 128, 16384):.2f} GB")
```

### 4. åˆ†æå¯åŠ¨æ—¥å¿—

å…³é”®æ—¥å¿—è¡Œï¼š
```
INFO: Model loading took X.XX GiB       # æ¨¡å‹æƒé‡å¤§å°
INFO: Available KV cache memory: X.XX GiB  # å¯ç”¨ KV Cache æ˜¾å­˜
ERROR: X.XX GiB KV cache is needed     # æ‰€éœ€ KV Cache æ˜¾å­˜
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. ç”Ÿäº§ç¯å¢ƒé…ç½®å»ºè®®

```bash
# ç¨³å®šæ€§ä¼˜å…ˆ
vllm serve <model_path> \
    --host 0.0.0.0 \
    --port 9000 \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.92 \
    --max-num-seqs 256 \
    --tensor-parallel-size 1
```

### 2. æ€§èƒ½ä¼˜åŒ–é…ç½®

```bash
# ååé‡ä¼˜å…ˆ
vllm serve <model_path> \
    --host 0.0.0.0 \
    --port 9000 \
    --max-model-len 16384 \
    --gpu-memory-utilization 0.90 \
    --max-num-seqs 512 \
    --enable-prefix-caching \
    --tensor-parallel-size 1
```

### 3. å¼€å‘è°ƒè¯•é…ç½®

```bash
# å¿«é€Ÿå¯åŠ¨ï¼Œæ–¹ä¾¿è°ƒè¯•
vllm serve <model_path> \
    --host 127.0.0.1 \
    --port 9000 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.85 \
    --disable-log-stats \
    --tensor-parallel-size 1
```

---

## ğŸ“– å¸¸è§é—®é¢˜ FAQ

### Q1: ä¸ºä»€ä¹ˆæ¨¡å‹åŠ è½½æˆåŠŸä½†åˆå§‹åŒ– KV Cache å¤±è´¥ï¼Ÿ

**A**: vLLM åˆ†ä¸¤é˜¶æ®µåˆ†é…æ˜¾å­˜ï¼š
1. åŠ è½½æ¨¡å‹æƒé‡ï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
2. åˆå§‹åŒ– KV Cacheï¼ˆä½¿ç”¨å‰©ä½™æ˜¾å­˜ï¼‰

å¦‚æœå‰©ä½™æ˜¾å­˜ä¸è¶³ä»¥æ”¯æŒè®¾å®šçš„ `max_model_len`ï¼Œå°±ä¼šæŠ¥é”™ã€‚

---

### Q2: é™ä½ max_model_len ä¼šå½±å“æ¨¡å‹èƒ½åŠ›å—ï¼Ÿ

**A**: ä¸ä¼šå½±å“æ¨¡å‹æœ¬èº«çš„èƒ½åŠ›ï¼Œåªæ˜¯é™åˆ¶äº†ï¼š
- å•æ¬¡è¯·æ±‚çš„æœ€å¤§è¾“å…¥é•¿åº¦
- è¾“å…¥ + è¾“å‡ºçš„æ€»é•¿åº¦

ä¾‹å¦‚ `max_model_len=16384` æ—¶ï¼Œå¦‚æœè¾“å…¥ 15000 tokensï¼Œåªèƒ½ç”Ÿæˆ 1384 tokensã€‚

---

### Q3: gpu_memory_utilization è®¾ç½®ä¸º 1.0 å¯ä»¥å—ï¼Ÿ

**A**: **ä¸æ¨è**ã€‚åŸå› ï¼š
- PyTorch å’Œ CUDA éœ€è¦é¢å¤–æ˜¾å­˜
- å¯èƒ½å¯¼è‡´ OOM å´©æºƒ
- æ²¡æœ‰ç¼“å†²ç©ºé—´åº”å¯¹å³°å€¼

å»ºè®®æœ€é«˜è®¾ç½®ä¸º 0.95ã€‚

---

### Q4: å¦‚ä½•åˆ¤æ–­åº”è¯¥è®¾ç½®å¤šå¤§çš„ max_model_lenï¼Ÿ

**A**: æ ¹æ®å®é™…åº”ç”¨åœºæ™¯ï¼š

| åœºæ™¯ | å»ºè®®å€¼ | è¯´æ˜ |
|------|--------|------|
| å®¢æœé—®ç­” | 4096 - 8192 | ç®€çŸ­å¯¹è¯ |
| ä»£ç ç”Ÿæˆ | 8192 - 16384 | ä¸­ç­‰é•¿åº¦ä»£ç  |
| æ–‡æ¡£æ‘˜è¦ | 16384 - 32768 | éœ€è¦è¾ƒé•¿ä¸Šä¸‹æ–‡ |
| é•¿æ–‡æœ¬åˆ†æ | 32768 - 40960 | å®Œæ•´æ–‡æ¡£å¤„ç† |

å¯ä»¥å…ˆè®¾ç½®è¾ƒå¤§å€¼ï¼Œæ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µè°ƒæ•´ã€‚

---

### Q5: å¤šä¸ªæ¨¡å‹å¦‚ä½•å…±äº« GPUï¼Ÿ

**A**: é™ä½æ¯ä¸ªæ¨¡å‹çš„ `gpu_memory_utilization`ï¼š

```bash
# æ¨¡å‹ 1
vllm serve model1 --port 9000 --gpu-memory-utilization 0.45

# æ¨¡å‹ 2ï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
vllm serve model2 --port 9001 --gpu-memory-utilization 0.45
```

---

## ğŸ”— ç›¸å…³èµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [vLLM GitHub Issues](https://github.com/vllm-project/vllm/issues)
- [KV Cache åŸç†è§£æ](https://docs.vllm.ai/en/latest/design/kernel/paged_attention.html)

---

## ğŸ“ æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | ç‰ˆæœ¬ | æ›´æ–°å†…å®¹ |
|------|------|----------|
| 2025-10-08 | v1.0 | åˆå§‹ç‰ˆæœ¬ï¼ŒåŸºäº Medical_Qwen3_8B é”™è¯¯æ¡ˆä¾‹ |

---

## ğŸ’¡ æ€»ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. **KV Cache æ˜¾å­˜ä¸ max_model_len æˆæ­£æ¯”**
2. **é™ä½ max_model_len æ˜¯æœ€ç›´æ¥æœ‰æ•ˆçš„è§£å†³æ–¹æ³•**
3. **ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®**ï¼š`max_model_len=32768` + `gpu_memory_utilization=0.92`
4. **æ ¹æ®å®é™…åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„åºåˆ—é•¿åº¦**
5. **é¢„ç•™ 5-10% æ˜¾å­˜ç¼“å†²é¿å… OOM**

**æ¨èå¯åŠ¨å‘½ä»¤**ï¼ˆMedical_Qwen3_8Bï¼‰ï¼š

```bash
vllm serve /root/autodl-tmp/vllm/zpeng1989/Medical_Qwen3_8B_Large_Language_Model \
    --host 0.0.0.0 \
    --port 9000 \
    --api-key muyu \
    --served-model-name Medical_Qwen3_8B_Large_Language_Model \
    --trust-remote-code \
    --max-model-len 32768 \
    --gpu-memory-utilization 0.95 \
    --tensor-parallel-size 1
```

è¿™ä¸ªé…ç½®å¯ä»¥ç¨³å®šè¿è¡Œï¼Œå¹¶ä¸”æ”¯æŒ 32K ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ»¡è¶³å¤§éƒ¨åˆ†åŒ»ç–—é—®ç­”å’Œæ–‡æ¡£åˆ†æåœºæ™¯ã€‚

