{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Deepseek企业级Agent项目开发实战</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Part 2. Ollama REST API - api/generate 接口详解 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Ollama 服务启动后会提供一系列原生 ` REST API` 端点。通过这些`Endpoints`可以在代码环境下与`ollama`启动的大模型进行交互、管理模型和获取相关信息。其中两个`endpoint` 是最重要的，分别是：\n",
    "  - <font color=\"red\">**POST /api/generate**</font>\n",
    "  - <font color=\"red\">**POST /api/chat**</font>\n",
    "\n",
    "&emsp;&emsp;其他端点情况：\n",
    "  - POST /api/create   \n",
    "  - POST /api/tags\n",
    "  - POST /api/show\n",
    "  - POST /api/copy\n",
    "  - DELETE /api/delete\n",
    "  - POST /api/pull\n",
    "  - POST /api/push\n",
    "  - POST /api/embed\n",
    "  - GET /api/ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. /api/generate 接口参数概览\n",
    "\n",
    "&emsp;&emsp;该接口使用提供的模型为给定提示生成响应。这是一个流式端点，因此会有一系列响应。最终响应对象将包括统计信息和请求中的其他数据。其中比较重要的参数我做了标红处理，大家重点理解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>常规参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名      | 类型      | 描述                                                         |\n",
    "| ----------- | --------- | ------------------------------------------------------------ |\n",
    "| <font color=\"red\">**model**</font>   | *(必需)*  | 模型名称，必须遵循 `model:tag` 格式，如果不提供，则将默认为 `latest`。 |\n",
    "| <font color=\"red\">**prompt**</font>  | *(必需)*  | 用于生成响应的提示。                                         |\n",
    "| **suffix**  | *(可选)*  | 模型响应后的文本。                                         |\n",
    "| **images**  | *(可选)*  | base64 编码图像的列表（适用于多模态模型，如 llava）。      |\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4> 高级参数 (可选)</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名      | 类型      | 描述                                                         |\n",
    "| ----------- | --------- | ------------------------------------------------------------ |\n",
    "| <font color=\"red\">**format**</font>  | *(可选)*  | 返回响应的格式。格式可以是 `json` 或 JSON 模式。<font color=\"red\">最主要的问题是避免产生大量空格</font>         |\n",
    "| <font color=\"red\">**options**</font> | *(可选)*  | 文档中列出的其他模型参数，例如 `temperature`。              |\n",
    "| <font color=\"red\">**system**</font>  | *(可选)*  | 系统消息，用于覆盖 Modelfile 中定义的内容。                 |\n",
    "| **template**| *(可选)*  | 要使用的提示模板，覆盖 Modelfile 中定义的内容。             |\n",
    "| <font color=\"red\">**stream**</font>  | *(可选)*  | 如果为 `false`，响应将作为单个响应对象返回，而不是对象流。 |\n",
    "| **raw**     | *(可选)*  | 如果为 `true`，则不会对提示应用格式。                       |\n",
    "| <font color=\"red\">**keep_alive**</font> | *(可选)* | 控制模型在请求后保持加载的时间（默认：5分钟）。             |\n",
    "| **context** | *(可选)*  | *(已弃用)* 从先前请求返回的上下文参数，用于保持简短的对话记忆。 |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中，Options参数详细解释如下，同样我对重点参数做了标红处理，大家重点理解。\n",
    "\n",
    "| 参数名 | 描述 | 值类型 | 示例用法 |\n",
    "| --------------- | ------------------------------------------------------------ | ------ | ---------------------- |\n",
    "| mirostat | 启用 Mirostat 采样以控制困惑度。（默认：0，0 = 禁用，1 = Mirostat，2 = Mirostat 2.0） | int | mirostat 0 |\n",
    "| mirostat_eta| 影响算法对生成文本反馈的响应速度。较低的学习率会导致调整较慢，而较高的学习率会使算法更具响应性。（默认：0.1） | float | mirostat_eta 0.1 |\n",
    "| mirostat_tau| 控制输出的连贯性和多样性之间的平衡。较低的值会导致更集中和连贯的文本。（默认：5.0） | float | mirostat_tau 5.0 |\n",
    "| <font color=\"red\">num_ctx</font> | 设置用于生成下一个标记的上下文窗口大小。（默认：2048）, 影响的是模型可以一次记住的最大 token 数量。 | int | num_ctx 4096|\n",
    "| repeat_last_n| 设置模型回溯的范围以防止重复。（默认：64，0 = 禁用，-1 = num_ctx） | int | repeat_last_n 64 |\n",
    "| repeat_penalty| 设置惩罚重复的强度。较高的值（例如 1.5）会更强烈地惩罚重复，而较低的值（例如 0.9）会更宽松。（默认：1.1） | float | repeat_penalty 1.1 |\n",
    "| <font color=\"red\">temperature</font> | 模型的温度。增加温度会使模型的回答更具创造性。（默认：0.8） | float | temperature 0.7 |\n",
    "| seed | 设置用于生成的随机数种子。将其设置为特定数字将使模型对相同提示生成相同的文本。（默认：0） | int | seed 42 |\n",
    "| <font color=\"red\">stop</font> | 设置使用的停止序列。当遇到此模式时，LLM 将停止生成文本并返回。可以通过在 modelfile 中指定多个单独的停止参数来设置多个停止模式。 | string | stop \"AI assistant:\" |\n",
    "| <font color=\"red\">num_predict</font> | 生成文本时要预测的最大标记数。（默认：-1，无限生成）,影响模型最大可以生成的 token 数量。 | int | num_predict 42 |\n",
    "| top_k | 降低生成无意义文本的概率。较高的值（例如 100）会给出更多样化的答案，而较低的值（例如 10）会更保守。（默认：40） | int | top_k 40 |\n",
    "| top_p | 与 top-k 一起工作。较高的值（例如 0.95）会导致更具多样性的文本，而较低的值（例如 0.5）会生成更集中和保守的文本。（默认：0.9） | float | top_p 0.9 |\n",
    "| min_p | top_p 的替代方案，旨在确保质量和多样性之间的平衡。参数 p 表示考虑标记的最小概率，相对于最可能标记的概率。例如，p=0.05 时，最可能的标记概率为 0.9，值小于 0.045 的 logits 会被过滤掉。（默认：0.0） | float | min_p 0.05 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于`endpoints`来说，如果使用代码调用，常规的调用方式是通`requests`库进行调用。如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成响应: {\n",
      "  \"model\": \"deepseek-r1:32b\",\n",
      "  \"created_at\": \"2025-10-03T14:39:14.149915367Z\",\n",
      "  \"response\": \"<think>\\n好的，用户让我生成一段关于人工智能的简短介绍。首先，我需要明确什么是人工智能的基本定义，确保涵盖主要概念。\\n\\n然后，得提到它的主要目标，比如模拟人类智能、学习和适应能力。接着，可以列举一些关键技术，如机器学习、深度学习和自然语言处理，这样内容更具体。\\n\\n还要说明AI的应用领域，比如医疗、金融、交通等，让读者了解其广泛性。最后，补充一下伦理和社会影响的问题，这样介绍会更全面。\\n\\n要保持简洁明了，用词通俗易懂，避免过于专业的术语，确保适合各种背景的读者。\\n</think>\\n\\n人工智能（Artificial Intelligence, AI）是指模拟人类智能的系统或机器，能够执行如学习、推理和问题解决等任务。通过算法和技术，AI从数据中获取知识，并逐步改进性能。应用广泛于医疗、金融、交通等领域，正深刻改变我们的生活方式。同时，需关注伦理和社会影响，确保技术为社会福祉服务。\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"context\": [\n",
      "    151644,\n",
      "    14880,\n",
      "    43959,\n",
      "    46944,\n",
      "    101888,\n",
      "    104455,\n",
      "    9370,\n",
      "    98237,\n",
      "    99534,\n",
      "    100157,\n",
      "    1773,\n",
      "    151645,\n",
      "    151648,\n",
      "    198,\n",
      "    99692,\n",
      "    3837,\n",
      "    20002,\n",
      "    104029,\n",
      "    43959,\n",
      "    104383,\n",
      "    101888,\n",
      "    104455,\n",
      "    9370,\n",
      "    98237,\n",
      "    99534,\n",
      "    100157,\n",
      "    1773,\n",
      "    101140,\n",
      "    3837,\n",
      "    35946,\n",
      "    85106,\n",
      "    100692,\n",
      "    106582,\n",
      "    104455,\n",
      "    105166,\n",
      "    91282,\n",
      "    3837,\n",
      "    103944,\n",
      "    102994,\n",
      "    99558,\n",
      "    101290,\n",
      "    3407,\n",
      "    101889,\n",
      "    3837,\n",
      "    49828,\n",
      "    104496,\n",
      "    104121,\n",
      "    99558,\n",
      "    100160,\n",
      "    3837,\n",
      "    101912,\n",
      "    105717,\n",
      "    103971,\n",
      "    100168,\n",
      "    5373,\n",
      "    100134,\n",
      "    33108,\n",
      "    104117,\n",
      "    99788,\n",
      "    1773,\n",
      "    102524,\n",
      "    3837,\n",
      "    73670,\n",
      "    118569,\n",
      "    101883,\n",
      "    114876,\n",
      "    3837,\n",
      "    29524,\n",
      "    102182,\n",
      "    100134,\n",
      "    5373,\n",
      "    102217,\n",
      "    100134,\n",
      "    33108,\n",
      "    99795,\n",
      "    102064,\n",
      "    54542,\n",
      "    3837,\n",
      "    99654,\n",
      "    43815,\n",
      "    33126,\n",
      "    100398,\n",
      "    3407,\n",
      "    104019,\n",
      "    66394,\n",
      "    15469,\n",
      "    106736,\n",
      "    100650,\n",
      "    3837,\n",
      "    101912,\n",
      "    100182,\n",
      "    5373,\n",
      "    100015,\n",
      "    5373,\n",
      "    99735,\n",
      "    49567,\n",
      "    3837,\n",
      "    99258,\n",
      "    104785,\n",
      "    99794,\n",
      "    41146,\n",
      "    100789,\n",
      "    33071,\n",
      "    1773,\n",
      "    100161,\n",
      "    3837,\n",
      "    104361,\n",
      "    100158,\n",
      "    112811,\n",
      "    106640,\n",
      "    99564,\n",
      "    103936,\n",
      "    3837,\n",
      "    99654,\n",
      "    100157,\n",
      "    36993,\n",
      "    33126,\n",
      "    100011,\n",
      "    3407,\n",
      "    30534,\n",
      "    100662,\n",
      "    110485,\n",
      "    30858,\n",
      "    34187,\n",
      "    3837,\n",
      "    11622,\n",
      "    99689,\n",
      "    116336,\n",
      "    86744,\n",
      "    100272,\n",
      "    3837,\n",
      "    101153,\n",
      "    102767,\n",
      "    104715,\n",
      "    116925,\n",
      "    3837,\n",
      "    103944,\n",
      "    100231,\n",
      "    100646,\n",
      "    102193,\n",
      "    9370,\n",
      "    104785,\n",
      "    8997,\n",
      "    151649,\n",
      "    271,\n",
      "    104455,\n",
      "    9909,\n",
      "    9286,\n",
      "    16488,\n",
      "    21392,\n",
      "    11,\n",
      "    15235,\n",
      "    7552,\n",
      "    104442,\n",
      "    105717,\n",
      "    103971,\n",
      "    100168,\n",
      "    9370,\n",
      "    72448,\n",
      "    57191,\n",
      "    102182,\n",
      "    3837,\n",
      "    100006,\n",
      "    75117,\n",
      "    29524,\n",
      "    100134,\n",
      "    5373,\n",
      "    113272,\n",
      "    33108,\n",
      "    86119,\n",
      "    100638,\n",
      "    49567,\n",
      "    88802,\n",
      "    1773,\n",
      "    67338,\n",
      "    107018,\n",
      "    108800,\n",
      "    3837,\n",
      "    15469,\n",
      "    45181,\n",
      "    20074,\n",
      "    15946,\n",
      "    45912,\n",
      "    100032,\n",
      "    90395,\n",
      "    104137,\n",
      "    105023,\n",
      "    102111,\n",
      "    1773,\n",
      "    99892,\n",
      "    100789,\n",
      "    34204,\n",
      "    100182,\n",
      "    5373,\n",
      "    100015,\n",
      "    5373,\n",
      "    99735,\n",
      "    106483,\n",
      "    3837,\n",
      "    36556,\n",
      "    101295,\n",
      "    101933,\n",
      "    103952,\n",
      "    107142,\n",
      "    1773,\n",
      "    91572,\n",
      "    3837,\n",
      "    58362,\n",
      "    100020,\n",
      "    112811,\n",
      "    106640,\n",
      "    99564,\n",
      "    3837,\n",
      "    103944,\n",
      "    99361,\n",
      "    17714,\n",
      "    99328,\n",
      "    118848,\n",
      "    47874,\n",
      "    1773\n",
      "  ],\n",
      "  \"total_duration\": 6496974502,\n",
      "  \"load_duration\": 228863547,\n",
      "  \"prompt_eval_count\": 13,\n",
      "  \"prompt_eval_duration\": 25569951,\n",
      "  \"eval_count\": 209,\n",
      "  \"eval_duration\": 6240912171\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests # type: ignore\n",
    "import json\n",
    "\n",
    "# 设置 API 端点\n",
    "generate_url = \"http://localhost:11434/api/generate\"    # 这里需要根据实际情况进行修改\n",
    "\n",
    "# 示例数据\n",
    "generate_payload = {\n",
    "    \"model\": \"deepseek-r1:32b\",   # 这里需要根据实际情况进行修改\n",
    "    \"prompt\": \"请生成一个关于人工智能的简短介绍。\",  # 这里需要根据实际情况进行修改\n",
    "    \"stream\": False,       # 默认使用的是True，如果设置为False，则返回的是一个完整的响应，而不是一个流式响应\n",
    "}\n",
    "\n",
    "# 调用生成接口\n",
    "response_generate = requests.post(generate_url, json=generate_payload)\n",
    "if response_generate.status_code == 200:\n",
    "    generate_response = response_generate.json()\n",
    "    print(\"生成响应:\", json.dumps(generate_response, ensure_ascii=False, indent=2))\n",
    "else:\n",
    "    print(\"生成请求失败:\", response_generate.status_code, response_generate.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;返回的响应中包含以下参数，其对应的描述如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>响应参数</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数名                  | 描述                                                         |\n",
    "| ----------------------- | ------------------------------------------------------------ |\n",
    "| **total_duration**      | 单次响应花费的总时间                                          |\n",
    "| **load_duration**       | 加载模型花费的时间                                   |\n",
    "| **prompt_eval_count**   | 提示中的token数                                               |\n",
    "| **prompt_eval_duration**| 评估提示所花费的时间（以纳秒为单位）                                 |\n",
    "| **eval_count**          | 响应中的token数                                               |\n",
    "| **eval_duration**       | 生成响应的时间（以纳秒为单位）                              |\n",
    "| **context**             | 在此响应中使用的对话的编码，可以在下一个请求中发送以保持对话记忆 |\n",
    "| **response**            | 空响应是流的，如果未流式传输，则将包含完整的响应             |\n",
    "\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;返回的响应体中重点关注以下几个参数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. response 参数格式化解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`response` 字段指的是模型生成的实际输出内容。对于 `DeepSeek-R1` 模型来说，`response` 字段中包含<think> 标签和正常文本，<think> 标签用于表示模型的思考过程或内部推理，而正常的文本则是模型生成的实际输出内容。注意：非推理类模型的返回结果中没有<think></think>标识。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n好的，用户让我生成一段关于人工智能的简短介绍。首先，我需要明确什么是人工智能的基本定义，确保涵盖主要概念。\\n\\n然后，得提到它的主要目标，比如模拟人类智能、学习和适应能力。接着，可以列举一些关键技术，如机器学习、深度学习和自然语言处理，这样内容更具体。\\n\\n还要说明AI的应用领域，比如医疗、金融、交通等，让读者了解其广泛性。最后，补充一下伦理和社会影响的问题，这样介绍会更全面。\\n\\n要保持简洁明了，用词通俗易懂，避免过于专业的术语，确保适合各种背景的读者。\\n</think>\\n\\n人工智能（Artificial Intelligence, AI）是指模拟人类智能的系统或机器，能够执行如学习、推理和问题解决等任务。通过算法和技术，AI从数据中获取知识，并逐步改进性能。应用广泛于医疗、金融、交通等领域，正深刻改变我们的生活方式。同时，需关注伦理和社会影响，确保技术为社会福祉服务。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以通过简单的字符串操作来分离 <think> 标签中的思考内容和正常的文本内容，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "思考内容:\n",
      " 好的，用户让我生成一段关于人工智能的简短介绍。首先，我需要明确什么是人工智能的基本定义，确保涵盖主要概念。\n",
      "\n",
      "然后，得提到它的主要目标，比如模拟人类智能、学习和适应能力。接着，可以列举一些关键技术，如机器学习、深度学习和自然语言处理，这样内容更具体。\n",
      "\n",
      "还要说明AI的应用领域，比如医疗、金融、交通等，让读者了解其广泛性。最后，补充一下伦理和社会影响的问题，这样介绍会更全面。\n",
      "\n",
      "要保持简洁明了，用词通俗易懂，避免过于专业的术语，确保适合各种背景的读者。\n",
      "\n",
      "正常内容:\n",
      " 人工智能（Artificial Intelligence, AI）是指模拟人类智能的系统或机器，能够执行如学习、推理和问题解决等任务。通过算法和技术，AI从数据中获取知识，并逐步改进性能。应用广泛于医疗、金融、交通等领域，正深刻改变我们的生活方式。同时，需关注伦理和社会影响，确保技术为社会福祉服务。\n"
     ]
    }
   ],
   "source": [
    "# 提取 <think> 标签中的内容\n",
    "think_start = generate_response[\"response\"].find(\"<think>\")\n",
    "think_end = generate_response[\"response\"].find(\"</think>\")\n",
    "\n",
    "if think_start != -1 and think_end != -1:\n",
    "    think_content = generate_response[\"response\"][think_start + len(\"<think>\"):think_end].strip()\n",
    "else:\n",
    "    think_content = \"No think content found.\"\n",
    "\n",
    "# 提取正常的文本内容\n",
    "normal_content = generate_response[\"response\"][think_end + len(\"</think>\"):].strip()\n",
    "\n",
    "# 打印结果\n",
    "print(\"思考内容:\\n\", think_content)\n",
    "print(\"\\n正常内容:\\n\", normal_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然也可以用相同的方式提取返回的响应中所有参数的值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deepseek-r1:32b\n",
      "Created At: 2025-10-03T14:39:14.149915367Z\n",
      "Response: <think>\n",
      "好的，用户让我生成一段关于人工智能的简短介绍。首先，我需要明确什么是人工智能的基本定义，确保涵盖主要概念。\n",
      "\n",
      "然后，得提到它的主要目标，比如模拟人类智能、学习和适应能力。接着，可以列举一些关键技术，如机器学习、深度学习和自然语言处理，这样内容更具体。\n",
      "\n",
      "还要说明AI的应用领域，比如医疗、金融、交通等，让读者了解其广泛性。最后，补充一下伦理和社会影响的问题，这样介绍会更全面。\n",
      "\n",
      "要保持简洁明了，用词通俗易懂，避免过于专业的术语，确保适合各种背景的读者。\n",
      "</think>\n",
      "\n",
      "人工智能（Artificial Intelligence, AI）是指模拟人类智能的系统或机器，能够执行如学习、推理和问题解决等任务。通过算法和技术，AI从数据中获取知识，并逐步改进性能。应用广泛于医疗、金融、交通等领域，正深刻改变我们的生活方式。同时，需关注伦理和社会影响，确保技术为社会福祉服务。\n",
      "Done: True\n",
      "Done Reason: stop\n",
      "Context: [151644, 14880, 43959, 46944, 101888, 104455, 9370, 98237, 99534, 100157, 1773, 151645, 151648, 198, 99692, 3837, 20002, 104029, 43959, 104383, 101888, 104455, 9370, 98237, 99534, 100157, 1773, 101140, 3837, 35946, 85106, 100692, 106582, 104455, 105166, 91282, 3837, 103944, 102994, 99558, 101290, 3407, 101889, 3837, 49828, 104496, 104121, 99558, 100160, 3837, 101912, 105717, 103971, 100168, 5373, 100134, 33108, 104117, 99788, 1773, 102524, 3837, 73670, 118569, 101883, 114876, 3837, 29524, 102182, 100134, 5373, 102217, 100134, 33108, 99795, 102064, 54542, 3837, 99654, 43815, 33126, 100398, 3407, 104019, 66394, 15469, 106736, 100650, 3837, 101912, 100182, 5373, 100015, 5373, 99735, 49567, 3837, 99258, 104785, 99794, 41146, 100789, 33071, 1773, 100161, 3837, 104361, 100158, 112811, 106640, 99564, 103936, 3837, 99654, 100157, 36993, 33126, 100011, 3407, 30534, 100662, 110485, 30858, 34187, 3837, 11622, 99689, 116336, 86744, 100272, 3837, 101153, 102767, 104715, 116925, 3837, 103944, 100231, 100646, 102193, 9370, 104785, 8997, 151649, 271, 104455, 9909, 9286, 16488, 21392, 11, 15235, 7552, 104442, 105717, 103971, 100168, 9370, 72448, 57191, 102182, 3837, 100006, 75117, 29524, 100134, 5373, 113272, 33108, 86119, 100638, 49567, 88802, 1773, 67338, 107018, 108800, 3837, 15469, 45181, 20074, 15946, 45912, 100032, 90395, 104137, 105023, 102111, 1773, 99892, 100789, 34204, 100182, 5373, 100015, 5373, 99735, 106483, 3837, 36556, 101295, 101933, 103952, 107142, 1773, 91572, 3837, 58362, 100020, 112811, 106640, 99564, 3837, 103944, 99361, 17714, 99328, 118848, 47874, 1773]\n",
      "Total Duration: 6496974502\n",
      "Load Duration: 228863547\n",
      "Prompt Eval Count: 13\n",
      "Prompt Eval Duration: 25569951\n",
      "Eval Count: 209\n",
      "Eval Duration: 6240912171\n"
     ]
    }
   ],
   "source": [
    "# 打印每个参数的值\n",
    "print(\"Model:\", generate_response[\"model\"])\n",
    "print(\"Created At:\", generate_response[\"created_at\"])\n",
    "print(\"Response:\", generate_response[\"response\"])\n",
    "print(\"Done:\", generate_response[\"done\"])\n",
    "print(\"Done Reason:\", generate_response[\"done_reason\"])\n",
    "print(\"Context:\", generate_response[\"context\"])\n",
    "print(\"Total Duration:\", generate_response[\"total_duration\"])\n",
    "print(\"Load Duration:\", generate_response[\"load_duration\"])\n",
    "print(\"Prompt Eval Count:\", generate_response[\"prompt_eval_count\"])\n",
    "print(\"Prompt Eval Duration:\", generate_response[\"prompt_eval_duration\"])\n",
    "print(\"Eval Count:\", generate_response[\"eval_count\"])\n",
    "print(\"Eval Duration:\", generate_response[\"eval_duration\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`Ollama` 返回的响应中，采用的时间单位均以纳秒返回。纳秒（nanosecond）和秒（second）之间的关系是：<font color=\"red\">1 秒 = 10⁹ 纳秒</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "单次调用总花费时间: 6.496974502\n",
      "加载模型花费时间: 0.228863547\n",
      "评估提示所花费的时间: 0.025569951\n",
      "生成响应的时间: 6.240912171\n"
     ]
    }
   ],
   "source": [
    "# 将纳秒转换为秒\n",
    "total_duration_s = generate_response[\"total_duration\"] / 1_000_000_000\n",
    "load_duration_s = generate_response[\"load_duration\"] / 1_000_000_000\n",
    "prompt_eval_duration_s = generate_response[\"prompt_eval_duration\"] / 1_000_000_000\n",
    "eval_duration_s = generate_response[\"eval_duration\"] / 1_000_000_000\n",
    "\n",
    "# 打印转换后的秒值\n",
    "print(\"单次调用总花费时间:\", total_duration_s)\n",
    "print(\"加载模型花费时间:\", load_duration_s)\n",
    "print(\"评估提示所花费的时间:\", prompt_eval_duration_s)\n",
    "print(\"生成响应的时间:\", eval_duration_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. num_ctx / num_predict 输入输出控制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`num_ctx` 和 `num_predict`参数都是需要放置在 `options` 参数中的，其中：\n",
    "\n",
    "- `num_ctx`该参数指的是大模型在一次对话中能够\"看到\"和\"记住\"的最大上下文长度，默认配置 2048，相当于一次只能向模型输入 2k `token`，超过 2k 模型就无法记住。当 `prompt` 特别长时往往会出现问题。并且现在开源模型往往支持长上下文，默认配置会严重限制本地模型能力。\n",
    "\n",
    "- `num_predict` 参数指的是模型响应返回的最大 token 数据量。\n",
    "\n",
    "&emsp;&emsp;我们可以这样测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成响应: {\n",
      "  \"model\": \"deepseek-r1:32b\",\n",
      "  \"created_at\": \"2025-10-03T14:42:27.181479878Z\",\n",
      "  \"response\": \"<think>\\n嗯，用户让我生成一个关于人工智能\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"length\",\n",
      "  \"context\": [\n",
      "    151644,\n",
      "    14880,\n",
      "    43959,\n",
      "    46944,\n",
      "    101888,\n",
      "    104455,\n",
      "    9370,\n",
      "    98237,\n",
      "    99534,\n",
      "    100157,\n",
      "    1773,\n",
      "    151645,\n",
      "    151648,\n",
      "    198,\n",
      "    106287,\n",
      "    3837,\n",
      "    20002,\n",
      "    104029,\n",
      "    43959,\n",
      "    46944,\n",
      "    101888,\n",
      "    104455\n",
      "  ],\n",
      "  \"total_duration\": 537172829,\n",
      "  \"load_duration\": 218187733,\n",
      "  \"prompt_eval_count\": 13,\n",
      "  \"prompt_eval_duration\": 25298913,\n",
      "  \"eval_count\": 10,\n",
      "  \"eval_duration\": 292216403\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests # type: ignore\n",
    "import json\n",
    "\n",
    "# 设置 API 端点\n",
    "generate_url = \"http://localhost:11434/api/generate\"    # 这里需要根据实际情况进行修改\n",
    "\n",
    "# 示例数据\n",
    "generate_payload = {\n",
    "    \"model\": \"deepseek-r1:32b\",   # 这里需要根据实际情况进行修改\n",
    "    \"prompt\": \"请生成一个关于人工智能的简短介绍。\",  # 这里需要根据实际情况进行修改\n",
    "    \"stream\": False,       # 默认使用的是True，如果设置为False，则返回的是一个完整的响应，而不是一个流式响应\n",
    "    \"options\": {\n",
    "        # \"num_ctx\": 7,  慎用，可能会导致Ollama服务不稳定，建议选择 1024 及以上\n",
    "        \"num_predict\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "# 调用生成接口\n",
    "response_generate = requests.post(generate_url, json=generate_payload)\n",
    "if response_generate.status_code == 200:\n",
    "    generate_response = response_generate.json()\n",
    "    print(\"生成响应:\", json.dumps(generate_response, ensure_ascii=False, indent=2))\n",
    "else:\n",
    "    print(\"生成请求失败:\", response_generate.status_code, response_generate.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 流式输出功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来看流式输出输出，其参数和如上代码保持一致，只需要在 `response_generate` 中添加 `stream=True`，最后再通过流式的方式进行响应结果处理即可。代码如下所示：\n",
    "\n",
    "> 这里有一个使用`DeepSeek-R1`的小技巧，将温度即`temperature`设置在0.5-0.7（建议0.6）的范围内，可以有效防止无尽的重复或不连贯的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户让我写一个关于人工智能的简短介绍。首先，我得考虑用户是谁，可能是学生、老师还是普通对科技感兴趣的人。他们可能需要一个简洁明了的概述，不需要太深入的技术细节。\n",
      "\n",
      "接下来，我应该涵盖人工智能的基本定义，比如它是什么，有什么核心领域，比如机器学习和深度学习。还要提到它的应用范围，比如医疗、金融、交通这些领域，这样用户能理解AI的实际影响。\n",
      "\n",
      "另外，伦理和社会问题也是重要的一部分，不能只讲优点，也要提挑战，比如隐私和就业问题。最后，展望未来，强调技术进步带来的可能性，但也要注意平衡发展。\n",
      "\n",
      "结构上，我需要先定义什么是人工智能，然后介绍它的核心技术，接着举例应用，再讨论挑战，最后总结未来的潜力。这样逻辑清晰，用户容易理解。\n",
      "</think>\n",
      "\n",
      "人工智能（Artificial Intelligence, AI）是指模拟人类智能的系统或机器，能够执行如学习、推理、问题解决和自然语言处理等任务。它通过算法和大数据分析，使计算机具备类似人类的决策和适应能力。AI的核心技术包括机器学习、深度学习和自然语言处理等。广泛应用于医疗、金融、交通等领域，为社会带来效率提升和创新解决方案，但也引发隐私、伦理和社会影响等方面的讨论。\n",
      "\n",
      "完整响应: {\n",
      "  \"model\": \"deepseek-r1:32b\",\n",
      "  \"created_at\": \"2025-10-03T15:06:41.749071697Z\",\n",
      "  \"response\": \"\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"context\": [\n",
      "    151644,\n",
      "    14880,\n",
      "    43959,\n",
      "    46944,\n",
      "    101888,\n",
      "    104455,\n",
      "    9370,\n",
      "    98237,\n",
      "    99534,\n",
      "    100157,\n",
      "    1773,\n",
      "    151645,\n",
      "    151648,\n",
      "    198,\n",
      "    106287,\n",
      "    3837,\n",
      "    20002,\n",
      "    104029,\n",
      "    61443,\n",
      "    46944,\n",
      "    101888,\n",
      "    104455,\n",
      "    9370,\n",
      "    98237,\n",
      "    99534,\n",
      "    100157,\n",
      "    1773,\n",
      "    101140,\n",
      "    3837,\n",
      "    35946,\n",
      "    49828,\n",
      "    101118,\n",
      "    20002,\n",
      "    105518,\n",
      "    3837,\n",
      "    104560,\n",
      "    99720,\n",
      "    5373,\n",
      "    101049,\n",
      "    99998,\n",
      "    100714,\n",
      "    32664,\n",
      "    99602,\n",
      "    103198,\n",
      "    100623,\n",
      "    1773,\n",
      "    99650,\n",
      "    87267,\n",
      "    85106,\n",
      "    46944,\n",
      "    110485,\n",
      "    30858,\n",
      "    34187,\n",
      "    9370,\n",
      "    113608,\n",
      "    3837,\n",
      "    104689,\n",
      "    99222,\n",
      "    100403,\n",
      "    105535,\n",
      "    104449,\n",
      "    3407,\n",
      "    104326,\n",
      "    3837,\n",
      "    35946,\n",
      "    99730,\n",
      "    102994,\n",
      "    104455,\n",
      "    105166,\n",
      "    91282,\n",
      "    3837,\n",
      "    101912,\n",
      "    99652,\n",
      "    102021,\n",
      "    3837,\n",
      "    104139,\n",
      "    100185,\n",
      "    100650,\n",
      "    3837,\n",
      "    101912,\n",
      "    102182,\n",
      "    100134,\n",
      "    33108,\n",
      "    102217,\n",
      "    100134,\n",
      "    1773,\n",
      "    104019,\n",
      "    104496,\n",
      "    104121,\n",
      "    99892,\n",
      "    101121,\n",
      "    3837,\n",
      "    101912,\n",
      "    100182,\n",
      "    5373,\n",
      "    100015,\n",
      "    5373,\n",
      "    99735,\n",
      "    100001,\n",
      "    100650,\n",
      "    3837,\n",
      "    99654,\n",
      "    20002,\n",
      "    26232,\n",
      "    101128,\n",
      "    15469,\n",
      "    108081,\n",
      "    99564,\n",
      "    3407,\n",
      "    101948,\n",
      "    3837,\n",
      "    112811,\n",
      "    106640,\n",
      "    86119,\n",
      "    100000,\n",
      "    99335,\n",
      "    106979,\n",
      "    3837,\n",
      "    53153,\n",
      "    91680,\n",
      "    99526,\n",
      "    103134,\n",
      "    3837,\n",
      "    104302,\n",
      "    28072,\n",
      "    104036,\n",
      "    3837,\n",
      "    101912,\n",
      "    107120,\n",
      "    33108,\n",
      "    101967,\n",
      "    86119,\n",
      "    1773,\n",
      "    100161,\n",
      "    3837,\n",
      "    109789,\n",
      "    100353,\n",
      "    3837,\n",
      "    104046,\n",
      "    99361,\n",
      "    101300,\n",
      "    102220,\n",
      "    103026,\n",
      "    3837,\n",
      "    77288,\n",
      "    104302,\n",
      "    60533,\n",
      "    102243,\n",
      "    99185,\n",
      "    3407,\n",
      "    100166,\n",
      "    17447,\n",
      "    3837,\n",
      "    35946,\n",
      "    85106,\n",
      "    60726,\n",
      "    91282,\n",
      "    106582,\n",
      "    104455,\n",
      "    3837,\n",
      "    101889,\n",
      "    100157,\n",
      "    104121,\n",
      "    110025,\n",
      "    3837,\n",
      "    102524,\n",
      "    114336,\n",
      "    99892,\n",
      "    3837,\n",
      "    87256,\n",
      "    104075,\n",
      "    104036,\n",
      "    3837,\n",
      "    100161,\n",
      "    102050,\n",
      "    105735,\n",
      "    102575,\n",
      "    1773,\n",
      "    99654,\n",
      "    104913,\n",
      "    104542,\n",
      "    3837,\n",
      "    20002,\n",
      "    100047,\n",
      "    101128,\n",
      "    8997,\n",
      "    151649,\n",
      "    271,\n",
      "    104455,\n",
      "    9909,\n",
      "    9286,\n",
      "    16488,\n",
      "    21392,\n",
      "    11,\n",
      "    15235,\n",
      "    7552,\n",
      "    104442,\n",
      "    105717,\n",
      "    103971,\n",
      "    100168,\n",
      "    9370,\n",
      "    72448,\n",
      "    57191,\n",
      "    102182,\n",
      "    3837,\n",
      "    100006,\n",
      "    75117,\n",
      "    29524,\n",
      "    100134,\n",
      "    5373,\n",
      "    113272,\n",
      "    5373,\n",
      "    86119,\n",
      "    100638,\n",
      "    33108,\n",
      "    99795,\n",
      "    102064,\n",
      "    54542,\n",
      "    49567,\n",
      "    88802,\n",
      "    1773,\n",
      "    99652,\n",
      "    67338,\n",
      "    107018,\n",
      "    33108,\n",
      "    104315,\n",
      "    101042,\n",
      "    3837,\n",
      "    32555,\n",
      "    104564,\n",
      "    102094,\n",
      "    101412,\n",
      "    103971,\n",
      "    9370,\n",
      "    102041,\n",
      "    33108,\n",
      "    104117,\n",
      "    99788,\n",
      "    1773,\n",
      "    15469,\n",
      "    104867,\n",
      "    99361,\n",
      "    100630,\n",
      "    102182,\n",
      "    100134,\n",
      "    5373,\n",
      "    102217,\n",
      "    100134,\n",
      "    33108,\n",
      "    99795,\n",
      "    102064,\n",
      "    54542,\n",
      "    49567,\n",
      "    1773,\n",
      "    100789,\n",
      "    110645,\n",
      "    100182,\n",
      "    5373,\n",
      "    100015,\n",
      "    5373,\n",
      "    99735,\n",
      "    106483,\n",
      "    3837,\n",
      "    17714,\n",
      "    99328,\n",
      "    100393,\n",
      "    101991,\n",
      "    100341,\n",
      "    33108,\n",
      "    99656,\n",
      "    104520,\n",
      "    3837,\n",
      "    106884,\n",
      "    102361,\n",
      "    107120,\n",
      "    5373,\n",
      "    112811,\n",
      "    106640,\n",
      "    99564,\n",
      "    108515,\n",
      "    104075,\n",
      "    1773\n",
      "  ],\n",
      "  \"total_duration\": 8465530419,\n",
      "  \"load_duration\": 178857359,\n",
      "  \"prompt_eval_count\": 13,\n",
      "  \"prompt_eval_duration\": 60210242,\n",
      "  \"eval_count\": 271,\n",
      "  \"eval_duration\": 8224691155\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests # type: ignore\n",
    "import json\n",
    "\n",
    "# 设置 API 端点\n",
    "generate_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# 示例数据\n",
    "generate_payload = {\n",
    "    \"model\": \"deepseek-r1:32b\",\n",
    "    \"prompt\": \"请生成一个关于人工智能的简短介绍。\",\n",
    "    \"options\": {\n",
    "        \"temperature\": 0.6, \n",
    "    }\n",
    "}\n",
    "\n",
    "# 调用生成接口\n",
    "response_generate = requests.post(generate_url, json=generate_payload, stream=True)  # 在这里添加stream=True\n",
    "if response_generate.status_code == 200:\n",
    "    # 处理流式响应\n",
    "    for line in response_generate.iter_lines(): \n",
    "        if line:\n",
    "            try:\n",
    "                # 解码并解析每一行的 JSON\n",
    "                response_json = json.loads(line.decode('utf-8'))\n",
    "                if 'response' in response_json:\n",
    "                    print(response_json['response'], end='', flush=True)\n",
    "\n",
    "                # 检查 response_json 字典中是否存在键 'done'，并且其值是否为 True。如果这个条件成立，表示生成的响应已经完成。\n",
    "                if response_json.get('done', False):\n",
    "                    print('\\n\\n完整响应:', json.dumps(response_json, ensure_ascii=False, indent=2))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON 解析错误: {e}\")\n",
    "else:\n",
    "    print(\"生成请求失败:\", response_generate.status_code, response_generate.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面提供一个优化版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "开始生成内容...\n",
      "============================================================\n",
      "\n",
      "\n",
      "【💭 思考过程】\n",
      "------------------------------------------------------------\n",
      "嗯，我现在要仔细理解一下快速排序算法的原理以及它的时间复杂度。我对快速排序有一些基本了解，但还不够深入，所以需要一步一步地梳理。\n",
      "\n",
      "首先，快速排序是一种高效的排序算法，属于分治法的一种。我记得分治法的基本思想是将问题分解成更小的部分来解决，然后再合并结果。那快速排序是怎么应用这个方法的呢？\n",
      "\n",
      "快速排序的大致步骤应该是这样的：选择一个基准元素（pivot），然后将数组分成两部分，一部分小于等于基准元素，另一部分大于等于基准元素。接着递归地对这两部分进行同样的操作，直到整个数组有序。\n",
      "\n",
      "那么，关键点在于如何选择基准元素和分区过程。我之前学过几种选择基准的方法，比如选第一个元素、最后一个元素，或者中间的某个位置，甚至随机选择。不同的选择会影响算法的性能，特别是最坏情况下时间复杂度的表现。\n",
      "\n",
      "接下来是分区的过程，这一步应该是快速排序的核心部分。假设我们已经选择了基准元素，那么我们需要将数组分成两部分：一部分所有元素都小于等于基准，另一部分则大于等于基准。具体来说，通常会用到双指针的方法，一个从左往右找比基准大的元素，另一个从右往左找比基准小的元素，然后交换它们的位置。这样，基准元素最终会被放在正确的位置上。\n",
      "\n",
      "举个例子可能会有帮助。比如数组是 [3,6,8,10,1,4,7]，假设我们选择第一个元素3作为基准。那么在分区过程中，左边的指针从位置0开始，找到第一个比3大的数，也就是6；右边的指针从末尾开始，找到第一个比3小的数，比如1。然后交换这两个数的位置，数组变成 [1,6,8,10,3,4,7]。接着继续这个过程，直到左右指针相遇，这样基准元素就被正确地放置在了中间位置。\n",
      "\n",
      "接下来，递归地对左边的子数组[1]和右边的子数组[6,8,10,3,4,7]进行同样的操作。对于右边的子数组，可能选择第一个元素6作为基准，然后分区后得到 [3,4,6,8,10,7]，然后再继续递归。\n",
      "\n",
      "关于时间复杂度，快速排序在平均情况下是O(n log n)，这比冒泡排序之类的O(n²)要好得多。但是最坏情况下，比如每次选择的基准都是数组中最小或最大的元素，这样分区后的子数组长度分别为n-1和0，导致递归深度达到n层，每层操作需要O(n)时间，总的时间复杂度退化为O(n²)。\n",
      "\n",
      "不过，通常我们会采用一些优化措施来避免这种情况。比如选择随机的基准元素或者三数取中法（即从数组开头、中间和末尾选三个数，取其中间值作为基准），这样可以减少最坏情况发生的概率，使得平均情况下时间复杂度保持在O(n log n)。\n",
      "\n",
      "空间复杂度方面，快速排序是原地排序吗？不完全是，因为每次递归调用都会占用栈空间。如果数组很大，可能会导致栈溢出。但是有一种优化版本叫做迭代式快速排序，可以避免这个问题，或者使用尾递归优化。但不管怎样，快速排序的空间复杂度通常是O(log n)的，这是由于递归带来的栈空间消耗。\n",
      "\n",
      "总结一下，快速排序的主要步骤包括选择基准、分区、递归排序左右子数组。时间复杂度在平均情况下是O(n log n)，最坏情况下是O(n²)，但通过优化基准选择可以大大降低出现最坏情况的概率。\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "【✨ 最终答案】\n",
      "------------------------------------------------------------\n",
      "快速排序是一种高效的排序算法，采用分治法策略，主要包含三个步骤：分解（Divide）、解决（Conquer）和合并（Combine）。以下是其详细解释：\n",
      "\n",
      "###1. 基本原理- **分解**：选择一个基准元素（pivot），将数组分为两部分，其中一部分所有元素小于等于基准，另一部分大于等于基准。\n",
      "- **解决**：递归地对这两部分进行快速排序。\n",
      "- **合并**：由于分解和解决过程已经确保了各子数组的有序性，因此无需额外合并步骤。\n",
      "\n",
      "###2. 分区操作分区是关键步骤，通常使用双指针法：\n",
      "1. 左指针从左向右寻找大于基准的元素。\n",
      "2. 右指针从右向左寻找小于基准的元素。\n",
      "3. 当找到时交换两者，并继续直到指针相遇，此时将基准放置于正确位置。\n",
      "\n",
      "###3. 时间复杂度分析- **平均情况**：O(n log n)，每次分区大致将数组分为两半，递归深度为log n，每层处理n个元素。\n",
      "- **最坏情况**：O(n²)，当每次选择的基准导致子数组大小分别为0和n-1时发生。\n",
      "- **优化**：通过随机选择或三数取中法选择基准，显著降低最坏情况概率。\n",
      "\n",
      "###4. 空间复杂度快速排序的空间复杂度为O(log n)，主要由递归调用栈占用空间决定。避免栈溢出的方法包括使用尾递归优化或迭代式实现。\n",
      "\n",
      "###5. 示例过程以数组 [3,6,8,10,1,4,7]为例：\n",
      "1.选择基准3，分区后得到 [1,6,8,10,3,4,7]。\n",
      "2.递归处理左右子数组[1]和[6,8,10,3,4,7]。\n",
      "3. 对右边子数组选择基准6，分区为 [3,4,6,8,10,7]，继续递归。\n",
      "\n",
      "### 总结快速排序通过高效的分治策略和优化的基准选择，在平均情况下表现出色，时间复杂度O(n log n)，适用于大规模数据排序。"
     ]
    }
   ],
   "source": [
    "import requests  # type: ignore\n",
    "import json\n",
    "\n",
    "# 设置 API 端点\n",
    "generate_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# 示例数据\n",
    "# 注意: 只有 DeepSeek-R1 系列模型会输出思考过程 <think>...</think>\n",
    "generate_payload = {\n",
    "    \"model\": \"deepseek-r1:32b\",  # 使用 DeepSeek-R1 才能看到思考过程\n",
    "    \"prompt\": \"请详细解释一下快速排序算法的原理和时间复杂度。\",\n",
    "    \"stream\": True,\n",
    "    \"options\": {\n",
    "        \"temperature\": 0.6,\n",
    "        #\"num_predict\": 1000  # DeepSeek-R1 思考过程较长，增加输出限制\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"开始生成内容...\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# 状态跟踪\n",
    "in_thinking = False\n",
    "last_content = \"\"  # 用于检测重复内容\n",
    "\n",
    "try:\n",
    "    with requests.post(generate_url, json=generate_payload, stream=True, timeout=120) as response:\n",
    "        if response.status_code == 200:\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        \n",
    "                        if \"response\" in data:\n",
    "                            content = data[\"response\"]\n",
    "                            \n",
    "                            # 跳过空内容\n",
    "                            if not content or content.isspace():\n",
    "                                continue\n",
    "                            \n",
    "                            # 检测思考过程标记\n",
    "                            if \"<think>\" in content or \"<reasoning>\" in content:\n",
    "                                if not in_thinking:\n",
    "                                    print(\"\\n【💭 思考过程】\")\n",
    "                                    print(\"-\" * 60)\n",
    "                                    in_thinking = True\n",
    "                                content = content.replace(\"<think>\", \"\").replace(\"<reasoning>\", \"\")\n",
    "                            \n",
    "                            if \"</think>\" in content or \"</reasoning>\" in content:\n",
    "                                content = content.replace(\"</think>\", \"\").replace(\"</reasoning>\", \"\")\n",
    "                                if in_thinking:  # 只在真的有思考过程时才打印分隔\n",
    "                                    print(\"\\n\" + \"-\" * 60)\n",
    "                                    print(\"\\n【✨ 最终答案】\")\n",
    "                                    print(\"-\" * 60)\n",
    "                                in_thinking = False\n",
    "                            \n",
    "                            # 输出内容（流式输出不应该有重复）\n",
    "                            print(content, end='', flush=True)\n",
    "                            last_content = content\n",
    "                        \n",
    "                        # 完成时打印统计\n",
    "                        if data.get(\"done\", False):\n",
    "                            print(\"\\n\\n\" + \"=\" * 60)\n",
    "                            print(\"✅ 生成完成\")\n",
    "                            print(\"=\" * 60)\n",
    "                            \n",
    "                            # 统计信息\n",
    "                            if \"total_duration\" in data:\n",
    "                                print(f\"总耗时: {data['total_duration'] / 1e9:.2f} 秒\")\n",
    "                            if \"eval_count\" in data and \"eval_duration\" in data:\n",
    "                                tokens = data[\"eval_count\"]\n",
    "                                duration = data[\"eval_duration\"] / 1e9\n",
    "                                speed = tokens / duration if duration > 0 else 0\n",
    "                                print(f\"生成 tokens: {tokens} ({speed:.2f} tokens/秒)\")\n",
    "                            if \"prompt_eval_count\" in data:\n",
    "                                print(f\"输入 tokens: {data['prompt_eval_count']}\")\n",
    "                            break\n",
    "                            \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"\\n解析错误: {e}\")\n",
    "                        continue\n",
    "        else:\n",
    "            print(f\"❌ 请求失败: {response.status_code}\")\n",
    "            print(f\"错误: {response.text}\")\n",
    "            \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"❌ 请求异常: {e}\")\n",
    "    print(\"提示: 请确保 Ollama 服务正在运行\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个代码的详细对比\n",
    "\n",
    "| 特性 | fufan 的代码 | 我写的代码 |\n",
    "|------|------------|-----------|\n",
    "| **核心功能** | ✅ 基础流式输出 | ✅ 增强流式输出 |\n",
    "| **思考过程识别** | ❌ 无 | ✅ 自动识别 `<think>` 标签 |\n",
    "| **输出格式化** | ❌ 无 | ✅ 区分思考过程和答案 |\n",
    "| **统计信息** | ❌ 只打印完整JSON | ✅ 格式化统计（速度、耗时等）|\n",
    "| **异常处理** | ⚠️ 基础 | ✅ 完善（网络、超时等）|\n",
    "| **资源管理** | ⚠️ 手动管理 | ✅ 使用 `with` 自动管理 |\n",
    "| **空白过滤** | ❌ 无 | ✅ 跳过空白内容 |\n",
    "| **超时设置** | ❌ 无 | ✅ 120秒超时 |\n",
    "| **视觉效果** | ⚠️ 简单 | ✅ 分隔线、标题等 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ollama 模型生命周期管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;默认情况下，<font color=\"red\">**通过`Ollama run`启动一个模型后，会将其在VRAM(显存)中保存5分钟**</font>。主要作用是为了做性能优化，通过保持模型在显存中，可以避免频繁的加载和卸载操作，从而提高响应速度，特别是在连续请求的情况下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"red\">**keep_alive**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们可以通过`ollama stop` 命令立即卸载某个模型。而在生成请求中，一种高效的方式是通过`keep_alive`参数来控制模型在请求完成后保持加载在内存中的时间。其可传入的参数规则如下：\n",
    "\n",
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>keep_alive 参数类型</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数类型               | 示例         | 描述                                       |\n",
    "|------------------------|--------------|--------------------------------------------|\n",
    "| 持续时间字符串         | \"10m\" 或 \"24h\" | 表示保持模型在内存中的时间，单位可以是分钟（m）或小时（h）。 |\n",
    "| 以秒为单位的数字       | 3600         | 表示保持模型在内存中的时间，单位为秒。   |\n",
    "| 任何负数               | -1 或 \"-1m\"  | 表示保持模型在内存中，负数值将使模型持续加载。 |\n",
    "| '0'                    | 0            | 表示在生成响应后立即卸载模型。             |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成响应: {\n",
      "  \"model\": \"deepseek-r1:32b\",\n",
      "  \"created_at\": \"2025-10-03T14:45:59.43996011Z\",\n",
      "  \"response\": \"<think>\\n嗯，用户让我生成一个关于人工智能的简短介绍。首先，我得想想什么是AI的核心概念，可能包括定义、应用领域和一些关键的技术点。\\n\\n然后，用户可能对AI的应用感兴趣，比如机器学习、自然语言处理这些方面。我应该涵盖这些内容，同时保持简洁明了。\\n\\n还要注意不要太技术化，让读者容易理解。或许可以提到一些实际的例子，比如智能助手或自动驾驶，这样更生动。\\n\\n最后，总结一下AI的影响和未来发展的可能性，让用户有个全面的认识。\\n</think>\\n\\n人工智能（Artificial Intelligence, AI）是指由人创造的能够执行通常需要人类智能的任务的系统或机器。这些任务包括学习、推理、问题解决、感知和语言理解等。AI技术广泛应用于多个领域，如自动驾驶、语音助手、医疗诊断、金融分析和娱乐推荐等，极大地改变了我们的生活方式和工作方式。随着技术的进步，人工智能正变得越来越智能化和自主化，为未来带来了无限可能。\",\n",
      "  \"done\": true,\n",
      "  \"done_reason\": \"stop\",\n",
      "  \"context\": [\n",
      "    151644,\n",
      "    14880,\n",
      "    43959,\n",
      "    46944,\n",
      "    101888,\n",
      "    104455,\n",
      "    9370,\n",
      "    98237,\n",
      "    99534,\n",
      "    100157,\n",
      "    1773,\n",
      "    151645,\n",
      "    151648,\n",
      "    198,\n",
      "    106287,\n",
      "    3837,\n",
      "    20002,\n",
      "    104029,\n",
      "    43959,\n",
      "    46944,\n",
      "    101888,\n",
      "    104455,\n",
      "    9370,\n",
      "    98237,\n",
      "    99534,\n",
      "    100157,\n",
      "    1773,\n",
      "    101140,\n",
      "    3837,\n",
      "    35946,\n",
      "    49828,\n",
      "    105839,\n",
      "    106582,\n",
      "    15469,\n",
      "    104867,\n",
      "    101290,\n",
      "    3837,\n",
      "    87267,\n",
      "    100630,\n",
      "    91282,\n",
      "    5373,\n",
      "    99892,\n",
      "    100650,\n",
      "    33108,\n",
      "    101883,\n",
      "    99936,\n",
      "    105535,\n",
      "    27442,\n",
      "    3407,\n",
      "    101889,\n",
      "    3837,\n",
      "    20002,\n",
      "    87267,\n",
      "    32664,\n",
      "    15469,\n",
      "    106736,\n",
      "    103198,\n",
      "    3837,\n",
      "    101912,\n",
      "    102182,\n",
      "    100134,\n",
      "    5373,\n",
      "    99795,\n",
      "    102064,\n",
      "    54542,\n",
      "    100001,\n",
      "    99522,\n",
      "    1773,\n",
      "    35946,\n",
      "    99730,\n",
      "    102994,\n",
      "    100001,\n",
      "    43815,\n",
      "    3837,\n",
      "    91572,\n",
      "    100662,\n",
      "    110485,\n",
      "    30858,\n",
      "    34187,\n",
      "    3407,\n",
      "    104019,\n",
      "    60533,\n",
      "    115596,\n",
      "    99361,\n",
      "    32108,\n",
      "    3837,\n",
      "    99258,\n",
      "    104785,\n",
      "    100047,\n",
      "    101128,\n",
      "    1773,\n",
      "    102130,\n",
      "    73670,\n",
      "    104496,\n",
      "    101883,\n",
      "    99912,\n",
      "    111564,\n",
      "    3837,\n",
      "    101912,\n",
      "    100168,\n",
      "    110498,\n",
      "    57191,\n",
      "    109044,\n",
      "    3837,\n",
      "    99654,\n",
      "    33126,\n",
      "    106267,\n",
      "    3407,\n",
      "    100161,\n",
      "    3837,\n",
      "    102050,\n",
      "    100158,\n",
      "    15469,\n",
      "    104126,\n",
      "    33108,\n",
      "    100353,\n",
      "    104174,\n",
      "    103026,\n",
      "    3837,\n",
      "    115987,\n",
      "    104627,\n",
      "    100011,\n",
      "    107945,\n",
      "    8997,\n",
      "    151649,\n",
      "    271,\n",
      "    104455,\n",
      "    9909,\n",
      "    9286,\n",
      "    16488,\n",
      "    21392,\n",
      "    11,\n",
      "    15235,\n",
      "    7552,\n",
      "    104442,\n",
      "    67071,\n",
      "    17340,\n",
      "    100211,\n",
      "    9370,\n",
      "    100006,\n",
      "    75117,\n",
      "    102119,\n",
      "    85106,\n",
      "    103971,\n",
      "    100168,\n",
      "    108530,\n",
      "    9370,\n",
      "    72448,\n",
      "    57191,\n",
      "    102182,\n",
      "    1773,\n",
      "    100001,\n",
      "    88802,\n",
      "    100630,\n",
      "    100134,\n",
      "    5373,\n",
      "    113272,\n",
      "    5373,\n",
      "    86119,\n",
      "    100638,\n",
      "    5373,\n",
      "    108272,\n",
      "    33108,\n",
      "    102064,\n",
      "    101128,\n",
      "    49567,\n",
      "    1773,\n",
      "    15469,\n",
      "    99361,\n",
      "    100789,\n",
      "    110645,\n",
      "    101213,\n",
      "    100650,\n",
      "    3837,\n",
      "    29524,\n",
      "    109044,\n",
      "    5373,\n",
      "    105761,\n",
      "    110498,\n",
      "    5373,\n",
      "    100182,\n",
      "    105262,\n",
      "    5373,\n",
      "    100015,\n",
      "    101042,\n",
      "    33108,\n",
      "    100415,\n",
      "    101914,\n",
      "    49567,\n",
      "    3837,\n",
      "    112661,\n",
      "    108135,\n",
      "    103952,\n",
      "    107142,\n",
      "    33108,\n",
      "    99257,\n",
      "    75768,\n",
      "    1773,\n",
      "    101067,\n",
      "    99361,\n",
      "    111888,\n",
      "    3837,\n",
      "    104455,\n",
      "    36556,\n",
      "    101197,\n",
      "    100064,\n",
      "    105123,\n",
      "    33108,\n",
      "    100842,\n",
      "    32108,\n",
      "    3837,\n",
      "    17714,\n",
      "    100353,\n",
      "    104923,\n",
      "    105066,\n",
      "    87267,\n",
      "    1773\n",
      "  ],\n",
      "  \"total_duration\": 6352931968,\n",
      "  \"load_duration\": 219984534,\n",
      "  \"prompt_eval_count\": 13,\n",
      "  \"prompt_eval_duration\": 23260845,\n",
      "  \"eval_count\": 206,\n",
      "  \"eval_duration\": 6108577599\n",
      "}\n",
      "Tokens per second: 33.72307164170642\n"
     ]
    }
   ],
   "source": [
    "import requests # type: ignore\n",
    "import json\n",
    "\n",
    "# 设置 API 端点\n",
    "generate_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# 示例数据\n",
    "generate_payload = {\n",
    "    \"model\": \"deepseek-r1:32b\",\n",
    "    \"prompt\": \"请生成一个关于人工智能的简短介绍。\",\n",
    "    \"stream\": False,\n",
    "    \"keep_alive\": \"10m\",   # 设置模型在请求后保持加载的时间\n",
    "    \"options\": {\n",
    "        \"temperature\": 0.6,\n",
    "    }\n",
    "}\n",
    "\n",
    "# 调用生成接口\n",
    "response_generate = requests.post(generate_url, json=generate_payload)\n",
    "if response_generate.status_code == 200:\n",
    "    generate_response = response_generate.json()\n",
    "    print(\"生成响应:\", json.dumps(generate_response, ensure_ascii=False, indent=2))\n",
    "else:\n",
    "    print(\"生成请求失败:\", response_generate.status_code, response_generate.text)\n",
    "\n",
    "\n",
    "\n",
    "if generate_response[\"eval_duration\"] != 0:\n",
    "    tokens_per_second = generate_response[\"eval_count\"] / generate_response[\"eval_duration\"] * 10**9\n",
    "    print(f\"Tokens per second: {tokens_per_second}\")\n",
    "else:\n",
    "    print(\"eval_duration is zero, cannot calculate tokens per second.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此时就可以在服务器控制台查看到，`deepseek-r1:32b`模型将可以在显存中保持10分钟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202502131907776.png\" width=100%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`keep_alive` 在工程化的项目中，往往需要根据请求的频率来设置，如果请求不频繁，可以使用默认值或较短的时间，以便在不使用时释放内存。而如果应用程序需要频繁调用模型，可以设置较长的 `keep_alive` 时间，以减少加载时间。很关键，非常影响服务器的性能和应用程序的用户体验。大家一定要注意。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们进入下一个课件中了解 `/api/chat` 接口。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
