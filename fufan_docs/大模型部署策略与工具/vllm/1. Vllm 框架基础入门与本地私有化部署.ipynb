{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac6d7ec-1b27-48f0-b6d5-232d43267797",
   "metadata": {},
   "source": [
    "# <center>企业级大模型部署推理管理工具</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d61b4-f378-4ec1-a53f-dfe921265258",
   "metadata": {},
   "source": [
    "## <center>Part 1. Vllm 框架基础入门与本地私有化部署</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd232ecf-51f0-4e3e-ba91-0e82948adfff",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对开源大模型来说，虽然模型权重开源，但并不意味着这样的模型就可以开箱即用，而是需要一些框架来支撑其运行和推理。无论哪个公司旗下的模型，现在都在兼容同一个接入规范，即`OpenAI`兼容接口，因为这种`API`规范几乎可以与任何`SDK`或者客户端无缝集成，所以对大模型部署框架而言，是否兼容`OpenAI`规范也就成为了衡量一个其是否成熟、是否可以被广泛使用的重要指标。所以大家都关注的问题就是：部署与`OpenAI API`规范兼容模型的最佳框架是什么？\n",
    "\n",
    "&emsp;&emsp;如下所示，目前主流的大模型部署框架以`llama.cpp`、`Ollama`和`Vllm`为主，其各自项目在`Github`上的`Stars`增长曲线如下所示。其中`Ollama`的`Stars`增长曲线是断档式领先，而`llama.cpp`和`Vllm`的`Stars`增长曲线则相对平缓，处于一个稳步增长的趋势。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c01ff0-bc25-49be-b9b5-5a6a6d70cf57",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181034368.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19320ea1-1e1f-4923-83ac-6b74f1d87736",
   "metadata": {},
   "source": [
    "&emsp;&emsp;之所以会呈现出这种现状，其实与各个框架的定位有本质的区别。首先，`llama.cpp` 是使用没有任何依赖关系的纯 C/C++ 实现，性能很高，可定制化和优化项非常多，但也因为底层是`C`语言，直接导致上手难度极高。`Ollama` 之所以能够受到最多开发者的关注，一个最根本的原因就是其部署和使用太简单了，兼容多种操作系统，且都提供了一键安装、单命令启动的快捷方式，可以极大降低初学者使用开源大模型的门槛，同时`Ollama`框架提供原生 `REST API` 和 `OpenAI API` 兼容性，也可以非常轻松的接入到其他的客户端中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e7e71-3933-41cb-a744-68b41eafaeef",
   "metadata": {},
   "source": [
    "# 1. Vllm 框架整体概览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd37f74d-06a3-4941-9543-1961cd26da80",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果说`Ollama`的优势在于其简洁性，那么`Vllm`则是一个另辟蹊径、优先考虑性能和可扩展性的大模型部署框架，其核心的优化点在`高效内存管理`、`持续批处理功能`和`张量并行性`，从而在生产环境中的高吞吐量场景中表现极佳，同时这也是为什么`Vllm`框架是目前最适用于企业真实生产环境部署的根本原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4dd35-073d-4817-9cfd-3fb38f99812e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`Vllm` 底层是基于`Pytorch` 构建，其`Gtihub` 开源地址为：https://github.com/vllm-project/vllm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdfb210-b21f-42a8-aad2-8ee406226917",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181137533.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba7776-e324-4dda-ab0a-a86279e21a71",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从各种基准测试数据来看，同等配置下，使用 `vLLM` 框架与 `Transformer` 等传统推理库相比，其吞吐量可以提高一个数量级，这归功于以下几个特性：\n",
    "\n",
    "- **高级 GPU 优化**：利用 `CUDA` 和 `PyTorch` 最大限度地提高 `GPU` 利用率，从而实现更快的推理速度。`Ollama`其实是对`CPU-GPU`的混合应用，但`vllm`是针对纯`GPU`的优化。\n",
    "- **高级内存管理**：通过`PagedAttention`算法实现对 `KV cache`的高效管理，减少内存浪费，从而优化大模型的运行效率。\n",
    "- **批处理功能**：支持连续批处理和异步处理，从而提高多个并发请求的吞吐量。\n",
    "- **安全特性**：内置 `API` 密钥支持和适当的请求验证，不像其他完全跳过身份验证的框架。\n",
    "- **易用性**：`vLLM` 与 `HuggingFace` 模型无缝集成，支持多种流行的大型语言模型，并兼容 `OpenAI` 的 `API` 服务器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383e07e-ab22-4aca-8a6f-8842c0973916",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以上核心的优化点原理以及应用方法，我们将在接下来的课程进行详细的讲解和实践。本节课程我们需要先进行`Vllm` 框架的私有化部署的学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b05db-8e6e-4cc7-aa39-68a7f14fd8f3",
   "metadata": {},
   "source": [
    "&emsp;&emsp; `vllm` 框架主要支持纯文本和多模态两种模式的模型，其中对支持的纯文本类语言模型可以在这里看到：https://docs.vllm.ai/en/latest/models/supported_models.html#supported-text-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63d7f1-f09d-4eb4-8d18-7b725d0c53ee",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511604.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5035830-c4c7-4a7a-b6be-d6bc3d3c7055",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其次是多模态模型，`Vllm`框架目前已经支持文本、图片、视频和音频四种模态的输入输出格式兼容规范，但是多模态模型官方并没有给出具体的模型列表，所以这里我们要进入到源码中对模型的集成文件中去匹配：https://github.com/vllm-project/vllm/tree/main/vllm/model_executor/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec9aac-34f4-4856-89fb-c4aa8a9cefbc",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181304597.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3aaf3-bf10-4687-bb69-5763a1384523",
   "metadata": {},
   "source": [
    "&emsp;&emsp;根据各个模型支持`.py`文件中的代码细节，这里梳理了一份目前最新版本`Vllm 0.8.4`支持的多模态模型如下表所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e0cda-055c-4766-a15a-96937da72147",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>多模态模型支持列表</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| .py 文件                          | 模型名称                          | 描述                                       |\n",
    "|----------------------------------|----------------------------------|------------------------------------------|\n",
    "| aya_vision.py                    | aya-vision                         | https://huggingface.co/CohereLabs/aya-vision-8b       |\n",
    "| blip.py / blip2.py              | BLIP                            | 经典视觉-语言模型（BLIP 系列）。               |\n",
    "| clip.py                          | CLIP                            | 经典的多模态对比学习模型（CLIP）。             |\n",
    "| deepseek_vl2.py                  | DeepSeek                       | DeepSeek 的多模态版本（VL 表示 Vision-Language）。 |\n",
    "| florence2.py                    | 微软模型                        | 微软的多模态模型。https://huggingface.co/microsoft/Florence-2-large                           |\n",
    "| fuyu.py                          | Fuyu                           | Adept 的多模态模型（Fuyu 系列）。              |\n",
    "| gemma3_mm.py                     | Gemma                          | mm 表示 Multimodal（Gemma 的多模态扩展）。    |\n",
    "| glm4v.py                         | GLM-4                          | GLM-4 的多模态版本（v 表示 Vision）。         |\n",
    "| h2ovl.py                         | H2O                            | H2O 的视觉-语言模型（vl 后缀）。              |\n",
    "| idefics2_vision_model.py / idefics3.py | Idefics                     | Meta 的多模态模型（Idefics 系列）。       |\n",
    "| internvl.py                      | InternVL-Chat                   | 上海 AI Lab 的多模态模型。                   |\n",
    "| kimi_vl.py                       | Kimi-VL-A3B                        | 新增的 Kimi-VL 模型  |\n",
    "| llava.py / llava_next.py / llava_next_video.py / llava_onevision.py | LLaVA                        | LLaVA 系列及其变种（开源视觉-语言模型）。 |\n",
    "| mllama4.py                       | LLaMA-4                       | 多模态版本的 LLaMA-4。                      |\n",
    "| molmo.py                         | Molmo                     | 提交记录涉及多模态数据处理。                   |\n",
    "| moonvit.py                       | Kimi-VL                        | 与 Kimi-VL 同时新增的多模态模型。             |\n",
    "| nvlm_d.py                        | NVLM-D-72B                         | NVIDIA 的多模态模型（vl 相关）。              |\n",
    "| paligemma.py                     | PaLI-Gemma                     | Google 的 PaLI-Gemma 多模态模型。            |\n",
    "| phi3v.py / phi4mm.py / phi4mm_audio.py | Phi-3 / Phi-4               | Phi-3 和 Phi-4 的多模态版本（支持视觉或音频）。 |\n",
    "| prithvi_geospatial_mae.py        | Prithvi-EO-1.0                    | 地理空间多模态模型（NASA 合作项目）。         |\n",
    "| qwen2_5_vl.py / qwen2_audio.py / qwen2_vl.py / qwen_vl.py | 通义千问                     | 通义千问的多模态版本（视觉或音频）。      |\n",
    "| siglip.py                        | SigLIP                        | Google 的 SigLIP（多模态对比学习）。         |\n",
    "| skyworkr1v.py                    | Skywork-R1V-38B                       | 幻方多模态模型（v 后缀）。                   |\n",
    "| smolvlm.py                       | SmolVLM                     | 轻量级多模态模型（新增支持）。     |\n",
    "| whisper.py                       | whisper-large-v3                   | OpenAI 的语音-文本多模态模型（虽然主要处理音频，但属于跨模态）。 |\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b93f3-03fd-430d-9f21-574063b6002d",
   "metadata": {},
   "source": [
    "# 2. Linux 操作系统部署 Vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42fc263-e5ec-4f90-b640-ebd5c4dc20b2",
   "metadata": {},
   "source": [
    "&emsp;&emsp; 首先需要重点说明的是：<font color=\"red\">`Vllm` 框架仅支持`Linux` 操作系统，官方并没有提供`Windows`的兼容版本</font>。同时除了有操作的限制，<font color=\"red\">对运行的`Python`版本也要求在`Python 3.8` ~ `Python 3.12` 之间</font>。这两个条件限制为部署`Vllm`的先决条件，即必须满足才可以顺利使用`Vllm`启动大模型并提供推理服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1ddbd-735d-4be9-8e2a-3157fd1dcee0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这个策略非常符合企业级的部署策略，`Vllm` 框架作为目前在实际生产环境中使用最为广泛的框架之一，其对`Linux` 操作系统的支持，以及对`Python` 版本的限制，都是为了确保在生产环境中能够稳定运行，并且能够提供高性能的推理服务。（在生产环境中，没有任何一家企业会在`Windows` 操作系统上部署模型服务或者应用服务）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074f208-c85a-4c99-b077-3290ae48d215",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因此，我们接下来就严格按照操作系统和`Python` 版本的要求，来进行`Vllm` 框架部署和使用的详细讲解和实践。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad4853-ba8f-418f-ba3f-5564b2b1fe40",
   "metadata": {},
   "source": [
    "&emsp;&emsp; 这里我们采用的`Linux` 操作系统是`Ubuntu 22.04`，同时配置四张`3090` 显卡来进行`Vllm` 框架的部署和使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cace774-f991-4201-855f-4c82203be4e8",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511611.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef525dd7-f585-4390-a35a-8c76e95a5318",
   "metadata": {},
   "source": [
    "&emsp;&emsp; `Vllm` 工具已经通过编译并上传到`PYPI` 仓库以及主流的包管理平台中，所以可以直接使用如`pip`、`conda` 等工具进行快速安装。这里我们使用比较常用的`conda` 包版本管理工具进行安装部署。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd71f5-25c2-4667-a09f-75778358ed11",
   "metadata": {},
   "source": [
    "- **Step 1. 安装`conda` 包版本管理工具**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48220cc-314b-47e0-9fe7-4faf3b4452fe",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先需要检查当前使用的操作系统是否已经安装了`conda` 包版本管理工具，可以通过`conda --version` 命令查看`Conda` 版本，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e78b2ce-2486-4a06-92ed-a0bf1c139766",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171523767.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2bc68-764f-42bf-a863-7dfbc786360e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果出现`Conda not found` 等报错，需要先安装`Conda` 环境，再执行接下来的步骤。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d1c96-fa70-4aca-a9c9-4d0bac98f239",
   "metadata": {},
   "source": [
    "- **Step 2. 创建Python虚拟环境**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51efe9e4-619a-40b6-8e22-068c0722a066",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`vllm`官方要求的是`Python 3.9` ~ `Python 3.12` 之间的版本，这里我们选择`Python 3.12` 版本。执行如下命令进行创建：\n",
    "\n",
    "```bash\n",
    "    conda create --name vllm python=3.12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8b92ed-7c4c-4621-9c10-d0eff2fc6dbf",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511612.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882626da-37dc-4f69-af24-3e65ac6b82db",
   "metadata": {},
   "source": [
    "- **Step 3. 激活虚拟环境**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce657cd-d7dc-448a-9983-c29c25662887",
   "metadata": {},
   "source": [
    "&emsp;&emsp;创建完虚拟环境后，使用`Conda` 激活虚拟环境，通过`conda activate vllm` 命令激活，如下图所示：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a720ee-34ab-41d4-a248-df81cd1ece65",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511613.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a512dd6-b835-4854-92b7-574149cb1984",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，使用`pip` 安装`vllm` 框架，执行如下命令：\n",
    "\n",
    "```bash\n",
    "    pip install vllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa5c79-7734-4b91-9747-6975e383cde5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511614.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a3ab0-87ad-4cf3-a3a9-38fb3c4b4eea",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此时耐心等待安装完成即可。待安装完成后，可以使用`pip show vllm` 命令查看`vllm` 框架的安装信息，可以明确看到当前安装的`vllm` 版本号。如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b33c6f-a2c6-423f-8cae-fa5dca829408",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511615.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581a63a-eb40-40b6-be49-14fecd6f1430",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，`Vllm` 框架的安装就已经完成了。整个过程就是非常常规的依赖包安装过程，唯一需要注意的就是`Python` 版本的要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae8a84-f679-403e-8eae-9fa296faaf94",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在安装完成`Vllm` 框架后，就可以开始进行大模型推理服务的启动和调用了。`Vllm` 框架提供了两种启动并调用大模型生成推理服务的方式，分别是<font color=\"red\">离线推理</font>和<font color=\"red\">在线推理</font>。其中：\n",
    "\n",
    "- **离线推理**： 离线推理类似于使用 `Pytorch` 模块一样，当我们需要使用大模型生成推理服务时，先加载模型，然后使用输入数据运行该模型，并获取输出结果。\n",
    "- **在线推理**： 在线推理类似于有一个服务器，可以先启动大模型，然后等待来自客户端的请求，一旦接收到请求，就会使用大模型生成推理服务，并返回结果，并且可以同时处理多个请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986fb51-691b-4411-97d4-8044c9c67048",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这是两种不同推理方式最本质的区别，毫无疑问<font color=\"red\">在线推理是更加符合实际生产环境的</font>。但为了帮助大家更好的理解`Vllm` 框架，这里我们还是先从离线推理开始讲解，然后再重点讲解在线推理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4f75a-7860-4cb1-829b-58eb11754935",
   "metadata": {},
   "source": [
    "# 3. vllm 离线推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c90054-c5fc-4afd-b4b7-1acc74d361af",
   "metadata": {},
   "source": [
    "&emsp;&emsp;无论是在线推理还是离线推理，都采用的是`API` 接口的方式来调用大模型生成推理服务。对离线推理来说，其加载模型、传输数据、获取结果的`API` 调用形式如下所示：\n",
    "\n",
    "```python\n",
    "    from vllm import LLM\n",
    "    # 加载模型\n",
    "    llm = LLM(model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")\n",
    "    \n",
    "    # 传输数据\n",
    "    outputs = llm.generate(\"你好，我是木羽\")\n",
    "\n",
    "    # 获取结果\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571cafc-7e65-421e-a9da-98bc9e84913d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;代码看起来简单，但如果想要跑通这几行代码，则需要我们提前做两件非常关键的准备工作：<font color=\"red\">其一是需要在`Linux` 服务器上下载大模型权重，其二则是需要配置一个可以加载服务器虚拟环境（即上一步创建的`vllm` 虚拟环境）的`Python IDE`解释器</font>。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842b864-edef-4180-b559-b58b91a9d00a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们就以`DeepSeek-R1-Distill-Qwen-32B` 大模型为例，默认情况下，`LLM(model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")` 这行命令的执行逻辑是：先检测当前服务器中是否存在`deepseek-ai/DeepSeek-R1-Distill-Qwen-32B` 的模型权重，如果存在则直接加载该目录下的模型权重，否则会从`Hugging Face` 上下载大模型权重。其存在的主要问题是：国内的服务器是没有办法访问`Hugging Face` 网站的，所以一定会报错。因此，国内的开发者需要采用更有效的方式来确保大模型权重的正确下载，即使用国内镜像源`ModelScope` 来执行模型权重的本地化存储。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846e1dc-c8a8-4bc7-9d5f-f697f1a4384d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;回到`Linux` 服务器中，首先通过如下命令安装`ModelScope` 的`pip` 包：\n",
    "\n",
    "```bash\n",
    "    pip install modelscope\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a046126-c609-4cde-a5f4-28803a1f3942",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511620.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7061e9-9aa9-48b7-97e9-3452878ffd5b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，进入到`ModelScope` 的主页中：https://www.modelscope.cn/home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0cf7f-396f-4ac7-a522-4ddd211a0158",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171613863.png\" width=80%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653e76a-3985-4a53-885b-d93af70ba946",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过关键词找到对应的模型详情页，比如我这里以`DeepSeek-R1-Distill-Qwen-32B` 为例，点击进入详情页，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97e285-0e7c-4048-94ef-261ea1fb6d06",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171613865.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183551b-e38e-4868-a7c2-aaef05ca5944",
   "metadata": {},
   "source": [
    "&emsp;&emsp;进入到模型详情页后，点击`模型文件`，然后再找到`下载模型`按钮，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e947817-ca64-489b-bad2-e3b61ca0aa49",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171613866.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f27b0-992d-4469-8c10-b70e3e86d0cc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在弹出来的会话框中，找到`SDK 下载` 标签，复制对应的模型服务指针标识：如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b72ead-9cea-4e27-930f-a359b332dba0",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171613867.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d2d15-1eca-46e4-91f0-18cb61620c3f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来回到`Linux` 服务器中，通过`vim` 新建一个`model_download.py` 文件：\n",
    "\n",
    "```python\n",
    "    vim model_download.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08ab66-779b-4c82-b001-1866191478da",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511621.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17758ea-b1de-4d6e-b1c3-a4daed6454ce",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后，按`i` 键进入编辑模式，将复制的代码粘贴到文件中：\n",
    "\n",
    "```python\n",
    "    from modelscope import snapshot_download\n",
    "\n",
    "    # model_dir = snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', cache_dir='/root/autodl-tmp', revision='master')\n",
    "    model_dir = snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', cache_dir='/home/08_vllm', revision='master')\n",
    "```\n",
    "\n",
    "&emsp;&emsp;注意，这里`cache_dir` 参数指定的路径是模型权重存储的路径，这里我们指定为`/home/08_vllm` 目录，即模型权重会存储到该目录下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929f9da-5a74-4acc-a3ac-42f52086fe74",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511622.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b0a4ec-3106-48c5-a99c-306330e6db02",
   "metadata": {},
   "source": [
    "&emsp;&emsp;修改完成后，按`Esc` 键退出编辑模式，再按`Shift+:` 键进入命令模式，输入`wq` 保存并退出。执行如下命令，运行`model_download.py` 文件：\n",
    "\n",
    "```bash\n",
    "    python model_download.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a7412-b60f-4854-8494-80092b8b5bc4",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511623.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f4cdb-60b4-4ce8-8473-979e677974a0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;耐心等待模型权重下载完成即可。待下载完成后，可以看到`/home/08_vllm` 目录下会多出一个`deepseek-ai` 文件夹，里面存放的就是`DeepSeek-R1-Distill-Qwen-32B` 大模型的权重文件。如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130dc5ea-3f9b-40d8-be09-dbf8b1ff91c6",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171622553.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f74db-3f1d-4968-9a44-acb21d088c7f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;按照上述流程中`model_download.py`下载模型的方式，可以允许我们随意下载任意在`ModelScope`中托管的模型权重至本地，因此这里依次下载了`Qwen2.5`系列和`DeepSeek R1` 系列的不同尺寸的模型，大家也可以根据自己的需求灵活选择模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc04d7-9201-43ae-a279-51992d227c9a",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181342835.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b665c2-f4bf-4cef-babd-64e1b702d8e5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，大模型权重就已经下载完成了。接下来，我们需要做的是配置一个可以加载服务器虚拟环境（即上一步创建的`vllm` 虚拟环境）的`Python IDF`解释器。这一步就会根据大家实际的开发情况产生不同的配置方法，总的来说会分成以下两种情况：\n",
    "\n",
    "1. 部署大模型的服务器和你要执行代码调用的`Python IDE`是同一台服务器，则可以直接选择到对应的虚拟环境并执行代码；\n",
    "2. 部署大模型的服务器和你要执行代码调用的`Python IDE`不是同一台服务器，则需要通过`SSH` 连接到部署大模型的服务器，然后选择到对应的虚拟环境并执行代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9bbb5-07f4-4b24-a9bf-233b766502f1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同一台服务器的情况非常简单，直接打开`Python IDE`，选择到对应的虚拟环境并执行代码即可。这里重点介绍第二种情况，即部署大模型的服务器和你要执行代码调用的`Python IDE`不是同一台服务器。同时这也是最常见的企业开发环境。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f11059-4beb-41c7-ba8d-f62d42a42652",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这类情况用最通俗的理解是：我们把大模型部署在了局域网/租赁的云服务器中，希望可以在本地电脑上执行代码调用模型服务。这类情况我们要做如下配置：（以`Jupyter Notebook`为例）:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb08a9-bc5b-4fce-9d1a-fe16513f4609",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先进入在局域网/租赁的云服务器的`Linux` 服务器中，在`vllm` 虚拟环境中安装`ipykernel` 和`jupyter` 包，执行如下命令：\n",
    "```bash\n",
    "    pip install ipykernel jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef4ff9-2b35-48fd-819f-4c3c8a81b68b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511616.png\" width=80%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2655702-679c-4669-a7ee-88348da1217e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;安全起见，对连接时的密码进行加密处理，否则明文写在配置文件中，容易造成数据安全风险，依次执行如下操作：\n",
    "```bash\n",
    "    from jupyter_server.auth import passwd\n",
    "    passwd()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8c26b-509e-4233-b708-630abe6b9a79",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753935.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a9af9-feca-4e44-abdd-75facc5d4fb1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;完成密码加密后，执行如下命令生成Jupyter Lab 的配置文件（jupyter_lab_config.py）：\n",
    "```bash\n",
    "    jupyter lab --generate-config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad06f8d-3531-428b-8259-d453bba3539b",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753934.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697ec12-66bc-4a39-b1cf-d76400c20bc9",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用`Vim`编辑器，找到如下配置，执行修改：\n",
    "```bash\n",
    "    c.ServerApp.allow_origin = '*'\n",
    "    c.ServerApp.allow_remote_access = True\n",
    "    c.ServerApp.ip = '0.0.0.0'\n",
    "    c.ServerApp.open_browser = False  \n",
    "    c.ServerApp.password = '加密后的密码'（上一步复制的加密串）\n",
    "    c.ServerApp.port = 8002 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72049b-a5a1-4878-af61-69abf43f662d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753936.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a560de-45ef-4b76-8c11-37493d5221c6",
   "metadata": {},
   "source": [
    "&emsp;&emsp;全部配置完成后，在服务器端启动Jupyter Lab服务，通过如下命令后台启动：\n",
    "```bash\n",
    "    nohup jupyter lab --allow-root > jupyterlab.log 2>&1 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4529e31-dbe2-4c93-af0e-81827dd10ec5",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://snowball101.oss-cn-beijing.aliyuncs.com/img/202401261753937.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd48952-9a89-4af8-b85c-f837d7966263",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后，将虚拟环境的`Kernel`写入`Jupyter Lab`，执行如下命令，创建一个`ipykernel` 内核：\n",
    "```bash\n",
    "    python -m ipykernel install --user --name vllm --display-name \"vllm\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0194c-f041-404a-b986-52ee3cf8424f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511617.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf0694-b6b3-4ed8-ae20-2277221ad906",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，在浏览器中打开`Jupyter Notebook` 服务，在`Jupyter kernel` 中就会出现选择`vllm` 内核的选项，如下图所示：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e7535-1804-4124-8289-293e87413e00",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511618.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64425fe7-00f8-455a-9d7c-411564a2a5be",
   "metadata": {},
   "source": [
    "&emsp;&emsp;选择`vllm` 内核并创建一个新的`notebook` 笔记本，可以通过`! pip show vllm` 快速验证是否正常加载到了`vllm` 的虚拟环境：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c3d79-2f56-48a9-93d9-20051d74e06d",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504171511619.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37940554-b48f-429b-9012-c2d52d4e4201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: vllm\n",
      "Version: 0.8.4\n",
      "Summary: A high-throughput and memory-efficient inference and serving engine for LLMs\n",
      "Home-page: https://github.com/vllm-project/vllm\n",
      "Author: vLLM Team\n",
      "Author-email: \n",
      "License: Apache License\n",
      "                           Version 2.0, January 2004\n",
      "                        http://www.apache.org/licenses/\n",
      "\n",
      "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "\n",
      "   1. Definitions.\n",
      "\n",
      "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "      and distribution as defined by Sections 1 through 9 of this document.\n",
      "\n",
      "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
      "      the copyright owner that is granting the License.\n",
      "\n",
      "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
      "      other entities that control, are controlled by, or are under common\n",
      "      control with that entity. For the purposes of this definition,\n",
      "      \"control\" means (i) the power, direct or indirect, to cause the\n",
      "      direction or management of such entity, whether by contract or\n",
      "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "\n",
      "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
      "      exercising permissions granted by this License.\n",
      "\n",
      "      \"Source\" form shall mean the preferred form for making modifications,\n",
      "      including but not limited to software source code, documentation\n",
      "      source, and configuration files.\n",
      "\n",
      "      \"Object\" form shall mean any form resulting from mechanical\n",
      "      transformation or translation of a Source form, including but\n",
      "      not limited to compiled object code, generated documentation,\n",
      "      and conversions to other media types.\n",
      "\n",
      "      \"Work\" shall mean the work of authorship, whether in Source or\n",
      "      Object form, made available under the License, as indicated by a\n",
      "      copyright notice that is included in or attached to the work\n",
      "      (an example is provided in the Appendix below).\n",
      "\n",
      "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
      "      form, that is based on (or derived from) the Work and for which the\n",
      "      editorial revisions, annotations, elaborations, or other modifications\n",
      "      represent, as a whole, an original work of authorship. For the purposes\n",
      "      of this License, Derivative Works shall not include works that remain\n",
      "      separable from, or merely link (or bind by name) to the interfaces of,\n",
      "      the Work and Derivative Works thereof.\n",
      "\n",
      "      \"Contribution\" shall mean any work of authorship, including\n",
      "      the original version of the Work and any modifications or additions\n",
      "      to that Work or Derivative Works thereof, that is intentionally\n",
      "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
      "      or by an individual or Legal Entity authorized to submit on behalf of\n",
      "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
      "      means any form of electronic, verbal, or written communication sent\n",
      "      to the Licensor or its representatives, including but not limited to\n",
      "      communication on electronic mailing lists, source code control systems,\n",
      "      and issue tracking systems that are managed by, or on behalf of, the\n",
      "      Licensor for the purpose of discussing and improving the Work, but\n",
      "      excluding communication that is conspicuously marked or otherwise\n",
      "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
      "\n",
      "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
      "      on behalf of whom a Contribution has been received by Licensor and\n",
      "      subsequently incorporated within the Work.\n",
      "\n",
      "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
      "      this License, each Contributor hereby grants to You a perpetual,\n",
      "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "      copyright license to reproduce, prepare Derivative Works of,\n",
      "      publicly display, publicly perform, sublicense, and distribute the\n",
      "      Work and such Derivative Works in Source or Object form.\n",
      "\n",
      "   3. Grant of Patent License. Subject to the terms and conditions of\n",
      "      this License, each Contributor hereby grants to You a perpetual,\n",
      "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "      (except as stated in this section) patent license to make, have made,\n",
      "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
      "      where such license applies only to those patent claims licensable\n",
      "      by such Contributor that are necessarily infringed by their\n",
      "      Contribution(s) alone or by combination of their Contribution(s)\n",
      "      with the Work to which such Contribution(s) was submitted. If You\n",
      "      institute patent litigation against any entity (including a\n",
      "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
      "      or a Contribution incorporated within the Work constitutes direct\n",
      "      or contributory patent infringement, then any patent licenses\n",
      "      granted to You under this License for that Work shall terminate\n",
      "      as of the date such litigation is filed.\n",
      "\n",
      "   4. Redistribution. You may reproduce and distribute copies of the\n",
      "      Work or Derivative Works thereof in any medium, with or without\n",
      "      modifications, and in Source or Object form, provided that You\n",
      "      meet the following conditions:\n",
      "\n",
      "      (a) You must give any other recipients of the Work or\n",
      "          Derivative Works a copy of this License; and\n",
      "\n",
      "      (b) You must cause any modified files to carry prominent notices\n",
      "          stating that You changed the files; and\n",
      "\n",
      "      (c) You must retain, in the Source form of any Derivative Works\n",
      "          that You distribute, all copyright, patent, trademark, and\n",
      "          attribution notices from the Source form of the Work,\n",
      "          excluding those notices that do not pertain to any part of\n",
      "          the Derivative Works; and\n",
      "\n",
      "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
      "          distribution, then any Derivative Works that You distribute must\n",
      "          include a readable copy of the attribution notices contained\n",
      "          within such NOTICE file, excluding those notices that do not\n",
      "          pertain to any part of the Derivative Works, in at least one\n",
      "          of the following places: within a NOTICE text file distributed\n",
      "          as part of the Derivative Works; within the Source form or\n",
      "          documentation, if provided along with the Derivative Works; or,\n",
      "          within a display generated by the Derivative Works, if and\n",
      "          wherever such third-party notices normally appear. The contents\n",
      "          of the NOTICE file are for informational purposes only and\n",
      "          do not modify the License. You may add Your own attribution\n",
      "          notices within Derivative Works that You distribute, alongside\n",
      "          or as an addendum to the NOTICE text from the Work, provided\n",
      "          that such additional attribution notices cannot be construed\n",
      "          as modifying the License.\n",
      "\n",
      "      You may add Your own copyright statement to Your modifications and\n",
      "      may provide additional or different license terms and conditions\n",
      "      for use, reproduction, or distribution of Your modifications, or\n",
      "      for any such Derivative Works as a whole, provided Your use,\n",
      "      reproduction, and distribution of the Work otherwise complies with\n",
      "      the conditions stated in this License.\n",
      "\n",
      "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
      "      any Contribution intentionally submitted for inclusion in the Work\n",
      "      by You to the Licensor shall be under the terms and conditions of\n",
      "      this License, without any additional terms or conditions.\n",
      "      Notwithstanding the above, nothing herein shall supersede or modify\n",
      "      the terms of any separate license agreement you may have executed\n",
      "      with Licensor regarding such Contributions.\n",
      "\n",
      "   6. Trademarks. This License does not grant permission to use the trade\n",
      "      names, trademarks, service marks, or product names of the Licensor,\n",
      "      except as required for reasonable and customary use in describing the\n",
      "      origin of the Work and reproducing the content of the NOTICE file.\n",
      "\n",
      "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
      "      agreed to in writing, Licensor provides the Work (and each\n",
      "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
      "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
      "      implied, including, without limitation, any warranties or conditions\n",
      "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
      "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
      "      appropriateness of using or redistributing the Work and assume any\n",
      "      risks associated with Your exercise of permissions under this License.\n",
      "\n",
      "   8. Limitation of Liability. In no event and under no legal theory,\n",
      "      whether in tort (including negligence), contract, or otherwise,\n",
      "      unless required by applicable law (such as deliberate and grossly\n",
      "      negligent acts) or agreed to in writing, shall any Contributor be\n",
      "      liable to You for damages, including any direct, indirect, special,\n",
      "      incidental, or consequential damages of any character arising as a\n",
      "      result of this License or out of the use or inability to use the\n",
      "      Work (including but not limited to damages for loss of goodwill,\n",
      "      work stoppage, computer failure or malfunction, or any and all\n",
      "      other commercial damages or losses), even if such Contributor\n",
      "      has been advised of the possibility of such damages.\n",
      "\n",
      "   9. Accepting Warranty or Additional Liability. While redistributing\n",
      "      the Work or Derivative Works thereof, You may choose to offer,\n",
      "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
      "      or other liability obligations and/or rights consistent with this\n",
      "      License. However, in accepting such obligations, You may act only\n",
      "      on Your own behalf and on Your sole responsibility, not on behalf\n",
      "      of any other Contributor, and only if You agree to indemnify,\n",
      "      defend, and hold each Contributor harmless for any liability\n",
      "      incurred by, or claims asserted against, such Contributor by reason\n",
      "      of your accepting any such warranty or additional liability.\n",
      "\n",
      "   END OF TERMS AND CONDITIONS\n",
      "\n",
      "   APPENDIX: How to apply the Apache License to your work.\n",
      "\n",
      "      To apply the Apache License to your work, attach the following\n",
      "      boilerplate notice, with the fields enclosed by brackets \"[]\"\n",
      "      replaced with your own identifying information. (Don't include\n",
      "      the brackets!)  The text should be enclosed in the appropriate\n",
      "      comment syntax for the file format. We also recommend that a\n",
      "      file or class name and description of purpose be included on the\n",
      "      same \"printed page\" as the copyright notice for easier\n",
      "      identification within third-party archives.\n",
      "\n",
      "   Copyright [yyyy] [name of copyright owner]\n",
      "\n",
      "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "   you may not use this file except in compliance with the License.\n",
      "   You may obtain a copy of the License at\n",
      "\n",
      "       http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "   Unless required by applicable law or agreed to in writing, software\n",
      "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "   See the License for the specific language governing permissions and\n",
      "   limitations under the License.\n",
      "\n",
      "Location: /root/anaconda3/lib/python3.12/site-packages\n",
      "Requires: aiohttp, blake3, cachetools, cloudpickle, compressed-tensors, depyf, einops, fastapi, filelock, gguf, huggingface-hub, importlib_metadata, lark, llguidance, lm-format-enforcer, mistral_common, msgspec, ninja, numba, numpy, openai, opencv-python-headless, opentelemetry-api, opentelemetry-exporter-otlp, opentelemetry-sdk, opentelemetry-semantic-conventions-ai, outlines, partial-json-parser, pillow, prometheus-fastapi-instrumentator, prometheus_client, protobuf, psutil, py-cpuinfo, pydantic, python-json-logger, pyyaml, pyzmq, ray, requests, scipy, sentencepiece, setuptools, six, tiktoken, tokenizers, torch, torchaudio, torchvision, tqdm, transformers, typing_extensions, watchfiles, xformers, xgrammar\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# ! pip show vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831fab8-e8ee-418b-a94a-38f9e62826c8",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果正常加载到了`vllm` 的虚拟环境，则可以看到`vllm` 的版本号，这里可以看到，与我们在服务器上安装的版本号是一致的。说明我们已经在本地电脑上成功加载到了`vllm` 的虚拟环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19b3f7-0ec8-4024-b527-1e28563d172b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，我们可以通过离线推理的`API` 接口来调用大模型生成推理服务。我们会对对话模型和推理模型依次展开详细的测试。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603245c-33ef-4d3b-9919-32859168bc51",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先以`Qwen2.5`系列为例测试在`vllm`框架下对对话类模型的使用。默认情况下，`Vllm` 从`huggingface`下载模型权重，并使用`transformers`库加载模型。这里我们的模型是从`ModelScope`下载的，所以在调用离线推理`API`时，需要搭配`trust_remote_code=True`参数，其次，对`model` 参数，指的是模型权重文件的本地路径，因此需要大家根据模型下载路径进行修改。即实例化代码如下图所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b020312b-dc3c-433d-a8f2-9104de573317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-05 16:59:33 [utils.py:233] non-default args: {'trust_remote_code': True, 'max_model_len': 4096, 'disable_log_stats': True, 'model': '/root/autodl-tmp/vllm/Qwen/Qwen3-4B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-05 16:59:33 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 10-05 16:59:33 [model.py:1510] Using max model len 4096\n",
      "INFO 10-05 16:59:33 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:33 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:33 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/root/autodl-tmp/vllm/Qwen/Qwen3-4B', speculative_config=None, tokenizer='/root/autodl-tmp/vllm/Qwen/Qwen3-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/vllm/Qwen/Qwen3-4B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:34 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m WARNING 10-05 16:59:35 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:35 [gpu_model_runner.py:2602] Starting to load model /root/autodl-tmp/vllm/Qwen/Qwen3-4B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:35 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:35 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69b8939cbd646c1af091c06845518e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:39 [default_loader.py:267] Loading weights took 3.70 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:40 [gpu_model_runner.py:2653] Model loading took 7.5552 GiB and 3.953302 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:46 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/f2d0106e42/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:46 [backends.py:559] Dynamo bytecode transform time: 5.78 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:49 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.091 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:50 [monitor.py:34] torch.compile takes 5.78 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:52 [gpu_worker.py:298] Available KV cache memory: 12.19 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:52 [kv_cache_utils.py:1087] GPU KV cache size: 88,784 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:52 [kv_cache_utils.py:1091] Maximum concurrency for 4,096 tokens per request: 21.68x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 16.08it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 19.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:59 [gpu_model_runner.py:3480] Graph capturing finished in 7 secs, took 0.84 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=20463)\u001b[0;0m INFO 10-05 16:59:59 [core.py:210] init engine (profile, create kv cache, warmup model) took 19.23 seconds\n",
      "INFO 10-05 17:00:00 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(model=\"/root/autodl-tmp/vllm/Qwen/Qwen3-4B\",\n",
    "          trust_remote_code=True,\n",
    "          max_model_len=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840dea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 释放显存\n",
    "\n",
    "# 删除 LLM 对象\n",
    "del llm\n",
    "\n",
    "# 触发垃圾回收\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 清空 CUDA 缓存\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cacd19-7941-4abb-a67b-0a9c7db2d646",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`LLM` 类是运行离线推理的主要类。当使用`LLM`类进行模型初始化的加载时，有如下几个最初级且重要的参数需要大家掌握：\n",
    "\n",
    "1. **device**: 该参数会指定模型加载的设备，vllm 支持在 `cuda, neuron, cpu, tpu, xpu, hpu`，默认是`auto`, 即初始化时会自动识别当前系统中可用的设备。\n",
    "2. **tensor_paralleis***ze\"：该参数会指定模型可以通过`GPU`的`Tensor`并行运行，默认是`1`，即不使用`GPU`的`Tensor`并行，但如果加载的模型参数超过单个`GPU`的显存，即使服务器上有多个`GPU`，也会报错`Out of memory`，并不会自动使用其他`GPU`进行并行。\n",
    "3. **gpu_memory_utilization**：该参数会指定模型加载时，`GPU`的显存取值范围为`0~1`，使用率，默认是`0.90`，即`GPU`的显存使用率不会0过`95%`，但该参数会直接占用`GPU`的显存，以避免`GPU`的显存被其他进占用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9510e5-913c-4cca-afa3-19ba469e9f6d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;比如对`Qwen2.5-7B`模型，采用默认配置其显存占用情况如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76069aa-6a03-4e4a-ab5b-8d904be17efe",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181430516.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7ef6c-6927-45f1-96b3-8f482860ec63",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果模型较大，则可以指定多块`GPU`，比如如下所示的参数调整："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a487aab-8ff8-492e-b206-a37026275250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-05 17:25:06 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-05 17:25:09 [utils.py:233] non-default args: {'trust_remote_code': True, 'max_model_len': 1024, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': '/root/autodl-tmp/vllm/Qwen/Qwen3-4B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-05 17:25:09 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-05 17:25:09 [model.py:1510] Using max model len 1024\n",
      "INFO 10-05 17:25:09 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:10 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:10 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/root/autodl-tmp/vllm/Qwen/Qwen3-4B', speculative_config=None, tokenizer='/root/autodl-tmp/vllm/Qwen/Qwen3-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/vllm/Qwen/Qwen3-4B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:12 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m WARNING 10-05 17:25:13 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:13 [gpu_model_runner.py:2602] Starting to load model /root/autodl-tmp/vllm/Qwen/Qwen3-4B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:13 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:13 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df90576c43814f3f8002bef649d1d801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:15 [default_loader.py:267] Loading weights took 2.01 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:16 [gpu_model_runner.py:2653] Model loading took 7.5552 GiB and 2.242479 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:22 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/7faaa76617/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:22 [backends.py:559] Dynamo bytecode transform time: 5.73 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:25 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.125 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:26 [monitor.py:34] torch.compile takes 5.73 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:28 [gpu_worker.py:298] Available KV cache memory: 9.84 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:28 [kv_cache_utils.py:1087] GPU KV cache size: 71,616 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:28 [kv_cache_utils.py:1091] Maximum concurrency for 1,024 tokens per request: 69.94x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 15.96it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:35 [gpu_model_runner.py:3480] Graph capturing finished in 7 secs, took 0.84 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=31880)\u001b[0;0m INFO 10-05 17:25:35 [core.py:210] init engine (profile, create kv cache, warmup model) took 19.24 seconds\n",
      "INFO 10-05 17:25:36 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(model=\"/root/autodl-tmp/vllm/Qwen/Qwen3-4B\",\n",
    "          trust_remote_code=True,\n",
    "          # tensor_parallel_size=2,\n",
    "          gpu_memory_utilization=0.8,\n",
    "          max_model_len=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109362d-a9a7-4cac-90b7-f36837a7d1a4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此时服务器的`GPU`资源占用情况就如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7324f-7578-4463-a222-bd475b7881b3",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181435870.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbee86-9608-49a4-90c8-d353d0b5ed47",
   "metadata": {},
   "source": [
    "&emsp;&emsp;大家可以根据自己实际加载模型的大小及硬件资源情况进行灵活的调整，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d8685c-7ea2-47a7-94e5-d0f5982cd325",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在加载好模型实例后，便可以进行远程模型服务的调用。发送数据并获取模型推理响应的接口是`.generate`方法，如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4b254f-e9b2-4e6f-8cbf-e5e1b27d17db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-05 17:26:13 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aa14d8c7fd47b9bae72eb609c63a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13be07bfcbe405e87b66faccf4e9143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='你好，请你介绍一下你自己', prompt_token_ids=[108386, 37945, 56568, 109432, 107828], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='，你是谁，来自哪里，有什么能力？\\n\\n您好！我是通义千', token_ids=[3837, 105043, 100165, 3837, 101919, 101314, 3837, 104139, 99788, 26850, 111308, 6313, 104198, 31935, 64559, 99320], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(\"你好，请你介绍一下你自己\")\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f4e0b-b09c-4b7b-8cb2-f2788eb31a71",
   "metadata": {},
   "source": [
    "&emsp;&emsp;响应结果返回的是一个`RequestOutput`对象，通过`RequestOutput.outputs[0].text`可以获取到模型的推理结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52c6c6f-9a5f-429c-852e-37328f1d49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: '，你是谁，来自哪里，有什么能力？\\n\\n您好！我是通义千'\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51679b4-5d2c-4e59-9681-269546c8025d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;除了传入字符串，当输入一个列表时，`.generate`方法会执行批量推理，针对每一个`prompt`执行一次推理，并返回一个`RequestOutput`对象的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e90c5ad-dd7e-46d2-a705-a61b2bf7099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"你好，请你介绍一下你自己\",\n",
    "    \"请问什么是机器学习\",\n",
    "    \"请问如何理解大模型？\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115cb2a5-3bde-4c4a-9ad4-bd9084942995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f86e088b5a7452e9e8c99844db6d5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d375219aee4ac8ab758a8f60940c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=1, prompt='你好，请你介绍一下你自己', prompt_token_ids=[108386, 37945, 56568, 109432, 107828], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='。 你好，我叫通义千问，是阿里巴巴集团旗下的通', token_ids=[1773, 220, 108386, 3837, 35946, 99882, 31935, 64559, 99320, 56007, 3837, 20412, 107076, 100338, 111477, 31935], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=2, prompt='请问什么是机器学习', prompt_token_ids=[109194, 106582, 102182, 100134], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='，它有哪些常见的类型，以及它们在实际应用中有什么用途？\\n\\n机器', token_ids=[3837, 99652, 104719, 102716, 31905, 3837, 101034, 104017, 18493, 99912, 99892, 15946, 104139, 105795, 26850, 102182], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={}),\n",
       " RequestOutput(request_id=3, prompt='请问如何理解大模型？', prompt_token_ids=[109194, 100007, 101128, 26288, 104949, 11319], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='大模型和小模型有什么区别？大模型的发展前景如何？\\n\\n理解大', token_ids=[26288, 104949, 33108, 30709, 104949, 104139, 102665, 11319, 26288, 104949, 103949, 102653, 100007, 26850, 101128, 26288], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(text)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "522f3c05-c940-4ac3-b0c7-a6f3746054c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '你好，请你介绍一下你自己', Generated text: '。 你好，我叫通义千问，是阿里巴巴集团旗下的通'\n",
      "Prompt: '请问什么是机器学习', Generated text: '，它有哪些常见的类型，以及它们在实际应用中有什么用途？\\n\\n机器'\n",
      "Prompt: '请问如何理解大模型？', Generated text: '大模型和小模型有什么区别？大模型的发展前景如何？\\n\\n理解大'\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94834dcb-98d5-47a8-8e63-b2ecddd8cff5",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以上就是在不修改任何模型加载和推理代码的情况下，快速使用`vllm` 框架调用本地模型进行推理的方式。整个过程看并不是特别复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb13c43-cae8-4644-a208-71126bd5cd1d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们再看推理类模型的在`Vllm` 离线推理`API`中的使用，这里我们以`DeepSeek-R1-Distill-Qwen-7B`为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79490b03-4d44-43d6-a1a5-e216ceddf246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-05 18:19:39 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 10-05 18:19:42 [utils.py:233] non-default args: {'trust_remote_code': True, 'max_model_len': 4096, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': '/root/autodl-tmp/vllm/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-05 18:19:42 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-05 18:19:42 [model.py:1510] Using max model len 4096\n",
      "INFO 10-05 18:19:42 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:43 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:43 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/root/autodl-tmp/vllm/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B', speculative_config=None, tokenizer='/root/autodl-tmp/vllm/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/vllm/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:45 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m WARNING 10-05 18:19:46 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:46 [gpu_model_runner.py:2602] Starting to load model /root/autodl-tmp/vllm/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:46 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:46 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4939dd3ecb046d1990e4e886cfa4588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:47 [default_loader.py:267] Loading weights took 0.86 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:47 [gpu_model_runner.py:2653] Model loading took 3.3461 GiB and 1.045258 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:52 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/cc8c9c85b5/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:52 [backends.py:559] Dynamo bytecode transform time: 4.02 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:54 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.324 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:54 [monitor.py:34] torch.compile takes 4.02 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:56 [gpu_worker.py:298] Available KV cache memory: 14.06 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:56 [kv_cache_utils.py:1087] GPU KV cache size: 526,688 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:19:56 [kv_cache_utils.py:1091] Maximum concurrency for 4,096 tokens per request: 128.59x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 28.87it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 31.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:20:00 [gpu_model_runner.py:3480] Graph capturing finished in 4 secs, took 0.62 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=48760)\u001b[0;0m INFO 10-05 18:20:00 [core.py:210] init engine (profile, create kv cache, warmup model) took 12.65 seconds\n",
      "INFO 10-05 18:20:01 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(model=\"/root/autodl-tmp/vllm/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B\",\n",
    "          trust_remote_code=True,\n",
    "          # tensor_parallel_size=2,\n",
    "          gpu_memory_utilization=0.8,\n",
    "          max_model_len=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510f661-52d1-415a-af66-090dba9731fc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其显存占用情况与`Qwen2.5:7b` 并无差异："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49609a-9e89-4632-8db3-45d7fb4d7e0f",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181456307.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52cb34d-5c86-448e-9b6f-3da4bdbda26b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来我们进行推理模型的调用测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a734e6de-1cf9-4aac-8dcf-7046ceff7f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-05 18:20:07 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf5b8704d60417eb12225bd3ac334d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262326e9bc8241a98d89539390b80f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='你好，请你介绍一下你自己', prompt_token_ids=[151646, 108386, 37945, 56568, 109432, 107828], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='，然后介绍你的研究领域和成就。要突出你的学术背景和研究', token_ids=[3837, 101889, 100157, 103929, 99556, 100650, 33108, 102174, 1773, 30534, 102017, 103929, 104380, 102193, 33108, 99556], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(\"你好，请你介绍一下你自己\")\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d622896-8803-4215-a4a3-c41466025e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e11049c7cac48f4b47229a207461473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef50ca0e8c5c4c719e81343b80ce9415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=1, prompt='请问什么是黑洞？', prompt_token_ids=[151646, 109194, 106582, 118718, 11319], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='并解释为什么“黑洞”这个名称会被误用。\\n\\n首先，我需要', token_ids=[62926, 104136, 100678, 2073, 118718, 854, 99487, 29991, 106764, 29056, 11622, 3407, 101140, 3837, 35946, 85106], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(\"请问什么是黑洞？\")\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654e7451-8955-43e2-8060-399e517041c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: '并解释为什么“黑洞”这个名称会被误用。\\n\\n首先，我需要'\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e7f7c-48c8-4932-8bcc-3a728e90a0fa",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里能看到从返回结果上看，当改用了推理模型后，其返回的`text`字段中的内容会被截断的非常短，这是因为推理模型会包含思考过程，所以需要对`generate`接口要求输出的`Token`数需求更大，因此这里要引入一个`SamplingParams` 的概念。默认情况下，`vLLM` 的离线推理`API`会使用模型原生定义的采样参数，也就是本地存储权重中的`generation_config.json`文件中的配置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1127c69-9d2e-40b8-bbed-8952575bce17",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181521542.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b01831b-657a-48fb-a488-4f506e07ef11",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，如果某些开源的模型权重中没有提供该文件，`Vllm`会根据其定义的`SamplingParams`类自动指定默认值。其源码位置如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa83c0-9337-428a-af0c-8e1c4a0f2043",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181524308.png\" width=80%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd3016a-221a-484b-97fc-21b015b38392",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中包含的全部可定义参数如下表所示:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f19a3-0228-44fb-b293-a03658ef06eb",
   "metadata": {},
   "source": [
    "<style>\n",
    ".center \n",
    "{\n",
    "  width: auto;\n",
    "  display: table;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<p align=\"center\"><font face=\"黑体\" size=4>控制模型输出采样参数列表</font></p>\n",
    "<div class=\"center\">\n",
    "\n",
    "| 参数                          | 描述                                                                                     |\n",
    "|-----------------------------|----------------------------------------------------------------------------------------|\n",
    "| `n`                         | 要返回的输出序列数量。                                                                  |\n",
    "| `best_of`                  | 从提示生成的输出序列数量。从这些 `best_of` 序列中返回前 `n` 个序列。`best_of` 必须大于或等于 `n`。默认情况下，`best_of` 设置为 `n`。警告：此功能仅在 V0 中支持。 |\n",
    "| `presence_penalty`         | 浮点数，根据新 token 是否出现在已生成的文本中对其进行惩罚。值 > 0 鼓励模型使用新 token，值 < 0 鼓励模型重复 token。 |\n",
    "| `frequency_penalty`        | 浮点数，根据新 token 在已生成文本中的频率对其进行惩罚。值 > 0 鼓励模型使用新 token，值 < 0 鼓励模型重复 token。 |\n",
    "| `repetition_penalty`       | 浮点数，根据新 token 是否出现在提示和已生成文本中对其进行惩罚。值 > 1 鼓励模型使用新 token，值 < 1 鼓励模型重复 token。 |\n",
    "| `temperature`              | 浮点数，控制采样的随机性。较低的值使模型更确定，较高的值使模型更随机。零表示贪婪采样。 |\n",
    "| `top_p`                    | 浮点数，控制考虑的 top token 的累积概率。必须在 (0, 1] 之间。设置为 1 以考虑所有 token。 |\n",
    "| `top_k`                    | 整数，控制考虑的 top token 数量。设置为 -1 以考虑所有 token。                       |\n",
    "| `min_p`                    | 浮点数，表示相对于最可能 token 的概率，考虑 token 的最小概率。必须在 [0, 1] 之间。设置为 0 以禁用此功能。 |\n",
    "| `seed`                     | 用于生成的随机种子。                                                                    |\n",
    "| `stop`                     | 停止生成的字符串列表。当生成这些字符串时，生成将停止。返回的输出将不包含停止字符串。 |\n",
    "| `stop_token_ids`          | 停止生成的 token 列表。当生成这些 token 时，生成将停止。返回的输出将包含停止 token，除非停止 token 是特殊 token。 |\n",
    "| `bad_words`                | 不允许生成的单词列表。更准确地说，只有当下一个生成的 token 可以完成序列时，才不允许对应 token 序列的最后一个 token。 |\n",
    "| `include_stop_str_in_output` | 是否在输出文本中包含停止字符串。默认值为 False。                                      |\n",
    "| `ignore_eos`               | 是否忽略 EOS token，并在生成 EOS token 后继续生成 token。                           |\n",
    "| `max_tokens`               | 每个输出序列生成的最大 token 数量。                                                    |\n",
    "| `min_tokens`               | 每个输出序列生成的最小 token 数量，直到可以生成 EOS 或 stop_token_ids。               |\n",
    "| `logprobs`                 | 每个输出 token 返回的 log 概率数量。当设置为 None 时，不返回概率。如果设置为非 None 值，结果将包括指定数量的最可能 token 的 log 概率，以及选择的 token。注意，实施遵循 OpenAI API：API 将始终返回采样 token 的 log 概率，因此响应中可能有多达 logprobs+1 个元素。 |\n",
    "| `prompt_logprobs`          | 每个提示 token 返回的 log 概率数量。                                                  |\n",
    "| `detokenize`               | 是否对输出进行反分词。默认值为 True。                                                  |\n",
    "| `skip_special_tokens`      | 是否在输出中跳过特殊 token。                                                            |\n",
    "| `spaces_between_special_tokens` | 是否在输出中的特殊 token 之间添加空格。默认值为 True。                               |\n",
    "| `logits_processors`        | 修改 logits 的函数列表，基于先前生成的 token，并可选地将提示 token 作为第一个参数。   |\n",
    "| `truncate_prompt_tokens`    | 如果设置为整数 k，将仅使用提示的最后 k 个 token（即左截断）。默认值为 None（即不截断）。 |\n",
    "| `guided_decoding`          | 如果提供，引擎将根据这些参数构建引导解码 logits 处理器。默认值为 None。               |\n",
    "| `logit_bias`               | 如果提供，引擎将构建一个应用这些 logit 偏置的 logits 处理器。默认值为 None。         |\n",
    "| `allowed_token_ids`        | 如果提供，引擎将构建一个 logits 处理器，仅保留给定 token ids 的分数。默认值为 None。 |\n",
    "| `extra_args`               | 任意额外参数，可供自定义采样实现使用。未被任何树内采样实现使用。                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9f5aa-4315-46cf-935f-9abe8a4cb830",
   "metadata": {},
   "source": [
    "&emsp;&emsp;所以与`Qwen2.5:7b`不同的是，`DeepSeek-R1-Distill-Qwen-7B` 没有设置`max_tokens`，从而导致会用`Vllm`的默认值。因此，对大模型推理生成定制化的配置方法，则是按照上表中的参数说明定义`sampling_params`实例，设置更长的`max_tokens`参数，因为推理模型包含思考过程，需要输出更多的 `Token`，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72958927-fd1b-42cd-a17f-022c58fd2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm_model.py\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# 根据 DeepSeek 官方的建议，temperature应在 0.5-0.7，推荐 0.6 \n",
    "sampling_params = SamplingParams(max_tokens=8192, temperature=0.6, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee896d14-1ddd-4693-8b21-3de34e91640e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;构建好自定义的采样参数后，在`generate`方法中与输入的问题同步传递，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4041ce4e-b470-44aa-b705-fab1a9b72673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fa1d5763f7440f919a5568b8aeb2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963234d311d24ad3ac2211c4dec51c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=8, prompt='你好，请你介绍一下你自己。', prompt_token_ids=[151646, 108386, 37945, 56568, 109432, 107828, 1773], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='这是一个比较基础的问题，不需要详细解答。\\n\\n好的，我现在要尝试解决这个问题。为了确保我能够正确回答，我会先回想一下自己相关的知识和经验。首先，我需要确定自己是哪个年龄段的人，这可能有助于了解我是否需要使用特定的表达方式。\\n\\n接下来，我需要了解自己在哪些领域有知识和经验，这有助于我准确地表达我的背景和兴趣。例如，如果我之前在教育领域有经验，或者在某个特定的行业有工作，这些都会对我回答的问题有帮助。\\n\\n然后，我需要思考自己是否需要学习或掌握某些特定的表达方式。例如，如果我之前在某个地方有特定的口语表达习惯，或者在某个领域有特定的术语，这些可能会影响我的回答质量。\\n\\n最后，我需要确认自己是否需要进行语言上的调整，以确保回答既准确又自然。例如，如果我使用了一些比较正式或复杂的词汇，可能会影响回答的自然流畅性。\\n\\n现在，我来分析一下自己可能需要学习或掌握的方面。首先，语言表达能力：我是否需要提高自己的口语能力，或者是否需要学习一些特定的表达方式？\\n\\n其次，文化背景：我是否需要了解自己所处的国家或地区的文化背景，以便更好地表达自己？\\n\\n第三，专业领域：我是否需要了解自己所从事的行业或相关领域的知识，以便更好地回答相关问题？\\n\\n第四，职业发展：我是否需要了解自己的职业目标，以便更好地表达自己？\\n\\n第五，自我认知：我是否需要了解自己的个人背景和性格特点，以便更好地表达自己？\\n\\n现在，我来逐一分析这些方面是否需要学习或掌握。\\n\\n首先，语言表达能力：我是否需要提高自己的口语能力？这可能取决于我回答的问题是否需要表达得更自然流畅。如果我之前在某个地方有特定的口语表达习惯，可能会影响我的回答质量。\\n\\n其次，文化背景：我是否需要了解自己所处的国家或地区的文化背景？这可能取决于我是否需要在回答中使用某些文化相关词汇，或者需要在某些情况下进行文化的比较。\\n\\n第三，专业领域：我是否需要了解自己所从事的行业或相关领域的知识？这可能取决于我是否需要在回答中使用某些专业术语，或者需要在某些情况下进行行业相关的比较。\\n\\n第四，职业发展：我是否需要了解自己的职业目标？这可能取决于我是否需要在回答中表达自己的职业规划，或者需要在某些情况下进行职业相关的比较。\\n\\n第五，自我认知：我是否需要了解自己的个人背景和性格特点？这可能取决于我是否需要在回答中表达自己的个人经历，或者需要在某些情况下进行自我评价。\\n\\n现在，我需要思考这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确定自己是否需要学习或掌握这些方面。例如，语言表达能力是否需要提高？是的，可能需要。\\n\\n文化背景是否需要了解？是的，可能需要。\\n\\n专业领域是否需要了解？是的，可能需要。\\n\\n职业发展是否需要了解？是的，可能需要。\\n\\n自我认知是否需要了解？是的，可能需要。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确定这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要确认这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些专业术语。\\n\\n职业发展是否需要了解？可能需要，因为回答中可能需要表达自己的职业目标。\\n\\n自我认知是否需要了解？可能需要，因为回答中可能需要表达自己的个人经历。\\n\\n现在，我需要考虑这些方面是否真的需要学习或掌握。例如，语言表达能力是否真的需要提高？可能需要，因为回答的质量可能需要更自然流畅。\\n\\n文化背景是否需要了解？可能需要，因为回答中可能需要使用某些文化相关词汇。\\n\\n专业领域是否需要了解？可能需要，因为回答中可能需要使用某些', token_ids=[105464, 99792, 99896, 103936, 3837, 104689, 100700, 106185, 3407, 99692, 3837, 107520, 30534, 104482, 100638, 105073, 1773, 100012, 103944, 35946, 100006, 88991, 102104, 3837, 105351, 60726, 114877, 100158, 99283, 105470, 100032, 33108, 100034, 1773, 101140, 3837, 35946, 85106, 60610, 99283, 20412, 104673, 117372, 100623, 3837, 43288, 87267, 105767, 99794, 35946, 64471, 85106, 37029, 105149, 9370, 102124, 75768, 3407, 104326, 3837, 35946, 85106, 99794, 99283, 18493, 102224, 100650, 18830, 100032, 33108, 100034, 3837, 43288, 105767, 35946, 102188, 29490, 102124, 97611, 102193, 33108, 100565, 1773, 77557, 3837, 62244, 35946, 101056, 18493, 99460, 100650, 18830, 100034, 3837, 100631, 18493, 106168, 105149, 9370, 99717, 18830, 99257, 3837, 100001, 101938, 102788, 102104, 103936, 18830, 100364, 3407, 101889, 3837, 35946, 85106, 104107, 99283, 64471, 85106, 100134, 57191, 102009, 104760, 105149, 9370, 102124, 75768, 1773, 77557, 3837, 62244, 35946, 101056, 18493, 106168, 100371, 18830, 105149, 9370, 113113, 102124, 100784, 3837, 100631, 18493, 106168, 100650, 18830, 105149, 9370, 116925, 3837, 100001, 87267, 103252, 97611, 102104, 99570, 3407, 100161, 3837, 35946, 85106, 81167, 99283, 64471, 85106, 71817, 102064, 101913, 101921, 3837, 23031, 103944, 102104, 99929, 102188, 99518, 99795, 1773, 77557, 3837, 62244, 35946, 37029, 105593, 99792, 101093, 57191, 106888, 110376, 3837, 87267, 103252, 102104, 9370, 99795, 110205, 33071, 3407, 99601, 3837, 35946, 36407, 101042, 100158, 99283, 87267, 85106, 100134, 57191, 102009, 9370, 99522, 1773, 101140, 3837, 102064, 102124, 99788, 5122, 35946, 64471, 85106, 100627, 100005, 113113, 99788, 3837, 100631, 64471, 85106, 100134, 101883, 105149, 9370, 102124, 75768, 26850, 102460, 3837, 99348, 102193, 5122, 35946, 64471, 85106, 99794, 99283, 31838, 44290, 9370, 99599, 57191, 105638, 99348, 102193, 3837, 105920, 105344, 102124, 99283, 26850, 99749, 3837, 99878, 100650, 5122, 35946, 64471, 85106, 99794, 99283, 31838, 101193, 9370, 99717, 57191, 78556, 104799, 100032, 3837, 105920, 105344, 102104, 78556, 86119, 26850, 102232, 3837, 99505, 99185, 5122, 35946, 64471, 85106, 99794, 100005, 99505, 100160, 3837, 105920, 105344, 102124, 99283, 26850, 102320, 3837, 104049, 102875, 5122, 35946, 64471, 85106, 99794, 100005, 99605, 102193, 33108, 102625, 100772, 3837, 105920, 105344, 102124, 99283, 26850, 99601, 3837, 35946, 36407, 112707, 101042, 100001, 99522, 64471, 85106, 100134, 57191, 102009, 3407, 101140, 3837, 102064, 102124, 99788, 5122, 35946, 64471, 85106, 100627, 100005, 113113, 99788, 11319, 43288, 87267, 108747, 35946, 102104, 103936, 64471, 85106, 102124, 49828, 33126, 99795, 110205, 1773, 62244, 35946, 101056, 18493, 106168, 100371, 18830, 105149, 9370, 113113, 102124, 100784, 3837, 87267, 103252, 97611, 102104, 99570, 3407, 102460, 3837, 99348, 102193, 5122, 35946, 64471, 85106, 99794, 99283, 31838, 44290, 9370, 99599, 57191, 105638, 99348, 102193, 11319, 43288, 87267, 108747, 35946, 64471, 85106, 18493, 102104, 15946, 37029, 104760, 99348, 78556, 110376, 3837, 100631, 85106, 18493, 104760, 104705, 71817, 107705, 99792, 3407, 99749, 3837, 99878, 100650, 5122, 35946, 64471, 85106, 99794, 99283, 31838, 101193, 9370, 99717, 57191, 78556, 104799, 100032, 11319, 43288, 87267, 108747, 35946, 64471, 85106, 18493, 102104, 15946, 37029, 104760, 99878, 116925, 3837, 100631, 85106, 18493, 104760, 104705, 71817, 99717, 105470, 99792, 3407, 102232, 3837, 99505, 99185, 5122, 35946, 64471, 85106, 99794, 100005, 99505, 100160, 11319, 43288, 87267, 108747, 35946, 64471, 85106, 18493, 102104, 15946, 102124, 100005, 99505, 100367, 3837, 100631, 85106, 18493, 104760, 104705, 71817, 99505, 105470, 99792, 3407, 102320, 3837, 104049, 102875, 5122, 35946, 64471, 85106, 99794, 100005, 99605, 102193, 33108, 102625, 100772, 11319, 43288, 87267, 108747, 35946, 64471, 85106, 18493, 102104, 15946, 102124, 100005, 99605, 100798, 3837, 100631, 85106, 18493, 104760, 104705, 71817, 104049, 103964, 3407, 99601, 3837, 35946, 85106, 104107, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 60610, 99283, 64471, 85106, 100134, 57191, 102009, 100001, 99522, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 85106, 100627, 11319, 20412, 9370, 3837, 87267, 85106, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 20412, 9370, 3837, 87267, 85106, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 20412, 9370, 3837, 87267, 85106, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 20412, 9370, 3837, 87267, 85106, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 20412, 9370, 3837, 87267, 85106, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 60610, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 81167, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99878, 116925, 3407, 99505, 99185, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99505, 100160, 3407, 104049, 102875, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 102124, 100005, 99605, 100798, 3407, 99601, 3837, 35946, 85106, 101118, 100001, 99522, 64471, 100672, 85106, 100134, 57191, 102009, 1773, 77557, 3837, 102064, 102124, 99788, 64471, 100672, 85106, 100627, 11319, 87267, 85106, 3837, 99519, 102104, 108042, 87267, 85106, 33126, 99795, 110205, 3407, 99348, 102193, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760, 99348, 78556, 110376, 3407, 99878, 100650, 64471, 85106, 99794, 11319, 87267, 85106, 3837, 99519, 102104, 15946, 87267, 85106, 37029, 104760], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(\"你好，请你介绍一下你自己。\", sampling_params=sampling_params)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2dce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: '请说明你的职业背景和经验，以及你的兴趣爱好。请说明你为什么选择这个职业，并说明你如何找到这个职业的。请说明你的职业目标和长期愿景，以及你的职业发展计划。请详细说明你的职业发展步骤和可能遇到的挑战。\\n\\n我是个刚开始进入这个行业的新人，所以需要详细的信息，包括你的职业背景和经验，以及你的兴趣爱好。请详细说明你的职业发展步骤和可能遇到的挑战。\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请详细说明我的职业发展步骤和可能遇到的挑战。\\n\\n好的，我需要详细的信息，包括我的职业背景和经验，以及我的兴趣爱好。请'\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d95c613-b580-4f0d-b8b7-671fdb6bbc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c776159a764e440faafc5a9fbd1d582a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d99ffc932994773acf65782ea18ebc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=3, prompt='你好，请问什么是黑洞？', prompt_token_ids=[151646, 108386, 37945, 56007, 106582, 118718, 11319], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='（中文）\\n\\n</think>\\n\\n黑洞是天体物理学中的一个概念，通常指一个具有极端引力的天体，其引力足以将光或物体从表面拉向中心。黑洞的形成和演化是天体物理学的重要研究内容。', token_ids=[9909, 104811, 27866, 151649, 271, 118718, 20412, 35727, 31914, 115481, 101047, 46944, 101290, 3837, 102119, 63367, 46944, 100629, 107601, 117794, 9370, 35727, 31914, 3837, 41146, 117794, 106131, 44063, 99225, 57191, 109840, 45181, 104386, 72225, 69041, 99488, 1773, 118718, 9370, 101894, 33108, 116973, 20412, 35727, 31914, 115481, 101945, 99556, 43815, 1773, 151643], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(\"你好，请问什么是黑洞？\", sampling_params=sampling_params)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526cc924-6a16-4d77-a5a1-7ae327a8490f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这里能够明显看到是可以正常的返回推理过程和最终的推理结果的全部内容了。但是从结果上看，很难区分哪些是推理过程，哪些是推理的最红结果，因此，在`Vllm`框架下使用推理模型时，一个比较有效的技巧是每个输入的文本问题，都要以`<think>\\n `结尾，便可以直接改善推理模型返回的数据格式，如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cc311c-cf73-4a16-aeca-6562c79fe0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f298ebd3bcc3474daa5fc4b6d204c4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aff1e658c174421abf08166a5b16877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=4, prompt='你好， 请你介绍一下你自己<think>\\n', prompt_token_ids=[151646, 108386, 3837, 220, 112720, 109432, 107828, 151648, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。\\n</think>\\n\\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。', token_ids=[111308, 6313, 104198, 67071, 105538, 102217, 30918, 50984, 9909, 33464, 39350, 7552, 73218, 100013, 9370, 100168, 110498, 33464, 39350, 10911, 16, 1773, 29524, 87026, 110117, 99885, 86119, 3837, 105351, 99739, 35946, 111079, 113445, 100364, 8997, 151649, 271, 111308, 6313, 104198, 67071, 105538, 102217, 30918, 50984, 9909, 33464, 39350, 7552, 73218, 100013, 9370, 100168, 110498, 33464, 39350, 10911, 16, 1773, 29524, 87026, 110117, 99885, 86119, 3837, 105351, 99739, 35946, 111079, 113445, 100364, 1773, 151643], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"你好， 请你介绍一下你自己<think>\\n\",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params=sampling_params)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc6c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: '您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。\\n</think>\\n\\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。'\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c6f1462-8c34-4876-b946-b170a0245d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5074634e2a3549749e6967e5303be1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5aadfa692b46b08b2e429e2c51d698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=5, prompt='帮我制定一个北京的三天旅游计划<think>\\n', prompt_token_ids=[151646, 108965, 104016, 46944, 68990, 9370, 106635, 99790, 101039, 151648, 198], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='嗯，用户让我帮忙制定北京的三天旅游计划。首先，我得考虑用户的需求是什么。可能他们刚刚从 elsewhere回来，想要一个愉快的旅行，或者是为了体验北京的特色。三天的时间，应该涵盖主要景点，同时留有足够的自由时间，让用户有更多活动。\\n\\n北京的景点很多，比如天安门、故宫、石景山、 etc. 我得先理清三天的最佳路线，确保每个景点都能安排进去，同时不重复太多。第一天可以安排大一点的景点，第二天中等偏下，第三天更小，这样时间分配比较合理。\\n\\n第一天可能从天安门开始，然后去故宫，再看看一些历史建筑，比如故宫的廊塔。然后可以去一些小的景点，比如石景山公园，或者北京四台山，这样既能体验自然风光，又能了解北京的生态。第二天的话，可以从石景山公园出发，去北京四台山，然后参观历史博物馆，再看看北京博物馆。第三天可以安排一些夜游的地方，比如天安门广场，或者附近的景山公园，或者去一些夜景比较好的地方，比如大兴门。\\n\\n另外，用户可能还希望有一些活动，比如参观博物馆、历史遗迹，或者一些美食体验，比如体验冰激凌，或者吃北京特色小吃，比如包子或者小笼包。这些内容可以增加用户的兴趣，让他们在行程中更充实。\\n\\n我还需要考虑交通安排，比如地铁和公交怎么安排，第一天可能先坐地铁到天安门，然后换乘公交到故宫，这样可以更高效地游览。第二天的话，先坐地铁到石景山，然后换乘公交到四台山，再参观博物馆。第三天的话，先坐地铁到大兴门，然后选择合适的交通工具前往夜游的地方。\\n\\n另外，用户可能希望行程安排不要太赶，时间控制得当，避免疲劳。所以，每个景点的安排要合理，不要太长，让用户有足够的自由时间活动。\\n\\n最后，我应该确保整个行程的逻辑流畅，每个景点都有对应的活动安排，这样用户能有一个完整的体验。可能还需要提到一些注意事项，比如交通安排、天气情况，或者提前预订门票，这样用户可以更顺利地完成行程。\\n</think>\\n\\n当然可以！以下是一个适合三天北京之旅的计划，涵盖了主要景点和周边活动，确保行程充实且愉快。你可以根据自己的兴趣和时间安排进行调整。\\n\\n---\\n\\n### **第一天：大开山（上午8:00 - 下午12:00）**\\n**上午：**\\n- **8:00 - 9:00**：从天安门广场步行前往故宫（大约20分钟）。\\n- **9:00 - 10:00**：参观天安门广场（了解北京的历史文化）。\\n- **10:00 - 11:00**：前往故宫博物院（参观“大开山”艺术作品）。\\n\\n**下午：**\\n- **11:00 - 12:00**：参观故宫的“天圆地方”（感受历史与自然的融合）。\\n- **12:00 - 13:00**：参观北京故宫的“大清宫”的历史遗迹（了解北京的历史背景）。\\n- **13:00 - 14:00**：参观石景山公园（感受北京的自然风光）。\\n\\n**晚上：**\\n- **14:00 - 15:00**：乘坐地铁返回天安门广场。\\n- **15:00 - 16:00**：参观北京故宫附近的“北京四台山”（了解北京生态系统的知识）。\\n- **16:00 - 17:00**：参观北京博物馆（了解北京的历史文化）。\\n\\n---\\n\\n### **第二天：北京四台山（上午8:00 - 下午4:00）**\\n**上午：**\\n- **8:00 - 9:00**：乘坐地铁前往四台山景区（参观“四台山”景点）。\\n- **9:00 - 10:00**：参观四台山国家森林公园（了解北京的自然风光）。\\n- **10:00 - 11:00**：参观北京四台山历史博物馆（了解四台山的历史背景）。\\n\\n**下午：**\\n- **11:00 - 12:00**：参观北京博物馆（了解北京的历史文化）。\\n- **12:00 - 13:00**：参观“北京小笼包博物馆”（体验北京特色美食）。\\n- **13:00 - 14:00**：参观北京胡同博物馆（了解北京胡同的历史文化）。\\n\\n**晚上：**\\n- **14:00 - 15:00**：乘坐地铁返回北京四台山景区。\\n- **15:00 - 16:00**：参观北京四台山公园（感受自然风光）。\\n- **16:00 - 17:00**：参观大兴门公园（感受大兴门的自然风光）。\\n\\n---\\n\\n### **第三天：大兴门夜游（上午8:00 - 下午4:00）**\\n**上午：**\\n- **8:00 - 9:00**：乘坐地铁前往大兴门景区（参观大兴门公园）。\\n- **9:00 - 10:00**：参观大兴门公园内的“大兴门”（感受自然风光）。\\n- **10:00 - 11:00**：参观大兴门公园内的“大兴门”（了解大兴门的历史遗迹）。\\n- **11:00 - 12:00**：参观大兴门公园内的“大兴门”（感受自然风光）。\\n\\n**下午：**\\n- **12:00 - 13:00**：参观大兴门公园内的“大兴门”（了解大兴门的历史遗迹）。\\n- **13:00 - 14:00**：参观大兴门公园内的“大兴门”（感受自然风光）。\\n- **14:00 - 15:00**：在大兴门公园内体验“冰激凌”（感受夜景）。\\n\\n**晚上：**\\n- **15:00 - 16:00**：参观大兴门公园内的“大兴门”（了解大兴门的历史遗迹）。\\n- **16:00 - 17:00**：在大兴门公园内体验“冰激凌”（感受夜景）。\\n- **17:00 - 18:00**：参观大兴门公园内的“大兴门”（感受自然风光）。\\n\\n---\\n\\n### **注意事项：**\\n1. **交通安排**：地铁和公交的安排非常关键，建议提前规划好交通路线。\\n2. **天气情况**：北京的天气多变，建议提前预订门票和保险。\\n3. **美食体验**：除了参观博物馆，还可以尝试体验北京特色小吃（如包子、小笼包、冰激凌）。\\n\\n希望这份计划能让你在三天内充分体验北京的美景和历史文化！如果有其他需求，随时告诉我哦！', token_ids=[106287, 3837, 20002, 104029, 106128, 104016, 68990, 9370, 106635, 99790, 101039, 1773, 101140, 3837, 35946, 49828, 101118, 20002, 104378, 102021, 1773, 87267, 99650, 104217, 45181, 17920, 104150, 3837, 103945, 46944, 107071, 9370, 102275, 3837, 100631, 104802, 101904, 68990, 9370, 100175, 1773, 106635, 101975, 3837, 99730, 102994, 99558, 105869, 3837, 91572, 99337, 115331, 101972, 20450, 3837, 115987, 18830, 99573, 99600, 3407, 68990, 9370, 105869, 99555, 3837, 101912, 35727, 50285, 64689, 5373, 113508, 5373, 99385, 85254, 57811, 5373, 4992, 13, 49434, 239, 49828, 60726, 21887, 79766, 106635, 108721, 104844, 3837, 103944, 103991, 105869, 104297, 103956, 104880, 3837, 91572, 16530, 105444, 101319, 1773, 110120, 73670, 103956, 26288, 100380, 9370, 105869, 3837, 105350, 15946, 49567, 99835, 16872, 3837, 99749, 35727, 33126, 30709, 3837, 99654, 20450, 104675, 99792, 100745, 3407, 110120, 87267, 45181, 35727, 50285, 64689, 55286, 3837, 101889, 85336, 113508, 3837, 87256, 101997, 101883, 100022, 99893, 3837, 101912, 113508, 9370, 101701, 101105, 1773, 101889, 73670, 85336, 101883, 30709, 9370, 105869, 3837, 101912, 99385, 85254, 57811, 102077, 3837, 100631, 68990, 63703, 53938, 57811, 3837, 99654, 111628, 101904, 99795, 106686, 3837, 107713, 99794, 68990, 9370, 100171, 1773, 105350, 100363, 3837, 112255, 99385, 85254, 57811, 102077, 104532, 3837, 85336, 68990, 63703, 53938, 57811, 3837, 101889, 104682, 100022, 104646, 3837, 87256, 101997, 68990, 104646, 1773, 99749, 35727, 73670, 103956, 101883, 99530, 82894, 103958, 3837, 101912, 35727, 50285, 64689, 104208, 3837, 100631, 107922, 85254, 57811, 102077, 3837, 100631, 85336, 101883, 99530, 85254, 99792, 99692, 100371, 3837, 101912, 26288, 99355, 64689, 3407, 101948, 3837, 20002, 87267, 97706, 99880, 18830, 101883, 99600, 3837, 101912, 104682, 104646, 5373, 100022, 116985, 3837, 100631, 101883, 104365, 101904, 3837, 101912, 101904, 100038, 99591, 100812, 3837, 100631, 99405, 68990, 100175, 107600, 3837, 101912, 112258, 100631, 30709, 101600, 67279, 1773, 100001, 43815, 73670, 100649, 107494, 100565, 3837, 105380, 18493, 106318, 15946, 33126, 108300, 3407, 35946, 106750, 101118, 99735, 103956, 3837, 101912, 104851, 33108, 102540, 99494, 103956, 3837, 110120, 87267, 60726, 99901, 104851, 26939, 35727, 50285, 64689, 3837, 101889, 71134, 100252, 102540, 26939, 113508, 3837, 99654, 73670, 33126, 102202, 29490, 107871, 1773, 105350, 100363, 3837, 60726, 99901, 104851, 26939, 99385, 85254, 57811, 3837, 101889, 71134, 100252, 102540, 26939, 63703, 53938, 57811, 3837, 87256, 104682, 104646, 1773, 99749, 35727, 100363, 3837, 60726, 99901, 104851, 26939, 26288, 99355, 64689, 3837, 101889, 50404, 106873, 118113, 104374, 99530, 82894, 103958, 3407, 101948, 3837, 20002, 87267, 99880, 106318, 103956, 115596, 99970, 3837, 20450, 100359, 49828, 39165, 3837, 101153, 108351, 1773, 99999, 3837, 103991, 105869, 9370, 103956, 30534, 100745, 3837, 115596, 45861, 3837, 99258, 20002, 115331, 101972, 20450, 99600, 3407, 100161, 3837, 35946, 99730, 103944, 101908, 106318, 9370, 104913, 110205, 3837, 103991, 105869, 101103, 110019, 99600, 103956, 3837, 99654, 20002, 26232, 104133, 105896, 101904, 1773, 87267, 106750, 104496, 101883, 108043, 3837, 101912, 99735, 103956, 5373, 104307, 99559, 3837, 100631, 104154, 109545, 107250, 3837, 99654, 20002, 73670, 33126, 102088, 29490, 60548, 106318, 8997, 151649, 271, 103942, 73670, 6313, 87752, 101909, 100231, 106635, 68990, 106399, 9370, 101039, 3837, 114369, 99558, 105869, 33108, 104239, 99600, 3837, 103944, 106318, 108300, 100136, 107071, 1773, 105048, 100345, 100005, 100565, 33108, 20450, 103956, 71817, 101921, 3407, 44364, 14374, 3070, 110120, 5122, 26288, 29767, 57811, 9909, 102449, 23, 25, 15, 15, 481, 56904, 16, 17, 25, 15, 15, 7552, 1019, 334, 102449, 5122, 1019, 12, 3070, 23, 25, 15, 15, 481, 220, 24, 25, 15, 15, 334, 5122, 45181, 35727, 50285, 64689, 104208, 109019, 104374, 113508, 9909, 104995, 17, 15, 83031, 7552, 8997, 12, 3070, 24, 25, 15, 15, 481, 220, 16, 15, 25, 15, 15, 334, 5122, 104682, 35727, 50285, 64689, 104208, 9909, 99794, 68990, 104754, 99348, 7552, 8997, 12, 3070, 16, 15, 25, 15, 15, 481, 220, 16, 16, 25, 15, 15, 334, 5122, 104374, 113508, 104645, 93823, 9909, 104682, 2073, 26288, 29767, 57811, 854, 100377, 100663, 7552, 3407, 334, 102172, 5122, 1019, 12, 3070, 16, 16, 25, 15, 15, 481, 220, 16, 17, 25, 15, 15, 334, 5122, 104682, 113508, 9370, 2073, 35727, 100213, 100371, 854, 9909, 101224, 100022, 57218, 99795, 9370, 101164, 7552, 8997, 12, 3070, 16, 17, 25, 15, 15, 481, 220, 16, 18, 25, 15, 15, 334, 5122, 104682, 68990, 113508, 9370, 2073, 26288, 79766, 99921, 97907, 100022, 116985, 9909, 99794, 68990, 104754, 102193, 7552, 8997, 12, 3070, 16, 18, 25, 15, 15, 481, 220, 16, 19, 25, 15, 15, 334, 5122, 104682, 99385, 85254, 57811, 102077, 9909, 101224, 68990, 9370, 99795, 106686, 7552, 3407, 334, 104030, 5122, 1019, 12, 3070, 16, 19, 25, 15, 15, 481, 220, 16, 20, 25, 15, 15, 334, 5122, 106825, 104851, 31526, 35727, 50285, 64689, 104208, 8997, 12, 3070, 16, 20, 25, 15, 15, 481, 220, 16, 21, 25, 15, 15, 334, 5122, 104682, 68990, 113508, 107922, 2073, 68990, 63703, 53938, 57811, 854, 9909, 99794, 68990, 100171, 105743, 100032, 7552, 8997, 12, 3070, 16, 21, 25, 15, 15, 481, 220, 16, 22, 25, 15, 15, 334, 5122, 104682, 68990, 104646, 9909, 99794, 68990, 104754, 99348, 7552, 3407, 44364, 14374, 3070, 105350, 5122, 68990, 63703, 53938, 57811, 9909, 102449, 23, 25, 15, 15, 481, 56904, 19, 25, 15, 15, 7552, 1019, 334, 102449, 5122, 1019, 12, 3070, 23, 25, 15, 15, 481, 220, 24, 25, 15, 15, 334, 5122, 106825, 104851, 104374, 63703, 53938, 57811, 101227, 9909, 104682, 2073, 63703, 53938, 57811, 854, 105869, 7552, 8997, 12, 3070, 24, 25, 15, 15, 481, 220, 16, 15, 25, 15, 15, 334, 5122, 104682, 63703, 53938, 57811, 99599, 113693, 9909, 99794, 68990, 9370, 99795, 106686, 7552, 8997, 12, 3070, 16, 15, 25, 15, 15, 481, 220, 16, 16, 25, 15, 15, 334, 5122, 104682, 68990, 63703, 53938, 57811, 100022, 104646, 9909, 99794, 63703, 53938, 57811, 104754, 102193, 7552, 3407, 334, 102172, 5122, 1019, 12, 3070, 16, 16, 25, 15, 15, 481, 220, 16, 17, 25, 15, 15, 334, 5122, 104682, 68990, 104646, 9909, 99794, 68990, 104754, 99348, 7552, 8997, 12, 3070, 16, 17, 25, 15, 15, 481, 220, 16, 18, 25, 15, 15, 334, 5122, 104682, 2073, 68990, 30709, 101600, 67279, 104646, 854, 9909, 101904, 68990, 100175, 104365, 7552, 8997, 12, 3070, 16, 18, 25, 15, 15, 481, 220, 16, 19, 25, 15, 15, 334, 5122, 104682, 68990, 115652, 104646, 9909, 99794, 68990, 115652, 104754, 99348, 7552, 3407, 334, 104030, 5122, 1019, 12, 3070, 16, 19, 25, 15, 15, 481, 220, 16, 20, 25, 15, 15, 334, 5122, 106825, 104851, 31526, 68990, 63703, 53938, 57811, 101227, 8997, 12, 3070, 16, 20, 25, 15, 15, 481, 220, 16, 21, 25, 15, 15, 334, 5122, 104682, 68990, 63703, 53938, 57811, 102077, 9909, 101224, 99795, 106686, 7552, 8997, 12, 3070, 16, 21, 25, 15, 15, 481, 220, 16, 22, 25, 15, 15, 334, 5122, 104682, 26288, 99355, 64689, 102077, 9909, 101224, 26288, 99355, 64689, 9370, 99795, 106686, 7552, 3407, 44364, 14374, 3070, 99749, 35727, 5122, 26288, 99355, 64689, 99530, 82894, 9909, 102449, 23, 25, 15, 15, 481, 56904, 19, 25, 15, 15, 7552, 1019, 334, 102449, 5122, 1019, 12, 3070, 23, 25, 15, 15, 481, 220, 24, 25, 15, 15, 334, 5122, 106825, 104851, 104374, 26288, 99355, 64689, 101227, 9909, 104682, 26288, 99355, 64689, 102077, 7552, 8997, 12, 3070, 24, 25, 15, 15, 481, 220, 16, 15, 25, 15, 15, 334, 5122, 104682, 26288, 99355, 64689, 102077, 102595, 2073, 26288, 99355, 64689, 854, 9909, 101224, 99795, 106686, 7552, 8997, 12, 3070, 16, 15, 25, 15, 15, 481, 220, 16, 16, 25, 15, 15, 334, 5122, 104682, 26288, 99355, 64689, 102077, 102595, 2073, 26288, 99355, 64689, 854, 9909, 99794, 26288, 99355, 64689, 104754, 116985, 7552, 8997, 12, 3070, 16, 16, 25, 15, 15, 481, 220, 16, 17, 25, 15, 15, 334, 5122, 104682, 26288, 99355, 64689, 102077, 102595, 2073, 26288, 99355, 64689, 854, 9909, 101224, 99795, 106686, 7552, 3407, 334, 102172, 5122, 1019, 12, 3070, 16, 17, 25, 15, 15, 481, 220, 16, 18, 25, 15, 15, 334, 5122, 104682, 26288, 99355, 64689, 102077, 102595, 2073, 26288, 99355, 64689, 854, 9909, 99794, 26288, 99355, 64689, 104754, 116985, 7552, 8997, 12, 3070, 16, 18, 25, 15, 15, 481, 220, 16, 19, 25, 15, 15, 334, 5122, 104682, 26288, 99355, 64689, 102077, 102595, 2073, 26288, 99355, 64689, 854, 9909, 101224, 99795, 106686, 7552, 8997, 12, 3070, 16, 19, 25, 15, 15, 481, 220, 16, 20, 25, 15, 15, 334, 5122, 18493, 26288, 99355, 64689, 102077, 31843, 101904, 2073, 100038, 99591, 100812, 854, 9909, 101224, 99530, 85254, 7552, 3407, 334, 104030, 5122, 1019, 12, 3070, 16, 20, 25, 15, 15, 481, 220, 16, 21, 25, 15, 15, 334, 5122, 104682, 26288, 99355, 64689, 102077, 102595, 2073, 26288, 99355, 64689, 854, 9909, 99794, 26288, 99355, 64689, 104754, 116985, 7552, 8997, 12, 3070, 16, 21, 25, 15, 15, 481, 220, 16, 22, 25, 15, 15, 334, 5122, 18493, 26288, 99355, 64689, 102077, 31843, 101904, 2073, 100038, 99591, 100812, 854, 9909, 101224, 99530, 85254, 7552, 8997, 12, 3070, 16, 22, 25, 15, 15, 481, 220, 16, 23, 25, 15, 15, 334, 5122, 104682, 26288, 99355, 64689, 102077, 102595, 2073, 26288, 99355, 64689, 854, 9909, 101224, 99795, 106686, 7552, 3407, 44364, 14374, 3070, 108043, 5122, 1019, 16, 13, 3070, 99735, 103956, 334, 5122, 104851, 33108, 102540, 9370, 103956, 99491, 99936, 3837, 101898, 104154, 100367, 52801, 99735, 104844, 8997, 17, 13, 3070, 104307, 99559, 334, 5122, 68990, 9370, 104307, 42140, 74040, 3837, 101898, 104154, 109545, 107250, 33108, 100404, 8997, 18, 13, 3070, 104365, 101904, 334, 5122, 103931, 104682, 104646, 3837, 104468, 104482, 101904, 68990, 100175, 107600, 9909, 29524, 112258, 5373, 30709, 101600, 67279, 5373, 100038, 99591, 100812, 7552, 3407, 99880, 106039, 101039, 26232, 102155, 18493, 106635, 31843, 100418, 101904, 68990, 9370, 108559, 33108, 110142, 6313, 107055, 92894, 100354, 3837, 102422, 106525, 104170, 6313, 151643], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"帮我制定一个北京的三天旅游计划<think>\\n\",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params=sampling_params)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453f363-91b9-4473-8e9b-43031ddfea35",
   "metadata": {},
   "source": [
    "&emsp;&emsp;通过格式化输出可以直接区分出思考内容和最终的回复内容，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd43a4ac-7cda-44f3-9441-945ce634765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '帮我制定一个北京的三天旅游计划<think>\\n'\n",
      "Think: '嗯，用户让我帮忙制定北京的三天旅游计划。首先，我得考虑用户的需求是什么。可能他们刚刚从 elsewhere回来，想要一个愉快的旅行，或者是为了体验北京的特色。三天的时间，应该涵盖主要景点，同时留有足够的自由时间，让用户有更多活动。\\n\\n北京的景点很多，比如天安门、故宫、石景山、 etc. 我得先理清三天的最佳路线，确保每个景点都能安排进去，同时不重复太多。第一天可以安排大一点的景点，第二天中等偏下，第三天更小，这样时间分配比较合理。\\n\\n第一天可能从天安门开始，然后去故宫，再看看一些历史建筑，比如故宫的廊塔。然后可以去一些小的景点，比如石景山公园，或者北京四台山，这样既能体验自然风光，又能了解北京的生态。第二天的话，可以从石景山公园出发，去北京四台山，然后参观历史博物馆，再看看北京博物馆。第三天可以安排一些夜游的地方，比如天安门广场，或者附近的景山公园，或者去一些夜景比较好的地方，比如大兴门。\\n\\n另外，用户可能还希望有一些活动，比如参观博物馆、历史遗迹，或者一些美食体验，比如体验冰激凌，或者吃北京特色小吃，比如包子或者小笼包。这些内容可以增加用户的兴趣，让他们在行程中更充实。\\n\\n我还需要考虑交通安排，比如地铁和公交怎么安排，第一天可能先坐地铁到天安门，然后换乘公交到故宫，这样可以更高效地游览。第二天的话，先坐地铁到石景山，然后换乘公交到四台山，再参观博物馆。第三天的话，先坐地铁到大兴门，然后选择合适的交通工具前往夜游的地方。\\n\\n另外，用户可能希望行程安排不要太赶，时间控制得当，避免疲劳。所以，每个景点的安排要合理，不要太长，让用户有足够的自由时间活动。\\n\\n最后，我应该确保整个行程的逻辑流畅，每个景点都有对应的活动安排，这样用户能有一个完整的体验。可能还需要提到一些注意事项，比如交通安排、天气情况，或者提前预订门票，这样用户可以更顺利地完成行程。\\n'\n",
      "Answer: '\\n\\n当然可以！以下是一个适合三天北京之旅的计划，涵盖了主要景点和周边活动，确保行程充实且愉快。你可以根据自己的兴趣和时间安排进行调整。\\n\\n---\\n\\n### **第一天：大开山（上午8:00 - 下午12:00）**\\n**上午：**\\n- **8:00 - 9:00**：从天安门广场步行前往故宫（大约20分钟）。\\n- **9:00 - 10:00**：参观天安门广场（了解北京的历史文化）。\\n- **10:00 - 11:00**：前往故宫博物院（参观“大开山”艺术作品）。\\n\\n**下午：**\\n- **11:00 - 12:00**：参观故宫的“天圆地方”（感受历史与自然的融合）。\\n- **12:00 - 13:00**：参观北京故宫的“大清宫”的历史遗迹（了解北京的历史背景）。\\n- **13:00 - 14:00**：参观石景山公园（感受北京的自然风光）。\\n\\n**晚上：**\\n- **14:00 - 15:00**：乘坐地铁返回天安门广场。\\n- **15:00 - 16:00**：参观北京故宫附近的“北京四台山”（了解北京生态系统的知识）。\\n- **16:00 - 17:00**：参观北京博物馆（了解北京的历史文化）。\\n\\n---\\n\\n### **第二天：北京四台山（上午8:00 - 下午4:00）**\\n**上午：**\\n- **8:00 - 9:00**：乘坐地铁前往四台山景区（参观“四台山”景点）。\\n- **9:00 - 10:00**：参观四台山国家森林公园（了解北京的自然风光）。\\n- **10:00 - 11:00**：参观北京四台山历史博物馆（了解四台山的历史背景）。\\n\\n**下午：**\\n- **11:00 - 12:00**：参观北京博物馆（了解北京的历史文化）。\\n- **12:00 - 13:00**：参观“北京小笼包博物馆”（体验北京特色美食）。\\n- **13:00 - 14:00**：参观北京胡同博物馆（了解北京胡同的历史文化）。\\n\\n**晚上：**\\n- **14:00 - 15:00**：乘坐地铁返回北京四台山景区。\\n- **15:00 - 16:00**：参观北京四台山公园（感受自然风光）。\\n- **16:00 - 17:00**：参观大兴门公园（感受大兴门的自然风光）。\\n\\n---\\n\\n### **第三天：大兴门夜游（上午8:00 - 下午4:00）**\\n**上午：**\\n- **8:00 - 9:00**：乘坐地铁前往大兴门景区（参观大兴门公园）。\\n- **9:00 - 10:00**：参观大兴门公园内的“大兴门”（感受自然风光）。\\n- **10:00 - 11:00**：参观大兴门公园内的“大兴门”（了解大兴门的历史遗迹）。\\n- **11:00 - 12:00**：参观大兴门公园内的“大兴门”（感受自然风光）。\\n\\n**下午：**\\n- **12:00 - 13:00**：参观大兴门公园内的“大兴门”（了解大兴门的历史遗迹）。\\n- **13:00 - 14:00**：参观大兴门公园内的“大兴门”（感受自然风光）。\\n- **14:00 - 15:00**：在大兴门公园内体验“冰激凌”（感受夜景）。\\n\\n**晚上：**\\n- **15:00 - 16:00**：参观大兴门公园内的“大兴门”（了解大兴门的历史遗迹）。\\n- **16:00 - 17:00**：在大兴门公园内体验“冰激凌”（感受夜景）。\\n- **17:00 - 18:00**：参观大兴门公园内的“大兴门”（感受自然风光）。\\n\\n---\\n\\n### **注意事项：**\\n1. **交通安排**：地铁和公交的安排非常关键，建议提前规划好交通路线。\\n2. **天气情况**：北京的天气多变，建议提前预订门票和保险。\\n3. **美食体验**：除了参观博物馆，还可以尝试体验北京特色小吃（如包子、小笼包、冰激凌）。\\n\\n希望这份计划能让你在三天内充分体验北京的美景和历史文化！如果有其他需求，随时告诉我哦！'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    if r\"</think>\" in generated_text:\n",
    "        think_content, answer_content = generated_text.split(r\"</think>\")\n",
    "    else:\n",
    "        think_content = \"\"\n",
    "        answer_content = generated_text\n",
    "    print(f\"Prompt: {prompt!r}\\nThink: {think_content!r}\\nAnswer: {answer_content!r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba264205-4ffd-4a65-bd4c-dcad02a3a205",
   "metadata": {},
   "source": [
    "&emsp;&emsp;同理，对于批量推理来说，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a49fd9-681f-4e23-8a1e-5313f954689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b10bbaab11b4a3b94d8a3ae00d850c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d22219e2df04d90afe331bf156d5036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '给我制定一个大模型的学习计划<think>\\n'\n",
      "Think: '嗯，用户让我制定一个大模型的学习计划。首先，我得弄清楚用户的具体需求是什么。大模型，比如GPT-3，确实是一个很前沿的AI技术，学习起来需要一定的资源和时间。用户可能是一位对AI很感兴趣的人，或者是正在学习这方面技术的人。\\n\\n用户可能需要一个详细的学习计划，但又不想太花时间，所以需要结构化和可执行的计划。我应该考虑用户可能的时间安排，是否需要长期规划还是短期目标。可能用户是学生，或者是一位职场人士，需要快速掌握大模型技术。\\n\\n接下来，我得分析用户可能的深层需求。用户可能不仅仅想学习技术，还希望了解如何应用大模型，比如在自然语言处理、图像识别等领域。可能还希望了解如何优化模型，或者如何处理大规模数据。\\n\\n然后，我需要考虑用户的背景。如果用户是刚开始学习大模型，计划可能需要更基础，比如学习基本原理、框架，然后逐步深入。如果用户是已经有了一些基础，可能需要更高级的技术和应用方向。\\n\\n用户可能没有明确提到，但可能希望计划能涵盖从理论到实践的各个阶段，包括学习框架、应用、优化等。我应该确保计划全面，涵盖这些方面。\\n\\n另外，用户可能对具体的学习资源不太确定，比如哪些教材、在线课程，或者哪些工具。我应该提供一些通用的建议，比如选择权威的教材，参加在线课程，或者使用专业的工具。\\n\\n我还需要考虑用户的兴趣点，比如是否对具体应用场景感兴趣，比如医疗、金融、教育等。可能用户希望在应用方面有更深入的了解，所以计划中应该包括这些应用案例。\\n\\n最后，我应该建议用户根据自己的时间和兴趣调整计划，确保计划既可行又有效。可能需要提供一些灵活调整的建议，比如如果时间不够，可以减少某些内容，或者提前学习基础。\\n\\n总结一下，我需要制定一个结构清晰、内容全面的学习计划，涵盖理论、实践、应用，同时考虑用户的背景和时间安排，帮助用户高效掌握大模型技术。\\n'\n",
      "Answer: '\\n\\n制定一个大模型的学习计划需要综合考虑你的背景、时间安排以及学习目标。以下是一个详细的计划框架，供你参考：\\n\\n---\\n\\n### **大模型学习计划框架**\\n\\n#### **1. 基础学习阶段**\\n- **目标**：掌握大模型的基本原理、架构和框架。\\n- **内容**：\\n  - 学习大模型的基本概念（如Transformer架构、GPT系列模型）。\\n  - 理解大模型的训练过程、数据预处理、优化算法等。\\n  - 掌握主流的大模型框架（如TensorFlow、PyTorch、Llama、Bard等）。\\n- **时间安排**：约1-2个月\\n- **建议**：建议从基础框架开始，逐步深入到复杂模型。\\n\\n#### **2. 应用学习阶段**\\n- **目标**：了解大模型在实际场景中的应用。\\n- **内容**：\\n  - 学习大模型在自然语言处理（NLP）、计算机视觉（CV）、语音识别（ASR）等领域的应用。\\n  - 研究具体应用场景，如医疗诊断、金融风险评估、教育评估等。\\n- **时间安排**：约1-2个月\\n- **建议**：结合实际应用案例，如论文阅读、项目实践。\\n\\n#### **3. 技术优化与提升阶段**\\n- **目标**：优化大模型的性能，提升训练和推理效率。\\n- **内容**：\\n  - 学习深度学习优化方法（如Batch Normalization、Weight Decay、Dropout等）。\\n  - 掌握模型微调和训练技巧，如数据增强、迁移学习等。\\n  - 学习模型推理优化，如并行推理、模型压缩等。\\n- **时间安排**：约1-2个月\\n- **建议**：结合实际项目，进行模型优化和性能提升。\\n\\n#### **4. 挑战与突破阶段**\\n- **目标**：解决大模型技术中的挑战，推动技术进步。\\n- **内容**：\\n  - 研究大模型的边界条件和极限情况。\\n  - 探索新技术（如MLM、Zero-Shot学习、Zero-Rate等）。\\n  - 参与开源项目，学习社区中的创新实践。\\n- **时间安排**：约1-2个月\\n- **建议**：与同行交流，分享学习心得。\\n\\n---\\n\\n### **学习资源推荐**\\n- **教材**：\\n  - **《大语言模型的架构与训练》**（以英为本，适合深入理解架构）\\n  - **《自然语言处理的深度学习》**（适合NLP相关应用）\\n- **在线课程**：\\n  - **Coursera**：如“Deep Learning Specialization”（TensorFlow专有课程）\\n  - **edX**：如“Python for Everybody”（基础编程课程）\\n  - **Udacity**：如“Introduction to Deep Learning”（基础入门课程）\\n- **工具与框架**：\\n  - **TensorFlow**：用于深度学习框架，支持NLP。\\n  - **PyTorch**：另一个常用的深度学习框架。\\n  - **Llama**：大模型训练框架。\\n  - **Bard**：用于NLP的开源模型。\\n- **文档与社区**：\\n  - **GitHub**：如` transformers `, `bert `，了解最新代码和实践。\\n  - **Stack Overflow**：解决技术问题。\\n  - **Reddit**：如r/MachineLearning，参与讨论。\\n\\n---\\n\\n### **时间管理建议**\\n1. **制定每日学习计划**：将学习时间分配到基础、应用、优化等不同阶段。\\n2. **定期检查进展**：每周或每两周检查学习进度，调整计划。\\n3. **灵活调整**：根据实际情况调整计划，例如如果时间不够，可以减少某些内容，或者提前学习基础。\\n\\n---\\n\\n希望这个计划对你有所帮助！如果需要进一步调整或补充，请随时告诉我。'\n",
      "\n",
      "Prompt: '帮我制定一个北京的三天旅游计划<think>\\n'\n",
      "Think: '嗯，用户想让我帮他制定北京三天的旅游计划。好的，我得先想想北京的景点分布和交通情况，这样才能安排好行程。\\n\\n首先，第一天可以考虑去外滩，那里有很多历史建筑和一些 interesting的景点。然后，去鼓楼或者文化公园，看看北京的-clock tower，那里有丰富的历史故事。接着，可以去一些现代的景点，比如故宫，这样可以体验一下不同的建筑风格。\\n\\n第二天，可以去大明宫，那里有很多的建筑和历史遗迹，比如紫禁城的门廊。然后，去天安门，那里有很多的活动，比如游行和参观一些纪念品。再然后，可以去一些现代的景点，比如故宫附近的景山公园，那里有很多的花园和自然景观。\\n\\n第三天，可以考虑去一些夜市，比如天安门附近，那里有很多的灯光秀和小吃摊。然后，去古韵街，那里有很多的古色古香的建筑和老街，体验一下历史文化。最后，可以去北京博物馆，那里有很多的文物和展品，了解北京的历史文化。\\n\\n另外，交通方面，北京的地铁比较方便，可以使用地铁游览各个景点。不过，如果用户不太熟悉地铁，也可以选择公交或者打车，但地铁的线路比较密集，可能需要更多时间。\\n\\n另外，用户可能希望行程安排合理，时间上不冲突，同时还要考虑到每个景点的开放时间，避免在开放时间内游览。\\n\\n可能用户是学生或者家庭，所以时间紧张，或者需要一个比较系统的行程安排。所以，我应该提供一个全面的计划，涵盖主要的景点和可能的活动，同时留出一定的自由度，让用户可以根据自己的兴趣调整行程。\\n\\n另外，考虑到北京的天气，第一天可以安排在晴朗的天气，第二天可能有雨，第三天则比较适合晚上游玩。不过，具体天气可能因人而异，用户可以根据实际情况调整。\\n\\n总的来说，我需要先列出三天的主要景点，然后安排每一天的游览顺序，确保行程合理且不会冲突。同时，提供一些活动建议，让用户可以自由选择，比如参观历史遗迹、体验现代建筑、或品尝当地小吃等。\\n\\n最后，我应该提醒用户，三天计划可以调整，根据自己的兴趣和时间安排进行调整，确保行程流畅且不重复。\\n'\n",
      "Answer: '\\n\\n当然可以！以下是一份针对北京三天的旅游计划，涵盖历史遗迹、现代建筑和夜市体验。希望你能在三天内充分感受北京的多元文化。\\n\\n---\\n\\n### **第一天：外滩、鼓楼、文化公园**\\n**上午：**\\n- **外滩**：前往外滩，欣赏两岸的景色。可以乘坐小船，欣赏外滩的海景，或者在沙滩上拍照。\\n- **鼓楼**：鼓楼是北京的历史见证者，可以漫步在鼓楼周围，感受古旧的建筑风格。\\n- **文化公园**：前往文化公园，参观一些历史遗迹，如紫禁城的门廊。\\n\\n**下午：**\\n- **故宫**：参观故宫，感受中国古代建筑的辉煌与历史的厚重。\\n- **大明宫**：参观大明宫，了解北京的历史文化。\\n- **景山公园**：乘坐小火车游览景山公园，欣赏自然景观。\\n\\n**晚上：**\\n- **天安门**：参观天安门，感受北京的现代与历史结合。\\n- **夜景**：可以在天安门附近的夜景景点，如大兴门、天安门广场附近的小吃摊，感受现代与历史的碰撞。\\n\\n---\\n\\n### **第二天：故宫、大明宫、天安门、古韵街**\\n**上午：**\\n- **故宫**：参观故宫，了解中国古代建筑的辉煌与历史的厚重。\\n- **大明宫**：参观大明宫，了解北京的历史文化。\\n- **天安门**：参观天安门，感受北京的现代与历史结合。\\n\\n**下午：**\\n- **景山公园**：乘坐小火车游览景山公园，欣赏自然景观。\\n- **古韵街**：前往古韵街，感受北京的历史街区，品尝地道的北京小吃。\\n\\n**晚上：**\\n- **古北街**：前往古北街，体验古色古香的建筑和老街。\\n- **夜景**：在古北街的夜景景点，如古北门附近的夜市，感受现代与历史的碰撞。\\n\\n---\\n\\n### **第三天：大明宫夜游、古韵街、北京博物馆**\\n**上午：**\\n- **大明宫**：参观大明宫，了解北京的历史文化。\\n- **夜游**：在大明宫附近的夜景，如大明宫内夜游，感受历史的厚重。\\n- **古韵街**：再次游览古韵街，体验古色古香的建筑和老街。\\n\\n**下午：**\\n- **北京博物馆**：参观北京博物馆，了解北京的历史文化。\\n- **夜游**：在博物馆附近的夜景，如大明宫内夜游，感受历史的厚重。\\n\\n**晚上：**\\n- **返程**：返回北京，结束愉快的三天之旅。\\n\\n---\\n\\n### **注意事项：**\\n1. **交通**：北京的地铁非常方便，可以使用地铁游览各个景点。如果用户不太熟悉地铁，可以考虑乘坐公交或打车。\\n2. **时间安排**：三天行程比较紧凑，建议用户合理安排时间，避免在开放时间内游览。\\n3. **天气**：第一天可以安排在晴朗的天气，第二天可能有雨，第三天适合晚上游玩。具体天气因人而异，建议用户根据实际情况调整。\\n4. **活动建议**：可以在每个景点附近安排一些活动，如拍照、品尝当地小吃等。\\n\\n希望这份计划能让你在三天内充分感受北京的多元文化！祝你旅途愉快！'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"给我制定一个大模型的学习计划<think>\\n\",\n",
    "    \"帮我制定一个北京的三天旅游计划<think>\\n\",\n",
    "]\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params=sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    if r\"</think>\" in generated_text:\n",
    "        think_content, answer_content = generated_text.split(r\"</think>\")\n",
    "    else:\n",
    "        think_content = \"\"\n",
    "        answer_content = generated_text\n",
    "    print(f\"Prompt: {prompt!r}\\nThink: {think_content!r}\\nAnswer: {answer_content!r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab86944-dfdb-4c02-905d-8fcc13a84ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706738cc-9568-49a6-abde-e032a05dea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a120d3-8410-4a1a-851a-c394069444ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7428965-544a-4de7-bc52-918637047a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f700f4-358a-4225-97c3-a8dbb00278b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "909c0add-8276-4414-9d53-fe4daad7e36d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
